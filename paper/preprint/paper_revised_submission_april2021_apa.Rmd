---
title             : "Detecting social information in a dense database of infants’ natural visual experience"
shorttitle        : "Detecting social information"

author: 
  - name          : "Bria L. Long"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Stanford CA 94305"
    email         : "bria@stanford.edu"
  - name          : "George Kachergis"
    affiliation   : "1"
  - name          : "Ketan Agrawal"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"

# authornote: |
#   Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
# 
#   Enter author note here.

abstract: |
          The faces and hands of caregivers and other social partners offer a rich source of social and causal information that may be critical for infants' cognitive and linguistic development. Previous work using manual annotation strategies and cross-sectional data has found systematic changes in the proportion of faces and hands in the egocentric perspective of young infants. Here, we examine the prevalence of faces and hands in a longitudinal collection of nearly 1700 headcam videos collected from three children along a span of 6 to 32 months of age—the SAYCam dataset [@SAYcam]. To analyze these naturalistic infant egocentric videos, we first validated the use of a modern convolutional neural network of pose detection (OpenPose) for the detection of faces and hands. We then applied this model to the entire dataset, and found a higher proportion of hands in view than previous reported and a moderate decrease the proportion of faces in children's view across age. In addition, we found variability in the proportion of faces/hands viewed by different children in different locations (e.g., living room vs. kitchen), suggesting that individual activity contexts may shape the social information that infants experience.
    
  
keywords          : "social cognition; face perception; infancy; head cameras; deep learning"
wordcount         : "3444"

bibliography      : ["library.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
                      include:
                        after_body: appendix.tex
---

```{r setup, include = FALSE}
library("papaja")
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(lme4)
library(langcog)
library(lsr) 
library(stringr)
library(lmerTest)
library(here)
theme_set(theme_few())
```

# Introduction
Infants are confronted by a blooming, buzzing onslaught of stimuli [@james1890principles] which they must learn to parse to make sense of the world around them. Yet they do not embark on this learning process alone: From as early as 3 months of age, young infants follow overt gaze shifts [@gredeback2008microstructure], and even newborns prefer to look at faces with direct vs. averted gaze [@Farroni2002], despite their limited acuity. As faces are likely to be an important conduit of social information that scaffolds cognitive development, psychologists have long hypothesized that faces are prevalent in the visual experience of young infants.  

Yet until recently most hypotheses about infants' visual experience have gone untested. Though parents and scientists alike have strong intuitions about what infants see, even the viewpoint of a walking child is not easily predicted by these intuitions [@clerkin2017; @franchak2011]. By equipping infants and toddlers with head-mounted cameras, researchers have begun to document the infant's egocentric perspective on the world. Using these methods, a growing body of work now demonstrates that the viewpoints of very young infants (less than 4 months of age) are indeed dominated by frequent, persistent views of the faces of their caregivers [@Jayaraman2015; @Sugden2014; @Jayaraman2018]. 

Beyond these early months, infants' motor and cognitive abilities mature, leading to vastly different perspectives on the world. For example, crawlers see fewer faces and hands than do walking children [@sanchez2018postural; @franchak2017see; @kretch2014] as well as different views of objects [@smith2011not]. Further, as infants learn to use their own hands to act on the world, they seem to focus on manual actions taken by their social partners, and their perspective starts to capture views of hands manipulating objects [@Fausey2016]. In turn, caregivers may also start to use their hands with more communicative intent, directing infants' attention by pointing and gesturing to different events and objects during play [@yu2013joint]. 

Here, we examine the social information present in the infant visual perspective---the presence of faces and hands---by analyzing a longitudinal collection of nearly 1700 headcam videos collected from three children along a span of 6 to 32 months of age—the SAYCam dataset [@SAYcam]. In addition to its size and longitudinal nature, this dataset is more naturalistic than those previously used in two key ways. First, recordings were taken under a large variety of activity contexts [@roy2015predicting; @bruner1985role] encompassing infants' viewpoints during both activities outside and inside the home. Even in other naturalistic datasets, the incredible variety in a typical infant's experience has been largely underrepresented (see examples in Figure 1; e.g., riding in the car, gardening, watching chickens during a walk, browsing magazines, nursing, brushing teeth). Second, the head-mounted cameras used in the SAYCam dataset captured a larger field of view than those typically used, allowing a more complete picture of the infant perspective. While head-mounted cameras with a more restricted field of view do represent where infants are foveating most of the time [@yoshida2008; @smith2015contributions], they may fail to capture short saccades to either faces or hands in the periphery, as the timescale of head movements is much longer. 

With hundreds of hours of footage (>40M frames), however, this large dataset necessitates a shift to an automated annotation strategy.  Indeed, annotation of the frames extracted from egocentric videos has been prohibitively time-consuming, meaning that most frames are typically not inspected, even in the most comprehensive studies. For example, @Fausey2016 collected a total of 143 hours of head-mounted camera footage (15.5 million frames), of which one frame every five seconds was hand-annotated (by four coders), totalling 103,383 frames (per coder)—an impressive number of annotations but nonetheless only 0.67% of the collected footage. To address this challenge, we use a modern computer vision model of pose detection to automatically detect the presence of hands and faces from the infant egocentric viewpoint. Specifically, we use OpenPose [@Cao2018openpose], a model optimized for jointly detecting human face, body, hand, and foot keypoints that operates well on scenes including multiple people, even if they are partially-occluded (see Figure 1). In prior work examining egocentric videos, OpenPose performed comparably to other modern face detection models [@sanchez2018postural].

In this paper, we first describe the dataset and validate the use of this model by comparing face and hand detections to a human-annotated set of 24,000 frames. Next, we report how the proportion of faces and hands changes with age in each of the three children in the dataset. We then investigate sources of variability in our more naturalistic dataset that may explain differences from prior work, including both the field-of-view of the head cameras as well as a diversity of locations in which videos were recorded.


```{r}
## Read in data 
load(here::here('data/preprocessed_data_2021/all_vid_data_from_bbs_all_detections.RData')) # all detections
```

```{r}
# load(here::here('data/preprocessed_data_2021/gold_sample_from_bbs_2021.RData')) # gold sample detections
load(here::here('data/preprocessed_data_2021/gold_sample_annotations2020-01-31.RData')) # human annotations


load(here::here('data/preprocessed_data_2021/gold_sample_from_bbs_2021_all_detections.RData')) 

# all_gold_sample_frames_op_hc = load(here::here('data/preprocessed_data_2021/gold_sample_from_bbs_2021_high_conf_detections.RData')) 
```

```{r} 
### How large is the dataset we're analyzing?
num_frames = sum(all_vid_data$num_frames)

fps=30
num_seconds = sum(all_vid_data$num_frames)/fps
num_videos = length(unique(all_vid_data$vid_name))
num_minutes = num_seconds/60
num_hours = num_minutes/60
num_days = num_hours/24 

bin_size = 7 # days
min_age = min(all_vid_data$age_days, na.rm=TRUE)
max_age = max(all_vid_data$age_days, na.rm=TRUE)
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age)
```

# Method

## Dataset
The dataset is described in detail in @SAYcam; we summarize these details here. Children wore Veho Muvi miniature cameras mounted on a custom camping headlamp harness ("headcams") at least twice weekly, for approximately one hour per recording session. One weekly session was on the same day each week at a roughly constant time of day, while the other(s) were chosen arbitrarily at the participating family’s discretion. At the time of the recording, all three children were in single-child households.  Videos captured by the headcam were 640x480 pixels, and a fisheye lens was attached to the camera to increase the field of view to approximately 109 degrees horizontal x 70 degrees vertical. Videos ^[All videos are available at https://nyu.databrary.org/volume/564 ] with technical errors or that were not taken from the egocentric perspective were excluded from the dataset.  We analyze `r num_videos` videos, with a total duration of `r round(num_hours,2)` hours (>50 million frames).



```{r examples, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=8, fig.height=3.5, set.cap.width=T, num.cols.cap=2,  fig.fullwidth = TRUE, fig.align = "center", fig.cap = "Example frames taken from the dataset, illustrating variability in the infant perspective across different locations. OpenPose detections are shown overlaid on these images (green dots = face, red dots = hands, orange dots = pose)."}
examples <- png::readPNG("figs/modifiedOPfigure_v4.png")
grid::grid.raster(examples)
```

## Detection Method
To automatically annotate the millions of frames in SAYCam, we used a pose detector, OpenPose ^[https://github.com/CMU-Perceptual-Computing-Lab/openpose]  [@Cao2018openpose; @Simon2017hand], which provided the locations of 18 body parts (ears, nose, wrists, etc.). To do so, a convolutional neural network was used for initial anatomical detection, and part affinity fields were subsequently applied for part association to produce a series of body part candidates. Once these body part candidates were matched to a single individual in the frame, they were finally assembled into a pose. Thus, while we only made use of the outputs of the face and hand detections, the entire set of pose information from an individual was used to determine the presence of a face/hand, making the process more robust to occlusion than methods optimized to detect only faces or hands. Note, however, that these face/hand detections are reliant on the detection of at least a partial pose, so some very up-close views of faces/hands may go undetected.

## Detection Validation

```{r}
# Function to evaluate detectors
evaluate_detector <- function(truth, detection) {
  if (truth == TRUE) {
    if (truth == detection) return ("TP") # was face/wrist, detected face/wrist
    else return("FN") # was face/wrist, missed face/wrist
  }
  else {
    if (truth == detection) return("TN") # was not face/wrist, did not detect face/wrist
    else return("FP") # was not face/wrist, detected face/wrist
  }
}

# function to return prfs
 return_prf_short = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  p = tp / (tp + fp)
  r = tp / (tp + fn)
  f=( 2 * p * r )/ (p + r)
  return(c(p,r,f))
 }

# 

# join human and OP detections
gold_sample <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op) %>%
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected))

 
 ## output prfs
 face_performance = return_prf_short(gold_sample$face_eval_ketan)
 hand_performance = return_prf_short(gold_sample$hand_eval_ketan)

```


```{r}
## for high confidence detections
gold_sample_hc <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op_hc) %>% # HIGH CONF DF
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected))

 
face_performance_hc = return_prf_short(gold_sample_hc$face_eval_ketan)
hand_performance_hc = return_prf_short(gold_sample_hc$hand_eval_ketan)
```


```{r}
### Examine gold sample performance by child hands
load(here::here('data/preprocessed_data_2021/child_adult_hand_annotations_by_frame.RData'))
# 
hands_missed <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  filter(!is.na(full_image_path)) %>%
  filter(hand_eval_ketan == 'FN')

gold_sample_no_child_hands <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  replace_na(list(child_hand_seg = FALSE)) %>% # replace NAs with false (frames not in annotations (NAs) did not have hands) 
  filter(child_hand_seg==FALSE) %>% # now these are counted as frames where OP didn't need to detect something
  mutate(hand_eval_adults = evaluate_detector(hand_present_ketan, hand_detected))

hand_performance_adults = return_prf_short(gold_sample_no_child_hands$hand_eval_adults)

# can render examples of missed hands if desired
# library(magick)
# dir.create(paste0('det_examples/missed_hand'), recursive=TRUE)
# for (i in seq(1,length(hands_missed$full_image_path),1)){
#   image_read(as.character(hands_missed$full_image_path[i])) %>%
#     image_append(stack = FALSE) %>%
#     image_write(file.path(paste0("det_examples/missed_hand/", hands_missed$vid_name[i],hands_missed$frame[i],'.jpg')))
# }

```

```{r} 
### Load turk hand annotations with all bounding boxes (multiple dets per frame)
load(here::here('data/preprocessed_data_2021/hand_annotations_2020-01-29.RData'))

child_hands <- hand_annotations %>%
  filter(label=="Child hand") 

adult_hands <- hand_annotations %>%
  filter(label=="Adult hand")
```

```{r}
### plot centers of the bounding boxes made for child and adult hands
child_hand_plot <- ggplot(child_hands, aes(x=center_x, y=center_y)) +
  geom_point(alpha=.1) + 
  stat_density_2d(aes(fill = ..level..), geom="polygon", alpha=.8) +
  coord_fixed(ratio=1) +
  ggtitle('A. Child hand density') +
  ylim(0,480) +
  xlim(0,640) +
  ylab('') +
  xlab('') +
  theme_few(base_size=6) +
  theme(legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  scale_y_reverse()
##
adult_hand_plot <- ggplot(adult_hands, aes(x=center_x, y=center_y)) +
  geom_point(alpha=.1) + 
  stat_density_2d(aes(fill = ..level..), geom="polygon", alpha=.8) +
  coord_fixed(ratio=1) +
  ylim(0,480) +
  xlim(0,640) +
  scale_y_reverse() +
  ylab('') +
  xlab('') +
  theme_few(base_size=6) +
    theme(legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  ggtitle('B. Adult hand density')
```


To test the validity of OpenPose's hand and face detections, we compared the accuracy of these detections relative to human annotations of 24,000 frames selected uniformly at random from videos of two children (S and A); XX frames with allocentric videos were excluded, as were these videos from the rest of analyses. Frames were jointly annotated for the presence of faces and hands by one author. A second set of coders recruited via AMT (Amazon Mechanical Turk) additionally annotated 3150 frames; agreement with the primary coder was >95%. 

As has been observed in other studies on automated annotation of headcam data [e.g. @frank2013; @sanchez2018postural; @bambach2015lending], detection tasks that are easy in third-person video can be quite challenging in egocentric videos, due to difficult angles and sizes as well as substantial occlusion. For example, the infant perspective often contains non-canonical viewpoints of faces (e.g., looking up at a caregiver's chin) as well as partially-occluded or oblique viewpoints of both faces and hands. Further, hand detection tends to be a harder computational problem than face detection [@Simon2017hand; @bambach2015lending]. We thus expected overall performance to be lower in these naturalistic videos than on either photos taken from the adult perspective or in egocentric videos in controlled, laboratory settings [e.g., @sanchez2018postural]. 

To evaluate OpenPose's performance, we compared its detections to the manually-annotated gold set of frames, calculating precision (hits / hits + false alarms), recall (hits / hits + misses), and F-score (the harmonic mean of precision and recall).  In our data, for faces, the F-score was `r round(face_performance[3],2)`, with a precision of `r round(face_performance[1],2)` and recall of `r round(face_performance[2],2)`. For hands, the F-score was `r round(hand_performance[3],2)`, with a precision of `r round(hand_performance[1],2)` and recall of `r round(hand_performance[2],2)`. While face and hand detections showed moderately good precision, face detections were overall slightly more accurate than hand detections.  In general, hand detections suffered from fairly low recall, indicating that OpenPose likely underestimated the proportion of hands in the dataset. 
We also evaluated OpenPose's performance while restricting detections to high-confidence detections (>.5 confidence, default threshold for visualization). For faces, we found that the F-score was `r round(face_performance_hc[3],2)`, with a precision of `r round(face_performance_hc[1],2)` and recall of `r round(face_performance_hc[2],2)`. For hands, the F-score for high confidence detections `r round(hand_performance_hc[3],2)`, with a precision of `r round(hand_performance_hc[1],2)` and recall of `r round(hand_performance_hc[2],2)`. Thus, as in prior work [@long2020automated], while precision was much highe for both faces and hands, the lower recall for high-confidence detection indicates that these lower-confidence detections still index the presence of social information in the infant view. 


```{r handLocation, fig.env="figure", fig.pos = "h", fig.align = "center", fig.width=3.5, fig.height=2, fig.cap = "Density estimates for the child (left) and adult (right) hands that were detected in the 24K frame random gold set; each dot represents the center of a bounding box made by an adult participant. Brighter values indicate more detections."}
cowplot::plot_grid(child_hand_plot, adult_hand_plot, nrow=1)
```

We suspected that this was in part because children’s own hands were often in view of the camera and unconnected to a pose—-a notoriously challenging detection problem [@bambach2015lending]. To assess this possibility, we obtained human annotations for the entire subsample of 9051 frames in which a hand was detected; participants (recruited via AMT) were asked to draw bounding boxes around children’s and adult’s hands. Overall, we found that `r round(mean(hands_missed$child_hand_seg),2)*100`% of missed hand detections were of child hands. When frames with children's hands were removed from the gold set, recall did improve somewhat to `r round(hand_performance_adults[3],2)`. We also observed that children’s hands tended to appear in the lower half of the frames; heatmaps of the bounding boxes obtained from these annotations can be seen in Figure \ref{fig:handLocation}.

Finally, we examined whether the precision, recall, and F-score for hands and faces varied with age or child, and did not find substantial variation. Thus, while OpenPose was trained on photographs from the adult perspective, this model still generalized relatively well to the egocentric infant viewpoint with no fine-tuning or post-processing of the detections.  As these detections were imperfect compared to human annotators, fine-tuning these models to better optimize for the infant viewpoint remains an open avenue for future work. Standard computer vision models are rarely trained on the egocentric viewpoint, and we suspect that training these models on more naturalistic data may lead to more robust, generalizable detectors. 


# Results
## Access to social information across age

```{r}
## summary for all detections
face_hand_by_age <- all_vid_data %>%
  ungroup() %>%
  tidyr::replace_na(list(faces_and_hands=0)) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_frames_total = sum(num_frames), 
            # prop faces overall
            num_faces = sum(num_faces), 
            num_hands = sum(num_hands), 
            prop_faces = num_faces / num_frames_total, 
            prop_hands = num_hands / num_frames_total,
            # in center FOV
            num_faces_center = sum(num_faces_center, na.rm=TRUE), 
            num_hands_center = sum(num_hands_center, na.rm=TRUE), 
            prop_faces_center = num_faces_center / num_frames_total, 
            prop_hands_center = num_hands_center / num_frames_total,
            # detailed face info
            prop_full_faces = sum(num_full_faces, na.rm=TRUE)/num_frames_total,
            prop_faces_and_hands = sum(faces_and_hands)/num_frames_total,
            # face/hand contingency
            prop_faces_with_hands = sum(faces_and_hands, na.rm=TRUE)/num_faces, 
            prop_hands_with_faces = sum(faces_and_hands, na.rm=TRUE)/num_hands 
            )

```


```{R}
#### Make longer format for plotting
all_detections <- face_hand_by_age %>%
  filter(num_frames_total > 2000) %>% # eliminate small data point that skews scaling
  select(prop_faces, prop_hands, num_frames_total, age_day_bin, child_id) %>%
  pivot_longer(cols = c(prop_faces, prop_hands), names_to = "region", values_to = "prop") %>%
  mutate(approach = "uncropped",
         region = ifelse(region == "prop_faces","Faces","Hands"))  
  
centered <- face_hand_by_age %>%
  filter(num_frames_total > 2000) %>% # eliminate small data point that skews scaling
  select(prop_faces_center, prop_hands_center, num_frames_total, age_day_bin, child_id) %>%
  gather(region, prop, prop_faces_center, prop_hands_center) %>%
  mutate(approach = "centered",
         region = ifelse(region == "prop_faces_center","Faces","Hands"))
```


```{r FacesAndHands, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "Proportion of faces and hands seen as a function of age for each child in the dataset. Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range. Dashed lines show estimated trend lines from proportion of faces/hands in view when detections are restricted to the center FOV, reducing the contribution of children's own hands."}
ggplot(all_detections, 
       aes(x=age_day_bin, y=prop, 
           size=log10(num_frames_total),
           col=region)) +
  geom_point(alpha=.2) +
  geom_smooth(span=10, aes(weight = num_frames_total), show.legend = FALSE) + 
  geom_smooth(data = centered, span=10, aes(weight = num_frames_total), show.legend = TRUE,
              lty = 2, span=10, se = FALSE) +
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.5) +
  facet_grid(.~child_id) + 
  theme_few(base_size=9) +
  ggthemes::scale_color_solarized(name = "") + 
  theme(legend.text=element_text(size=8)) +
  guides(size = FALSE) +
  theme(legend.position="bottom") 
```



We analyzed the social information in view across the entire dataset, looking specifically at the proportions of faces and hands that were in view for each child.^[All analyses and preprocessed data files for this paper are available at https://tinyurl.com/detecting-social-info] Data from videos were binned according to the age of the child (in weeks). First, we saw that the proportion of faces in view showed a moderate decrease across this age range (see Figure \ref{fig:FacesAndHands}); in contrast, we did not observe an increase in the proportion of hands in view, but rather a slight decrease. These effects were quantified with two separate linear mixed-models (see Tables 1 & 2).^[Face/hand detections were binned across each week of filming. Participant's age was converted into months and centered for these analyses. Random slopes for the effect of age by child led to a singular fit and were removed from both analyses; see full model specification in accompanying codebase.]

However, the most striking result from these analyses is a much overall greater proportion of hands in view than have previously been reported [@Fausey2016]. We found this to be true across all ages, in all three children, and regardless of whether we analyzed human annotations (on the 24K random subset, see dotted lines in Appendix Figure \ref{fig:goldSetSanity}) or OpenPose annotations on the entire dataset (see solid lines in Figure \ref{fig:FacesAndHands}). This is notable especially given that OpenPose showed relatively low recall for hands, indicating that this may be an underestimate of the proportion of hands in view. Nonetheless, one reason this could be the case is the much larger field of view that was captured by the cameras used in this study: These cameras were outfitted with a fish-eye lens in an attempt to capture as much of the children’s field of view as possible, leading to a larger field of view (109 degrees horizontal x 70 degrees vertical) than in many previous studies; for example, in @Fausey2016 the FOV was 69 x 41 degrees. This larger FOV may have allowed the SAYCam cameras to capture not only the presence of a social partner’s hands interacting with objects or gestures, but also the children’s own hands, leading to more frequent hand detections.

As children's hands tended to occur in the lower visual field (see Figure \ref{fig:handLocation}), we thus re-analyzed the entire dataset while restricting our analysis to the center field of view, decreasing the proportion of hand detections from `r round(sum(all_vid_data$num_hands)/sum(all_vid_data$num_frames),2)*100`% to `r round(sum(all_vid_data$num_hands_center)/sum(all_vid_data$num_frames),2)*100`%, but only decreased face detections from `r round(sum(all_vid_data$num_faces)/sum(all_vid_data$num_frames),2)*100`% to `r round(sum(all_vid_data$num_faces_center)/sum(all_vid_data$num_frames),3)*100`%. This cropping likely removed both the majority of detections of children's own hands but also some detections of adult hands (see Figure \ref{fig:handLocation}), especially as OpenPose was biased to miss children's hands when they were in view. Nonetheless, within this modified field of view, we still observed more hand detections than face detections (see dashed lines in Figure \ref{fig:FacesAndHands}).

In a second set of analysis, we explored finer-grained changes in the ways in which infants’ experienced this social information across development, capitalizing on the fact that OpenPose provides not only face and hand detections but also positional keypoints. In particular, we explored this dataset with the idea that younger children may tend to see larger, complete faces towards the center of their visual field—that is, that have both eyes, nose, and a mouth in view—while older children may experience more incomplete views of faces.

First, we explored the proportion of faces that were detected that tended to be “complete” faces—that is, faces where the eyes, mouth, and nose were all detected by OpenPose. Figure \ref{eye_distance} shows that roughly half of the faces detected by OpenPose were full-view faces, suggesting that infants’ are often seeing partial views of their caregiver’s faces. More generally, we found that the proportion of full faces in view roughly followed the proportion of faces in view, again converging with the findings of Fausey et al., 2016 showing a decrease in the proportion of faces in the infant view across early development. 



```{r}
face_hand_detailed_plot <- face_hand_by_age %>%
  gather(region, prop, prop_full_faces, prop_faces, prop_hands, prop_faces_and_hands) %>%
  mutate(approach = "uncropped",
         region = case_when(region == "prop_faces" ~"All faces",
                            region == "prop_full_faces" ~ "Full faces (eyes, nose, mouth visible)",
                            region == "prop_hands" ~ "All hands",
                            region == "prop_faces_and_hands" ~ "Both hands and faces visible"),
         child_name = case_when(child_id =='A' ~ "Child A",
                      child_id == "S" ~ "Child S",
                      child_id == "Y" ~ "Child Y")) %>%
  filter(num_frames_total > 5000) # skews scaling of detections to have this 1 sparse age bin

```

```{r faceHandDetailed, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "Proportion of face, hand, 'full' face (i.e. nose, mouth, and nose visilbe), and joint face/hand detections as a function of age for each child in the dataset. Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range. }
ggplot(face_hand_detailed_plot, 
       aes(x=age_day_bin, y=prop, 
           col=region)) +
  geom_point(alpha=.1, aes(size=log10(num_frames_total)), show.legend= FALSE) +
  geom_smooth(span=10, aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.5) +
  facet_grid(.~child_name) + 
  theme_few(base_size=12) +
  scale_color_manual(values = c( "#2048d6","#d62020","#751098","#142c83"), name = "") +
  scale_size_continuous(name = "") +
  guides(size = FALSE) +
  theme(legend.text=element_text(size=8)) +
  theme(legend.position="bottom") 
```


```{r}
# eye distance... ugh
eye_dist <- all_vid_data %>%
  group_by(child_id, age_day_bin) %>%
  summarize(eye_distance = mean(avg_eye_distance, na.rm=TRUE), num_frames_total = sum(num_frames)) %>%
  filter(num_frames>2000)
```

```{r avgEyeDistance, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=3, fig.height=3, fig.cap = "Average distance between eyes in faces detected as a function of age for each child in the dataset (each color = different child). Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range. }
ggplot(eye_dist, 
       aes(x=age_day_bin, y=eye_distance, 
           col=child_id)) +
  geom_point(alpha=.1, aes(size=log10(num_frames_total)), show.legend= FALSE) +
  geom_smooth(method='lm', aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Average distance between eyes') + 
  xlab('Age (Months)') +
  theme_few(base_size=12) +
  scale_color_brewer(palette="Dark2") +
  guides(size = FALSE) +
  theme(legend.text=element_text(size=12)) +
  theme(legend.position="none", aspect.ratio=.75) 
```

```{r}
# eye distance... ugh
face_hand_contingency <- face_hand_by_age %>%
  pivot_longer(c(prop_faces_with_hands, prop_hands_with_faces), values_to="prop", names_to = "region") %>%
  mutate(region = case_when(
    region == "prop_faces_with_hands" ~ "Proportion hands (given face detection)",
    region == "prop_hands_with_faces" ~ "Proportion faces (given hand detection)"))
```

```{r faceHandContingency, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=5, fig.height=3, fig.cap = "Face hand contingnecy }
ggplot(face_hand_contingency, 
       aes(x=age_day_bin, y=prop, 
           col=child_id)) +
  geom_point(alpha=.1, aes(size=log10(num_frames_total)), show.legend= FALSE) +
  geom_smooth(method='lm', aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Average distance between eyes') + 
  xlab('Age (Months)') +
  theme_few(base_size=12) +
  facet_grid(~region) +
  scale_color_brewer(palette="Dark2") +
  guides(size = FALSE) +
  theme(legend.text=element_text(size=12)) +
  theme(legend.position="none", aspect.ratio=.75) 
```

In addition, we found faces tended to be farther away from children across age (restricting our analysis to faces where both eyes were detected). Figure X shows the average interpupillary distance on faces as a function of each child’s age at the time of recording, showing a trend from larger, closer faces (with a larger interpupillary distance) to smaller faces that were farther away (with a smaller interpupillary distance). We saw relatively similar trends across all three children in the dataset, and a linear mixed-effect model confirmed a main effect of age (STATS; see repository for full model specification). However, when we examined the size of all faces and hands in the infant view, we found consistency across development (see Appendix, Figure X): while faces (and full body poses) were of course bigger than hands on average, these properties were relatively stable across development and children (see Appendix, Figure X). 

Next, we explored where in the visual field children tended to see faces and hands, suspecting, perhaps, that these distributions might become wider as children grow older and learn to locomote on their own, following preliminary analyses from Frank, 2012.  As expected, faces tended to appear in the upper visual field vs. hands, which tended to be more centrally located. Figure XX shows the average position of faces and hands in the visual field; each dot represents the average position from one video within a given age range. However, we found little evidence for any changes in the positions of faces and hands across age, suggesting that this is a relatively stable property of infants’ -- and likely adults’ -- visual environment from 6 months of age. 


```{r eval=FALSE}
summary_by_age_days <- all_vid_data %>%
  group_by(age_days,child_id) %>%
  summarize(num_faces = sum(num_faces), 
            num_faces_center = sum(num_faces_center), 
            num_hands = sum(num_hands), 
            num_hands_center = sum(num_hands_center), 
            num_detect = sum(num_frames)) %>%
  mutate(prop_faces = num_faces / num_detect, 
         prop_hands = num_hands / num_detect,
         prop_faces_center = num_faces_center / num_detect, 
          prop_hands_center = num_hands_center / num_detect)

long_summary_by_age_days <- summary_by_age_days %>%
  select(prop_faces, prop_hands, age_days, child_id) %>%
  pivot_longer(col = c(prop_faces, prop_hands), names_to="region", values_to = "prop")

summary_by_age_days$age_scale <- scale(summary_by_age_days$age_days, scale = FALSE)[,1] / 30.3
long_summary_by_age_days$age_scale <- scale(long_summary_by_age_days$age_days, scale = FALSE)[,1] / 30.3

```

```{r eval=FALSE}
# run lmer models -- faces over age
faces_model_full_age_out = summary(lmer(prop_faces_center ~ age_scale + (1 | child_id), data = summary_by_age_days))

# hands over age
hands_model_full_age_out = summary(lmer(prop_hands_center ~ age_scale + (1 | child_id), data = summary_by_age_days))

# explicit modeling of faces vs. hands and interaction with age
# main effect of ages and regions; no interaction.
diff_model_full_age_out = summary(lmer(prop ~ region*age_scale + (1 | child_id), data = long_summary_by_age_days))
```

```{r eval=FALSE}
xtable(faces_model_full_age_out$coefficients, digits=c(4,3,3,3,3,3),"Model coefficients from a linear mixed model predicting the proportion of faces seen by infants in the center FOV.")

xtable(hands_model_full_age_out$coefficients, digits=c(4,3,3,3,3,3),"Model coefficients from a linear mixed model predicting the proportion of hands seen by infants n the center FOV.")
```


\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept)  9.845e-02  1.049e-02  1.956e+00   9.386  0.01198 * 
Age   -1.418e-03  4.503e-04  4.310e+02  -3.150  0.00175 **
   \hline
\end{tabular}
\caption{Model coefficients from a linear mixed model predicting the proportion of faces seen by infants in the center FOV.} 
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept)  1.612e-01  8.930e-03  1.881e+00   18.05   0.0040 **
Age   -9.987e-04  5.945e-04  4.287e+02   -1.68   0.0937 
   \hline
\end{tabular}
\caption{Model coefficients from a linear mixed model predicting the proportion of hands seen by infants in the center FOV.} 
\end{table}


## Access to social information in different locations
```{r}
load(here::here('data/data_preprint/saycam_metadata.RData')) # meta

num_videos_annotated = sum(!is.na(meta$Location))
videos_included = sum(!is.na(meta$Location) & meta$count_locations==1)
```

```{r}
vid_data_by_location <- all_vid_data %>%
  left_join(meta) %>%
  filter(!is.na(Location)) %>%
  filter(count_locations==1) 
# 
vid_data_by_location$Location = droplevels(vid_data_by_location$Location) 

# merge locations that seem similar
levels_to_modify = levels(vid_data_by_location$Location)
levels_to_modify[10] = 'Outside' # Outside on property
levels_to_modify[12] = 'Outside' # Off Property

# reset levels for both cropped/uncropped
levels(vid_data_by_location$Location) <- levels_to_modify

percentage_of_dataset = sum(vid_data_by_location$num_frames)/sum(all_vid_data$num_frames)
num_frames = sum(vid_data_by_location$num_frames)

```


```{r}
face_hand_by_location <- vid_data_by_location %>%
  group_by(Location,child_id,age_day_bin) %>%
  summarize(num_detect = sum(num_frames),prop_faces = sum(num_faces)/num_detect, prop_hands = sum(num_hands)/num_detect,  prop_hands_center =  sum(num_hands_center, na.rm=TRUE)/num_detect, prop_faces_center = sum(num_faces_center, na.rm=TRUE)/num_detect)  %>%
  group_by(Location) %>%
  filter(num_detect > 2000)

## Compute CIs for faces/hands by each location, and then merge back raw data so they can be plotted together
## Get rid of any locations where we didn't have enough data to make a CI (not more than 1 age_bin)
hands_to_plot_centered <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_hands_center") %>%
  mutate(region = "Hands (Center)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_hands_center) %>%
  filter(!is.na(ci_lower)) 

faces_to_plot_centered <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_faces_center") %>%
  mutate(region = "Face (Center)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_faces_center) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter

faces_to_plot_all <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_faces") %>%
  mutate(region = "Faces (All)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_faces) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter

hands_to_plot_all <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_hands") %>%
  mutate(region = "Hands (All)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_hands) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter
  
all_to_plot <- faces_to_plot_centered %>%
  full_join(hands_to_plot_centered) %>%
  full_join(hands_to_plot_all) %>%
  full_join(faces_to_plot_all) %>%
  group_by(Location) %>%
  mutate(num_detect_location = sum(num_detect)) %>%
  ungroup() %>%
  mutate(Location = fct_reorder(Location, num_detect_location))
```

```{r DetByLocation, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "Proportion of faces and hands by location in which egocentric videos were filmed; each panel represents data from an individual child (location annotations were not yet available for Y). Each dot represents data from a week in which videos were filmed and are scaled by the number of frames."}

all_to_plot$region <- as.factor(all_to_plot$region)
all_to_plot$region <- relevel(all_to_plot$region, ref ="Faces (All)")

ggplot(all_to_plot, aes(x = Location, y = mean, col=region)) + 
  geom_point(aes(x=Location, y=prop, size=log10(num_detect)), alpha=.1, position = position_dodge(width=.6)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, linetype=region),position = position_dodge(width=.6)) +
  coord_flip() + 
  ylab('Proportion Detected')+
  xlab('')+
  theme_few(base_size=18) +
  theme(legend.position = "bottom") +
  facet_grid(.~child_id)  +
  scale_size_continuous(name = "Detections (Log 10)") +
  scale_linetype_manual(values=c("solid", "dashed","solid","dashed"), name = "") +
  scale_color_manual(values = c("#268bd2", "#51acec", "#dc322f","#ea5e5c"), name = "") +
  theme(legend.text=element_text(size=8))
  
```


```{r include=FALSE}
### Chi-squared tests for location variability
vid_data_by_location_A <- vid_data_by_location %>%
  filter(child_id == "A") %>%
  group_by(Location) %>%
  summarize(num_faces_by_loc = sum(num_faces), num_no_faces_by_loc = sum(num_frames)-num_faces_by_loc,
            num_hands_by_loc = sum(num_hands),  num_no_hands_by_loc = sum(num_frames)-num_hands_by_loc)  

vid_data_by_location_S <- vid_data_by_location %>%
  filter(child_id == "S") %>%
  group_by(Location) %>%
  summarize(num_faces_by_loc = sum(num_faces), num_no_faces_by_loc = sum(num_frames)-num_faces_by_loc,
            num_hands_by_loc = sum(num_hands),  num_no_hands_by_loc = sum(num_frames)-num_hands_by_loc)  

Faces_A <- cbind(vid_data_by_location_A$num_faces_by_loc, vid_data_by_location_A$num_no_faces_by_loc)
Faces_S <- cbind(vid_data_by_location_S$num_faces_by_loc, vid_data_by_location_S$num_no_faces_by_loc)

Hands_A <- cbind(vid_data_by_location_A$num_hands_by_loc, vid_data_by_location_A$num_no_hands_by_loc)
Hands_S <- cbind(vid_data_by_location_S$num_hands_by_loc, vid_data_by_location_S$num_no_hands_by_loc)

chisq.test(Faces_A)
chisq.test(Faces_S)
chisq.test(Hands_A)
chisq.test(Hands_S)
```


How does variability across different contexts influence the social information in the infant view? Intuitively, some activities in different contexts may be characterized by a much higher proportion of faces (e.g., diaper changes in bedrooms) than others (e.g., playtime in the living room). We thus next examined variation in presence of hands and faces across different locations. Of the `r num_videos` videos, `r num_videos_annotated` were annotated [@SAYcam] for the location they were filmed in. Of these, `r videos_included` videos were filmed in single location, representing 17 percent of the dataset and over 5 million frames. Activities varied somewhat predictably by these contexts: for example, eating tended to occur in the kitchen, whereas playtime was the dominant activity in the living room. Overall, we found that the proportion of faces vs. hands varied across filming locations, and, to some extent, across children; separate chi-squared tests for each child and detection type revealed significant variability in detections by location in each case ^[all *p*<.001, see accompanying codebase]. For example, while both A and S saw a relatively similar proportion of faces vs. hands in the bedroom, they saw quite different amounts of faces vs. hands in kitchens (see Figure \ref{fig:DetByLocation}). 

# General Discussion
Here, we analyzed the social information in view in a dense, longitudinal dataset, applying a modern computer-vision model to quantify the proportion of hands and faces seen from each of three children's egocentric perspective from 6 to 32 months of age. This analysis has yielded a better understanding of infants' evolving access to social information. We found a moderate decrease across age in the proportion of faces in view in the videos, in keeping with previous work [@Fausey2016]. This finding is particularly notable given that, in previous cross-sectional data, this effect seems to be most strongly driven by infants younger than 4 months of age [e.g., @Fausey2016; @Jayaraman2015; @Sugden2014] who see both more frequent and more persistent faces [@Jayaraman2018].  

We also found an unexpectedly high proportion of hands in the view of infants, even when restricting the field-of-view to the center field of view the videos to make the viewpoints comparable to those of headcams used in previous work [@Fausey2016]. Why might this be the case? One idea is that these videos contain the viewpoints of children not only during structured interactions (e.g., play sessions at home or in the lab) but during everyday activities when children may be playing by themselves or simply observing the actions of caregivers and other people in their environment. During these less structured times, caregivers may move about in the vicinity of the child but not interact with them as directly---leading to views where a person and their hands are visible from a distance, but this person's face may be turned away from the infant or occluded (see examples in Figure 1). Indeed, using the same pose detector on videos from in-lab play sessions, @sanchez2018postural found the opposite trend: slightly fewer hand detections than face detections from 8-16 months of age. Work that directly examines the variability in the social information in view across more vs. less structured activity contexts could further test this idea.

A coarse analysis based on the location the videos were filmed in further highlights the variability of the social information in view during different activities, showing differences across locations and between individual children. Within a given, well-defined context---e.g., mealtime in kitchens---S saw more faces than A, and S saw more faces in the kitchen than in other locations. This variability likely stems from the fact that there are at least three ways to feed a young child: 1) sitting in front of the child, facing them as they sit in a high chair; 2) sitting behind the child, holding them as they face outward, and 3) sitting side by side. Each of these positions offer the child differing degrees of visual access to faces and hands. While the social information in view may be variable across children in different activity contexts, these analyses suggest they could be stable within a given child's day-to-day experience.

Overall, these analyses underscore the importance of how, when, from whom, and what data we sample; these choices become central when we attempt to draw conclusions about the regularities of experience. Indeed, while unprecedented in size, this dataset still has many limitations. These videos only represent a small portion of the everyday experience of these three children, all of whom come from relatively privileged households in western societies and thus are not representative of the global population. Any idiosyncrasies in how and when these particular families chose to film these videos also undoubtedly influences the variability seen here. And without eye-tracking data, we do not know if children are attending to the social information in their visual field. 

Nonetheless, we believe that these advances in datasets and methodologies represent a step in the right direction. The present paper demonstrates the feasibility of using a modern computer vision model to annotate the entirety of a very large dataset (here, >40M million frames) for the presence and size of people, hands, and faces, representing orders of magnitude more data relative to prior work. We propose that the large-scale analysis of dense datasets, collected with different fields of view, cameras and from many different laboratories, will lead to generalizable conclusions about the regularities of infant experience that scaffold learning.


# Acknowledgements
Thanks to the creators of the SAYCam dataset who made this work possible and to Alessandro Sanchez for his contributions to the codebase. This work was funded by a Jacobs Foundation Fellowship to MCF, a John Merck Scholars award to MCF, and NSF #1714726 to BLL. 

\newpage

# References
```{r create_r-references}
r_refs(file = "library.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
