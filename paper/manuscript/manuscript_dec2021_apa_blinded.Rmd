---
title: A longitudinal analysis of the social information in infants' naturalistic
  visual experience using automated detections
author:
- name: Blinded
  affiliation: '1'
  corresponding: yes
  address: ''
  email: ' '
- name: ' '
  affiliation: '1'
shorttitle: Longitudinal analysis of social information
output: papaja::apa6_docx
authornote: "The data and code that support the findings of this study are available
  at https://osf.io/cdhw4/. \n"
abstract: |
  The faces and hands of caregivers and other social partners offer a rich source of social and causal information that is likely critical for infants’ cognitive and linguistic development. Previous work using manual annotation strategies and cross-sectional data has found systematic changes in the proportion of faces and hands in the egocentric perspective of young infants. Here, we validated the use of a modern convolutional neural network (OpenPose) for the detection of faces and hands in naturalistic egocentric videos. We then applied this model to a longitudinal collection of more than 1700 headcam videos from three children ages 6 to 32 months. Using these detections, we confirm and extend prior results from cross-sectional studies. First, we found a moderate decrease in the proportion of faces in children’s view across age and a higher proportion of hands in view than previously reported. Second, we found variability in the proportion of faces and hands viewed by different children in different locations (e.g., living room vs. kitchen), suggesting that individual activity contexts may shape the social information that infants experience. Third, we found evidence that children may see closer, larger views of people, hands, and faces earlier in development. These longitudinal analyses provide an additional perspective on the changes in the social information in view across the first few years of life and suggest that pose detection models can successfully be applied to naturalistic egocentric video datasets to extract descriptives about infants’ changing social environment.
keywords: social cognition; face perception; infancy; head cameras; deep learning
wordcount: X
bibliography: library.bib
floatsintext: no
figurelist: no
tablelist: no
footnotelist: no
linenumbers: no
mask: no
draft: no
documentclass: apa6
classoption: man
affiliation:
- id: '1'
  institution: Blinded

---

```{r setup, include = FALSE}
library("papaja")
```


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(lme4)
library(langcog)
library(lsr) 
library(stringr)
library(lmerTest)
library(here)
library(papaja)
theme_set(theme_few())
```

# Introduction
Infants are confronted by a blooming, buzzing onslaught of stimuli [@james1890principles] that they must learn to parse to make sense of the world around them. Yet they do not embark on this learning process alone: From as early as 3 months of age, young infants follow overt gaze shifts [@gredeback2008microstructure], and even newborns prefer to look at faces with direct vs. averted gaze [@Farroni2002], despite their limited acuity. As faces are likely to be an important conduit of social information that scaffolds cognitive development, psychologists have long hypothesized that faces are prevalent in the visual experience of young infants.  

Yet until recently most hypotheses about infants' visual experience have gone untested. 
Though parents and scientists alike have strong intuitions about what infants see, even the viewpoint of a walking child is hard to intuit [@clerkin2017; @franchak2011].
By equipping infants and toddlers with head-mounted cameras, researchers have begun to document the infant's egocentric perspective on the world [@smith2015contributions; @smith2018developing; @franchak2011] and the consequences of this changing view for early learning.
Using these methods, a growing body of work now demonstrates that the viewpoints of very young infants (less than 4 months of age) are indeed dominated by frequent, persistent views of the faces of their caregivers [@Jayaraman2015; @jayaraman2013visual; @jayaraman2017faces; @sugden2014spy; @Jayaraman2018]. 

Beyond these early months, infants' motor and cognitive abilities mature, leading to vastly different perspectives on the world [@iverson2010]. For example, children see fewer faces and hands when crawling than walking or sitting [@luo2020head; @sanchez2018postural; @franchak2017see; @kretch2014; @franchak2019changing; @yamamoto2020transition; @long2021automated] as well as different views of objects [@smith2011not; @luo2020head]. Further, as infants learn to use their own hands to act on the world, they seem to focus on manual actions taken by their social partners, and their perspective starts to capture views of hands manipulating objects [@Fausey2016]. In turn, caregivers may also start to use their hands with more communicative intent, directing infants' attention by pointing and gesturing to different events and objects during play [@yu2013joint]. 

Here, we examine the social information present in the infant visual perspective -- the presence of faces and hands -- by analyzing a longitudinal collection of more than 1700 headcam videos collected at home from three children along a span of 6 to 32 months of age -- the SAYCam dataset [@SAYcam]. We analyze the video data from this dense, longitudinal dataset as case studies of how the social information in the infant view changes over the course of development in individual children. These kinds of longitudinal case studies provide an important counterpoint to cross-sectional investigations for several reasons. By repeatedly recording data from the same child, they allow us to investigate developmental changes within each child, avoiding the ecological fallacy of assuming that between-person patterns reflect within-person mechanisms [@piantadosi1988ecological]. Second, they allow us to relate developmental changes to milestones achieved by an individual child (e.g., the onset of walking). Third, they eliminate several sources of variability (e.g., parent/child familiarity with camera, home environment) that may obscure fine-grained developmental patterns.

Finally, longitudinal case studies allow us to investigate the ecological validity of results in everyday, naturalistic learning contexts. Indeed, in addition to its size and longitudinal nature, the SAYCam dataset is more naturalistic than those previously used in two key ways. First, recordings were taken under a large variety of activity contexts [@roy2015predicting; @bruner1985role] encompassing infants' viewpoints during both activities outside and inside the home. Even in other naturalistic datasets, the incredible variety in a typical infant's experience has been largely underrepresented (see examples in Figure 1; e.g., riding in the car, gardening, watching chickens during a walk, browsing magazines, nursing, brushing teeth). Second, the head-mounted camera devices used in the SAYCam dataset captured a larger field of view than those typically used, allowing a more complete picture of the infant perspective. While head-mounted cameras with a more restricted field of view do represent where infants are foveating most of the time [@yoshida2008; @smith2015contributions], they may fail when faces or hands appear in children's peripheral vision but are still part of a joint interaction.

However, the SAYCam data – while naturalistic and densely sampled within individuals – come from only a handful of children. These are a group of three children of developmental psychologists growing up in US contexts that conform to the WEIRD rubric [@henrich2010most] for numerous reasons they cannot on their own yield generalizable conclusions about developmental changes in the infant view. Instead, they serve as a testbed for validating prior developmental findings within the daily lives of individual children – and for generating novel hypotheses that can be tested in cross-sectional datasets or targeted in-lab experiments.

With hundreds of hours of footage (>42M frames), this large dataset necessitates a shift to an automated annotation strategy.
Indeed, annotation of the frames extracted from egocentric videos has been prohibitively time-consuming, meaning that most frames are typically not inspected, even in the most comprehensive studies. 
For example, @Fausey2016 collected a total of 143 hours of head-mounted camera footage (15.5 million frames), of which one frame every five seconds was hand-annotated (by four coders), totalling 103,383 frames (per coder)--an impressive number of annotations but nonetheless only 0.67% of the collected footage. 

To address this challenge, we use a modern computer vision model of pose detection to automatically detect the presence of hands and faces from the infant egocentric viewpoint. 
Specifically, we use OpenPose [@Cao2018openpose], a model optimized for jointly detecting human face, body, hand, and foot keypoints. 
In prior work examining egocentric videos taken from the infant perspective during in-lab play sessions, OpenPose performed comparably to other modern face detection models [@long2021automated].
However, these at-home, naturalistic videos contain more cluttered visual environments, challenging lighting conditions, and many different novel viewpoints.
In this paper, we first validate the use of OpenPose by comparing face and hand detections to a human-annotated set of 24,000 frames. Though OpenPose does not detect faces and hands with perfect accuracy, we find that this model still operates relatively well on these challenging egocentric scenes including multiple people, even if they are partially-occluded (see Figure 1 for example detections and Appendix Figure A1 for examples of missed detections and false alarms). 

We thus capitalize on this computational advance to annotate the entirety of this large, longitudinal video dataset. 
We use these detections to first report how the proportion of faces and hands changes with age in each of the three children in the dataset. 
We then investigate sources of variability in our more naturalistic dataset that may explain differences from prior work, including both the field-of-view of the head cameras as well as a diversity of locations in which videos were recorded.
Finally, making use of automated annotation of pose bounding boxes, we analyze the size, location, and variability of detected faces and poses across development in these three children.
Overall, these findings largely validate prior cross-sectional findings, and further suggest that automated annotations can be used to analyze naturalistic video datasets to build towards generalizable conclusions about the statistics of the social information in the infant view across a wide variety of populations.

```{r}
# Read in data 
## Face/hand detections by video
load(file=here::here('data/preprocessed_data_2022/all_vid_data_from_bbs_all_detections.RData')) # all detections
```

```{r}
# load bounding box summaries by video
load(file=here::here('data/preprocessed_data_2022/bounding_box_summaries_all.RData'))
```

```{r}
# gold sample annotations for these specific frames
load(here::here('data/preprocessed_data_2022/gold_sample_annotations2020-01-31.RData')) # human annotations
```

```{r}
# load high confidence detections for gold sample frames
load(here::here('data/preprocessed_data_2022/gold_sample_from_bbs_2021_high_conf_detections.RData'))
all_gold_sample_frames_op_hc = all_gold_sample_frames_op
```

```{r}
# load all detections for gold sample frames
load(here::here('data/preprocessed_data_2022/gold_sample_from_bbs_2021_with_centerall_detections.RData')) 
```

```{r} 
### How large is the dataset we're analyzing?
num_frames = sum(all_vid_data$num_frames)

fps=30
num_seconds = sum(all_vid_data$num_frames)/fps
num_videos = length(unique(all_vid_data$vid_name))
num_minutes = num_seconds/60
num_hours = num_minutes/60
num_days = num_hours/24 

bin_size = 7 # days
min_age = min(all_vid_data$age_days, na.rm=TRUE)
max_age = max(all_vid_data$age_days, na.rm=TRUE)
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age)
```

# Method

## Dataset
The dataset is described in detail in @SAYcam; we summarize these details here. Children wore Veho Muvi miniature cameras mounted on a custom camping headlamp harness ("headcams") at least twice weekly, for approximately one hour per recording session. One weekly session was on the same day each week at a roughly constant time of day, while the other(s) were chosen arbitrarily at the participating family’s discretion. At the time of the recording, all three children were in single-child households.  Videos captured by the headcam were 640x480 pixels, and a fisheye lens was attached to the camera to increase the field of view to approximately 109 degrees horizontal x 70 degrees vertical. Videos ^[All videos are available at https://nyu.databrary.org/volume/564 ] with technical errors or that were not taken from the egocentric perspective were excluded from the dataset.  We analyze `r num_videos` videos, with a total duration of `r round(num_hours,2)` hours (>42 million frames).

```{r examples, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=8, fig.height=3.5, set.cap.width=T, num.cols.cap=2,  fig.fullwidth = TRUE, fig.align = "center", fig.cap = "Example frames taken from the dataset, illustrating variability in the infant perspective across different locations. OpenPose detections are shown overlaid on these images (green dots = face, red dots = hands, orange dots = pose)."}
examples <- png::readPNG("figs/modifiedOPfigure_v4.png")
grid::grid.raster(examples)
```

## Detection Method
To  annotate the millions of frames in SAYCam automatically, we used a pose detector, OpenPose^[https://github.com/CMU-Perceptual-Computing-Lab/openpose]  [@Cao2018openpose; @Simon2017hand]. The OpenPose system provides the locations of up to 18 body parts (ears, nose, wrists, etc.) from individual frames. OpenPose relies on a convolutional neural network for initial anatomical detection. It then uses part affinity fields for part association to produce a series of body part candidates. Once these body part candidates are matched to a single individual in the frame, they are finally assembled into a pose. While in this study we only measured face and hand presence, the entire set of pose information from an individual was used to determine the presence of a face/hand, making the process much more robust to occlusion than methods optimized to detect *only* faces or hands. Of course, these face/hand detections are nevertheless reliant on the detection of at least a partial pose, so some very up-close views of faces/hands may still go undetected.


## Detector Validation
```{r}
# Function to evaluate detectors
evaluate_detector <- function(truth, detection) {
  if (truth == TRUE) {
    if (truth == detection) return ("TP") # was face/wrist, detected face/wrist
    else return("FN") # was face/wrist, missed face/wrist
  }
  else {
    if (truth == detection) return("TN") # was not face/wrist, did not detect face/wrist
    else return("FP") # was not face/wrist, detected face/wrist
  }
}

# function to return prfs
 return_prf_short = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  p = tp / (tp + fp)
  r = tp / (tp + fn)
  f=( 2 * p * r )/ (p + r)
  return(c(p,r,f))
 }

# 
```

```{r}
# join human and OP detections
gold_sample <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op) %>%
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected)) %>%
   mutate(face_eval_ketan_center = evaluate_detector(face_present_ketan, face_detected_center), hand_eval_ketan_center = evaluate_detector(hand_present_ketan, hand_detected_center))

 
 ## output prfs
 face_performance = return_prf_short(gold_sample$face_eval_ketan)
 hand_performance = return_prf_short(gold_sample$hand_eval_ketan)
 
 face_performance_center = return_prf_short(gold_sample$face_eval_ketan_center)
 hand_performance_center = return_prf_short(gold_sample$hand_eval_ketan_center)
```


```{r}
## for high confidence detections
gold_sample_hc <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  left_join(all_gold_sample_frames_op_hc) %>% # HIGH CONF DF
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected))


face_performance_hc = return_prf_short(gold_sample_hc$face_eval_ketan)
hand_performance_hc = return_prf_short(gold_sample_hc$hand_eval_ketan)
```


```{r}
### Examine gold sample performance by child hands
load(here::here('data/preprocessed_data_2022/child_adult_hand_annotations_by_frame.RData'))
# 
hands_missed <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  filter(!is.na(full_image_path)) %>%
  filter(hand_eval_ketan == 'FN')

gold_sample_no_child_hands <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  replace_na(list(child_hand_seg = FALSE)) %>% # replace NAs with false (frames not in annotations (NAs) did not have hands)
  filter(child_hand_seg==FALSE) %>% # now these are counted as frames where OP didn't need to detect something
  mutate(hand_eval_adults = evaluate_detector(hand_present_ketan, hand_detected))


gold_sample_child_hands <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  filter(child_hand_seg==TRUE) 

hand_performance_adults = return_prf_short(gold_sample_no_child_hands$hand_eval_adults)

# can render examples of missed hands if desired
# library(magick)
# dir.create(paste0('det_examples/missed_hand'), recursive=TRUE)
# for (i in seq(1,length(hands_missed$full_image_path),1)){
#   image_read(as.character(hands_missed$full_image_path[i])) %>%
#     image_append(stack = FALSE) %>%
#     image_write(file.path(paste0("det_examples/missed_hand/", hands_missed$vid_name[i],hands_missed$frame[i],'.jpg')))
# }
```

```{r}
num_frames_excluded = 24000-length(gold_sample$face_detected)
```

As has been observed in other studies on automated annotation of headcam data [e.g. @frank2013; @long2021automated; @sanchez2018postural; @bambach2015lending], detection tasks that are easy in third-person video can be quite challenging in egocentric videos, due to difficult angles and sizes as well as substantial occlusion. For example, the infant perspective often contains non-canonical viewpoints of faces (e.g., looking up at a caregiver's chin) as well as partially-occluded or oblique viewpoints of both faces and hands. Further, hand detection tends to be a harder computational problem than face detection [@Simon2017hand; @bambach2015lending]. We thus expected overall performance to be lower in these naturalistic videos than on either photos taken from the adult perspective or in egocentric videos in controlled, laboratory settings [e.g., @long2021automated]. 

### OpenPose vs. human annotations of faces/hands
To test the validity of OpenPose's hand and face detections, we compared the accuracy detections relative to human annotations of 24,000 frames selected uniformly at random from videos of two children (S and A); this sample was conducted before child Y was added to the dataset.  Frames were jointly annotated for the presence of faces and hands by one author. A second set of coders recruited via AMT (Amazon Mechanical Turk) additionally annotated 3150 frames; agreement with the primary coder was >95%. Upon manually inspecting these 24K frames, we noticed that `r  num_frames_excluded` were sampled from videos taken from the allocentric perspective (i.e., not from the infant viewpoint); these frames and videos containing these frames were subsequently excluded from all other analyses.

To evaluate OpenPose's performance, we compared its detections to the manually-annotated "gold set" of frames, calculating precision (hits / (hits + false alarms)), recall (hits / (hits + misses)), and F-score (the harmonic mean of precision and recall).  In our data, for faces, the F-score was `r round(face_performance[3],2)`, with a precision of `r round(face_performance[1],2)` and recall of `r round(face_performance[2],2)`. For hands, the F-score was `r round(hand_performance[3],2)`, with a precision of `r round(hand_performance[1],2)` and recall of `r round(hand_performance[2],2)`. While face and hand detections showed moderately good precision, face detections were overall slightly more accurate than hand detections. In general, hand detections suffered from fairly low recall, indicating that OpenPose likely underestimated the proportion of hands in the dataset. 

We next inspected frames where OpenPose failed to detect faces/hands or erroneously detected them to gain insight into the factors driving performance. Appendix Figure A1 shows randomly sampled examples of both missed detections and false alarms for both faces and hands and highlights the challenging detection conditions presented by these at-home, naturalistic videos. Many missed faces included very far away or very close up views of faces at odd angles, as well as faces that were partially occluded by odd lighting conditions. False alarms for faces included animal faces depicted in picture books and poses where the person was not facing a camera. Hands were sometimes erroneously detected when a pose was present but the hands were out of view, and hands were often missed when they were seen only from the side and unconnected to a pose or belonged to the child. One conclusion of this examination is that egocentric videos using cameras with larger view angles, higher resolution, and more lighting-invariant sensors may lead to better performance, even in the absence of better detection models.

### Detecting children's own hands 
We thus suspected that OpenPose's low recall for hands in this dataset was because children’s own hands were often in view of the camera and unconnected to a pose -- a notoriously challenging detection problem [@bambach2015lending]. To assess this possibility, we obtained additional human annotations for the subsample of 9051 frames in the gold set frames where a hand was present; participants (recruited via Amazon Mechanical Turk) were asked to draw bounding boxes around children’s and adult’s hands. Overall, we found that `r round(mean(hands_missed$child_hand_seg),2)*100`% of missed hand detections were of child hands; OpenPose was only able to detect children's hands when there were in view `r round(mean(gold_sample_child_hands$hand_detected),2)*100`% of the time. Accordingly, when frames with children's hands were removed from the gold set, recall did improve somewhat to `r round(hand_performance_adults[3],2)`. We also observed that children’s hands tended to appear in the lower half of the frames; heatmaps of the bounding boxes obtained from these annotations can be seen in Appendix Figure A2. 

### Restricting detections to the center field-of-view
Given that the fish-eye lens may have distorted the edges of the visual field in the videos, we also examined whether we would see improved precision by restricting the included detections to the center field-of-view, and excluding detections that occured in the outer 20\% of each frame. This restricted field-of-view is also more comparable to video data from other head-mounted cameras. However, we found that excluding these detections did not improve the model's precision for either faces or hands; rather, it only decreased recall. Excluding peripheral detections led to an overall f-score of `r round(face_performance_center[3],2)`, for faces with a precision of `r round(face_performance_center[1],2)` and recall of `r round(face_performance_center[2],2)`. For hands, the F-score for center detections was `r round(hand_performance_center[3],2)`, with a precision of `r round(hand_performance_center[1],2)` and recall of `r round(hand_performance_center[2],2)`. Thus, detections that were made in the periphery of these videos were not more likely to be false alarms for either faces or hands. Appendix Figure A3 illustrates this finding, showing the density of false positives for both faces and hands that are concentrated in the center of the field of view. 

### Accuracy as a function of detection confidence
We next examined the degree to which including detections that the model was more or less confident in changed overall model performance for face/hand detection. When we restricted detections to high-confidence face/hand detections (>.5 confidence, default threshold for visualization in OpenPose) was not beneficial -- improving precision but dramatically impairing recall and thus overall performance: the F-score for high-confidence face detections was `r round(face_performance_hc[3],2)`, with a precision of `r round(face_performance_hc[1],2)` and recall of `r round(face_performance_hc[2],2)`; for high-confidence hand detections, the F-score was `r round(hand_performance_hc[3],2)`, with a precision of `r round(hand_performance_hc[1],2)` and recall of `r round(hand_performance_hc[2],2)`. Further analyses sweeping across the entire range of possible confidence detection thresholds revealed that including all detections was the most beneficial for optimizing the rate of true positives (see Appendix Figure A4 for full ROC curves for both face and hand detections).

### Accuracy across child, location, and age
Finally, we examined whether model accuracy for hands and faces varied across age or between child and the location in which the videos were filmed (e.g., kitchen vs. living room). Some of the videos at taken at older ages for child A did seem to show lower hand detection accuracy, perhaps due the presence of child's hands; however, there were also fewer videos at these time points (see plots for age/child in Appendix, Figure A5). We did not find any discernible trends in face/hand detection accuracy across different filming locations (see Figure A6). However, we did still see considerable variation in model accuracy across different videos, suggesting that particular recording activities, lighting conditions, or situations could cause conditions that would impair detector performance. 

Thus, while OpenPose was trained on photographs from the adult perspective, this model still performed relatively well on these challenging the egocentric infant viewpoint with no fine-tuning or post-processing of the detections, especially given these challenging and varied videos. Including all detections made by OpenPose in all locations led to the most accurate model performance. We next use this detection method to analyze the rest of the large dataset, with the caveat that we are likely underestimating the proportion of hands in the infant view.

# Results and Discussion

## Access to social information across age
```{r}
## summary for all detections
face_hand_by_age <- all_vid_data %>%
  ungroup() %>%
  tidyr::replace_na(list(faces_and_hands=0)) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_frames_total = sum(num_frames), 
            # prop faces overall
            num_faces = sum(num_faces), 
            num_hands = sum(num_hands), 
            prop_faces = num_faces / num_frames_total, 
            prop_hands = num_hands / num_frames_total,
            # in center FOV
            num_faces_center = sum(num_faces_center, na.rm=TRUE), 
            num_hands_center = sum(num_hands_center, na.rm=TRUE), 
            prop_faces_center = num_faces_center / num_frames_total, 
            prop_hands_center = num_hands_center / num_frames_total,
            # detailed face info
            prop_full_faces = sum(num_full_faces, na.rm=TRUE)/num_frames_total,
            prop_faces_and_hands = sum(faces_and_hands)/num_frames_total,
            # face/hand contingency
            prop_joint_given_faces = sum(faces_and_hands, na.rm=TRUE)/num_faces, 
            prop_joint_given_hands = sum(faces_and_hands, na.rm=TRUE)/num_hands,
            prop_joint_given_faces_center = sum(face_and_hands_center, na.rm=TRUE)/num_faces_center, 
            prop_joint_given_hands_center = sum(face_and_hands_center, na.rm=TRUE)/num_hands_center)

```

```{r}
#### Make longer format for plotting
all_detections <- face_hand_by_age %>%
  filter(num_frames_total > 5000) %>% # eliminate small data point that skews scaling
  select(prop_faces, prop_hands, num_frames_total, age_day_bin, child_id) %>%
  pivot_longer(cols = c(prop_faces, prop_hands), names_to = "region", values_to = "prop") %>%
  mutate(approach = "uncropped",
         region = ifelse(region == "prop_faces","All faces","All hands"),
         child_name = case_when(child_id =='A' ~ "Child A",
                      child_id == "S" ~ "Child S",
                      child_id == "Y" ~ "Child Y"))  
  
centered <- face_hand_by_age %>%
  filter(num_frames_total > 5000) %>% # eliminate small data point that skews scaling
  select(prop_faces_center, prop_hands_center, num_frames_total, age_day_bin, child_id) %>%
  gather(region, prop, prop_faces_center, prop_hands_center) %>%
  mutate(approach = "centered",
         region = ifelse(region == "prop_faces_center","Faces in center FOV","Hands in center FOV"),
         child_name = case_when(child_id =='A' ~ "Child A",
                      child_id == "S" ~ "Child S",
                      child_id == "Y" ~ "Child Y"))

detailed <- face_hand_by_age %>%
  filter(num_frames_total > 5000) %>%
  select(prop_full_faces, prop_faces_and_hands, num_frames_total, age_day_bin, child_id) %>%
  gather(region, prop, prop_full_faces, prop_faces_and_hands) %>%
  mutate(approach = "detailed",
         region = case_when(region == "prop_full_faces" ~ "Full faces",
                            region == "prop_faces_and_hands" ~ "Both hands & faces"),
         child_name = case_when(child_id =='A' ~ "Child A",
                      child_id == "S" ~ "Child S",
                      child_id == "Y" ~ "Child Y")) 
```



```{r}
first_panel <- ggplot(all_detections, 
       aes(x=age_day_bin, y=prop, 
           size=log10(num_frames_total),
           col=region)) +
  geom_point(alpha=.1) +
  geom_smooth(span=10, aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.5) +
  facet_grid(cols = vars(child_name)) + 
  theme_few(base_size=10) +
  ggthemes::scale_color_solarized(name = "") + 
  theme(legend.text=element_text(size=10)) +
  guides(size = FALSE) +
  theme(legend.position="bottom") +
  ggtitle('A.')

second_panel <- ggplot(centered, 
       aes(x=age_day_bin, y=prop, 
           size=log10(num_frames_total),
           col=region)) +
  geom_point(alpha=.1) +
  geom_smooth(span=10, aes(weight = num_frames_total),linetype = 'dashed', show.legend = TRUE) + 
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.5) +
  facet_grid(cols = vars(child_name)) + 
  theme_few(base_size=10) +
  ggthemes::scale_color_solarized(name = "") + 
  theme(legend.text=element_text(size=10)) +
  guides(size = FALSE) +
  theme(legend.position="bottom")  +
  ggtitle('B.')


third_panel <- ggplot(detailed, 
       aes(x=age_day_bin, y=prop, 
           size=log10(num_frames_total),
           col=region)) +
  geom_point(alpha=.1) +
  geom_smooth(span=10, aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.5) +
  facet_grid(cols = vars(child_name)) + 
  theme_few(base_size=10) +
  scale_color_manual(values = c("#751098","#142c83"), name = "") +
  theme(legend.text=element_text(size=10)) +
  guides(size = FALSE) +
  theme(legend.position="bottom") +
  ggtitle('C.')

```

```{r faceHandDetailed, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=9, fig.cap = "Proportion of frames with (A) All face and hand detections,  (B) Face/hand detections that fell within the center field-of-view (reducing the contribution of children's own hands) and (C) Face detections that were full faces (e.g., eyes, nose, and mouth all visible) and that co-occurred with hands, plotted as a function of age for each child (A, S, and Y). Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range."}

cowplot::plot_grid(first_panel, second_panel, third_panel, labels=NULL, nrow=3, label_fontface = "bold", align = "v", axis = "b")
ggsave('figs_pdfs/test.pdf', width = 7, height = 9)
```

We analyzed the social information in view across the entire dataset, looking specifically at the proportions of faces and hands detected for each child. All analyses and preprocessed data files for this paper are available at tinyurl.com/longitudinal-social-info. Data from videos were binned according to the age of the child (in weeks). First, we saw that the proportion of faces in view showed a moderate decrease across this age range (see Figure \ref{fig:faceHandDetailed}), in keeping with prior findings [@Fausey2016]; in contrast, we did not observe an increase in the proportion of hands in view. 
These effects were quantified with two separate linear mixed-effect models (see Tables 1 & 2).^[Face/hand detections were binned across each week of filming. Participant's age was converted into months and centered for these analyses. Random slopes for the effect of age by child led to a singular fit and were removed from both analyses; see full model specification in accompanying codebase.] After visualizing the data (see Figure \ref{fig:faceHandDetailed}A), we examined whether the addition of quadratic terms relating children's age to the proportion of faces/hands detected would provide better fit to the data than linear terms alone, and found that this was true in both cases (see Tables 1 & 2), though the linear term was also significant for faces. Thus, these exploratory results point towards the idea that some children may experience overall more social information in view in the second year of life. 

However, the most striking result from these analyses is a much greater overall proportion of hands in view than has previously been reported [@Fausey2016]. We found this observation to be true across all ages, in all three children, and regardless of whether we analyzed human annotations (on the 24K random subset, see dotted lines in Appendix Figure A7) or OpenPose annotations on the entire dataset (see Figure \ref{fig:faceHandDetailed}A). This finding is notable especially given that OpenPose showed relatively low recall for hands, indicating that our measurements may in fact be an underestimate of the proportion of hands in view. In fact, analysis of the human gold standard annotations revealed a much higher proportion of hands relative to faces than the automated annotations. 

One reason we could have observed more hands in view than previous studies is the much larger field of view that was captured by the cameras used in this study. These cameras were outfitted with a fish-eye lens in an attempt to capture as much of the children’s field of view as possible, leading to a larger field of view (109 degrees horizontal x 70 degrees vertical) than in many previous studies. For example, in @Fausey2016 the FOV was 69 x 41 degrees. This larger FOV may have allowed the SAYCam cameras to capture not only the presence of a social partner’s hands interacting with objects or gestures, but also the children’s own hands, leading to more frequent hand detections.

As we found that children's hands tended to occur in the lower visual field (see Appendix Figure A2), we thus re-analyzed the entire dataset while restricting our analysis to the center field of view, decreasing the proportion of hand detections from `r round(sum(all_vid_data$num_hands)/sum(all_vid_data$num_frames),2)*100`% to `r round(sum(all_vid_data$num_hands_center,na.rm=TRUE)/sum(all_vid_data$num_frames),2)*100`%, and decreasing face detections from `r round(sum(all_vid_data$num_faces)/sum(all_vid_data$num_frames),2)*100`% to `r round(sum(all_vid_data$num_faces_center,na.rm=TRUE)/sum(all_vid_data$num_frames),3)*100`%. This cropping likely removed both the majority of detections of children's own hands but also some detections of adult hands (see Figure Appendix A2), especially as OpenPose was biased to miss children's hands when they were in view. Nonetheless, within this modified field of view, we still observed more hand detections than face detections (see dashed lines in Figure \ref{fig:faceHandDetailed}). We also still found a higher proportion of hands in view relative to faces when excluding any frames containing child hand's from the human annotated gold sample (see Appendix Figure A7). 

Finally, we analyzed how these two sources of social information co-occurred.
To do so, we calculated the number of frames in which infants saw faces and hands together relative to overall proportions of faces/hands that were detected for each child and age range. Faces and hands were jointly present in `r sum(all_vid_data$faces_and_hands,na.rm=TRUE)/sum(all_vid_data$num_frames)*100` percent of frames (see face hand-occurrences across age in Figure \ref{fig:faceHandDetailed}C). As shown in Figure \ref{fig:faceHandContingency}, all three infants were more likely to see hands independently -- without the presence of a face -- than they were likely to see faces independently. That is, generally speaking when a face was present, a hand also tended to be present. 


```{r eval=T}
summary_by_age_days <- all_vid_data %>%
  group_by(age_days,child_id) %>%
  summarize(num_faces = sum(num_faces), 
            num_faces_center = sum(num_faces_center), 
            num_hands = sum(num_hands), 
            num_hands_center = sum(num_hands_center), 
            num_detect = sum(num_frames)) %>%
  mutate(prop_faces = num_faces / num_detect, 
         prop_hands = num_hands / num_detect,
         prop_faces_center = num_faces_center / num_detect, 
          prop_hands_center = num_hands_center / num_detect)

long_summary_by_age_days <- summary_by_age_days %>%
  select(prop_faces, prop_hands, age_days, child_id) %>%
  pivot_longer(col = c(prop_faces, prop_hands), names_to="region", values_to = "prop")

summary_by_age_days$age_scale <- scale(summary_by_age_days$age_days, scale = FALSE)[,1] / 30.3
long_summary_by_age_days$age_scale <- scale(long_summary_by_age_days$age_days, scale = FALSE)[,1] / 30.3

```

```{r eval=T, include=F, echo=F}
# run lmer models -- faces over age
faces_model1 <- lmer(prop_faces_center ~ age_scale + (1 | child_id), data = summary_by_age_days)
faces_model_full_age_out = summary(faces_model1)
# # singular:
# faces_model2 = summary(lmer(prop_faces_center ~ poly(age_scale, 2) + (poly(age_scale, 2) | child_id),
                           # data = summary_by_age_days))

faces_model2 <- lmer(prop_faces_center ~ poly(age_scale, 2) + (1 | child_id), 
                            data = summary_by_age_days)
faces_comparison <- anova(faces_model1, faces_model2) # chisq 7.17 p = 0.007407 ** - complex model preferred
faces_out <- summary(faces_model2) # poly(age_scale, 2)1 -.20 **, poly(age_scale, 2)2 -.16**


# hands over age
hands_model1 <- lmer(prop_hands_center ~ age_scale + (1 | child_id), data = summary_by_age_days)
hands_model_full_age_out = summary(hands_model1)
hands_model2 <- lmer(prop_hands_center ~ poly(age_scale, 2) + (1 | child_id), 
                            data = summary_by_age_days)
hands_comparison <- anova(hands_model1, hands_model2) # chisq = 17.11, p<.001 - complex model preferred

hands_out <- summary(hands_model2) # poly(age_scale, 2)1  -0.14, p=.06, poly(age_scale, 2)2  -0.32, p<.001

# explicit modeling of faces vs. hands and interaction with age
# main effect of ages and regions; no interaction.
diff_model1 <- lmer(prop ~ region*age_scale + (1 | child_id), data = long_summary_by_age_days)
diff_model_full_age_out = summary(diff_model1)
diff_model2 <- lmer(prop ~ region*poly(age_scale, 2) + (1 | child_id), data = long_summary_by_age_days)
diff_comparison <- anova(diff_model1, diff_model2) # chisq = 23.66, p<.001 - complex model preferred

diff_out <- summary(diff_model2) 
# regionprop_hands .04***, poly(age_scale, 2)1 -.42**, poly(age_scale, 2)2 -.29*, marginal hand*poly(age_scale, 2)2
```


```{r faces-regression-table}
faces_tab <- faces_out$coefficients
rownames(faces_tab)[2] = "Age"
rownames(faces_tab)[3] = "Age**2"
apa_table(faces_tab, digits=3,
           caption = "Coefficients from a mixed-effects regression predicting the proportion of faces seen by infants in the center FOV.")
```

```{r hands-regression-table}
hands_tab <- round(hands_out$coefficients, 3)
rownames(hands_tab)[2] = "Age"
rownames(hands_tab)[3] = "Age**2"
hands_tab[3,5] = "<.001"
apa_table(hands_tab, 
           caption = "Coefficients from a mixed-effects regression predicting the proportion of hands seen by infants in the center FOV.")
```


```{r linear-regression-tables, eval=FALSE, include=F}
# old -- now want the above poly models
xtable(faces_model_full_age_out$coefficients, digits=c(4,3,3,3,3,3),"Model coefficients from a linear mixed model predicting the proportion of faces seen by infants in the center FOV.")

xtable(hands_model_full_age_out$coefficients, digits=c(4,3,3,3,3,3),"Model coefficients from a linear mixed model predicting the proportion of hands seen by infants in the center FOV.")
```

<!-- old linear regression tables
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.098 & 0.010 & 1.956 & 9.386 & 0.012 \\ 
  Age & -0.001 & 0.000 & 430.976 & -3.150 & 0.002 \\ 
   \hline
\end{tabular}
\caption{Model coefficients from a linear mixed model predicting the proportion of faces seen by infants in the center FOV.} 
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Estimate & Std. Error & df & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 0.161 & 0.009 & 1.881 & 18.046 & 0.004 \\ 
  Age & -0.001 & 0.001 & 428.651 & -1.680 & 0.094 \\ 
   \hline
\end{tabular}
\caption{Model coefficients from a linear mixed model predicting the proportion of hands seen by infants n the center FOV.} 
\end{table}
-->



```{r}
face_hand_contingency <- face_hand_by_age %>%
  pivot_longer(c(prop_joint_given_hands, prop_joint_given_faces), values_to="prop", names_to = "region") %>%
  mutate(region = case_when(
    region == "prop_joint_given_faces" ~ "Prop. hands (given face det)",
    region == "prop_joint_given_hands" ~ "Prop. faces (given hand det)"))
    
    # region == "prop_joint_given_faces_center" ~ "Prop. center hands (given center face det)",
    # region == "prop_joint_given_hands_center" ~ "Prop. center faces (given center hand det)"))
```

```{r faceHandContingency, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "Proportion of joint face and hands detection within frames where hands (left) or faces (right) were detected." }
ggplot(face_hand_contingency, 
       aes(x=age_day_bin, y=prop, 
           col=child_id)) +
  geom_point(alpha=.1, aes(size=log10(num_frames_total)), show.legend= FALSE) +
  geom_smooth(method='lm', aes(weight = num_frames_total), show.legend = TRUE) + 
  ylab('Proportion joint hand/face detection') + 
  xlab('Age (Months)') +
  theme_few(base_size=12) +
  facet_grid(~region) +
  # scale_color_manual(values = c( "#DAA0EE","#710D93"), name="") +
  scale_color_brewer(palette="Dark2") +
  guides(size = FALSE) +
  theme(legend.text=element_text(size=10)) 
```



## Variability in social information across learning contexts 
```{r}
load(here::here('data/preprocessed_data_2022/saycam_metadata.RData')) # meta

num_videos_annotated = sum(!is.na(meta$Location))
videos_included = sum(!is.na(meta$Location) & meta$count_locations==1)
```

```{r}
vid_data_by_location <- all_vid_data %>%
  left_join(meta) %>%
  filter(!is.na(Location)) %>%
  filter(count_locations==1) 
# 
vid_data_by_location$Location = droplevels(vid_data_by_location$Location) 

# merge locations that seem similar
levels_to_modify = levels(vid_data_by_location$Location)
levels_to_modify[10] = 'Outside' # Outside on property
levels_to_modify[12] = 'Outside' # Off Property

# reset levels for both cropped/uncropped
levels(vid_data_by_location$Location) <- levels_to_modify

percentage_of_dataset = sum(vid_data_by_location$num_frames)/sum(all_vid_data$num_frames)
num_frames = sum(vid_data_by_location$num_frames)

```


```{r}
face_hand_by_location <- vid_data_by_location %>%
  group_by(Location,child_id,age_day_bin) %>%
  summarize(num_detect = sum(num_frames),prop_faces = sum(num_faces)/num_detect, prop_hands = sum(num_hands)/num_detect,  prop_hands_center =  sum(num_hands_center, na.rm=TRUE)/num_detect, prop_faces_center = sum(num_faces_center, na.rm=TRUE)/num_detect)  %>%
  group_by(Location) %>%
  filter(num_detect > 2000)

## Compute CIs for faces/hands by each location, and then merge back raw data so they can be plotted together
## Get rid of any locations where we didn't have enough data to make a CI (not more than 1 age_bin)
hands_to_plot_centered <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_hands_center") %>%
  mutate(region = "Hands (Center)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_hands_center) %>%
  filter(!is.na(ci_lower)) 

faces_to_plot_centered <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_faces_center") %>%
  mutate(region = "Face (Center)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_faces_center) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter

faces_to_plot_all <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_faces") %>%
  mutate(region = "Faces (All)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_faces) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter

hands_to_plot_all <- face_hand_by_location %>%
  group_by(Location, child_id) %>%
  multi_boot_standard(col="prop_hands") %>%
  mutate(region = "Hands (All)") %>%
  ungroup %>%
  left_join(face_hand_by_location) %>%
  mutate(prop = prop_hands) %>%
  filter(!is.na(ci_lower))  # if we didn't have enough points to make a CI, filter
  
all_to_plot <- faces_to_plot_centered %>%
  full_join(hands_to_plot_centered) %>%
  full_join(hands_to_plot_all) %>%
  full_join(faces_to_plot_all) %>%
  group_by(Location) %>%
  mutate(num_detect_location = sum(num_detect)) %>%
  ungroup() %>%
  mutate(Location = fct_reorder(Location, num_detect_location))
```

```{r DetByLocation, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=5, fig.cap = "Proportion of faces and hands by location in which egocentric videos were filmed; each panel represents data from an individual child (location annotations were not yet available for Y). Each dot represents data from a week in which videos were filmed and are scaled by the number of frames."}

all_to_plot$region <- as.factor(all_to_plot$region)
all_to_plot$region <- relevel(all_to_plot$region, ref ="Faces (All)")

ggplot(all_to_plot, aes(x = Location, y = mean, col=region)) + 
  geom_point(aes(x=Location, y=prop, size=log10(num_detect)), alpha=.1, position = position_dodge(width=.6)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper, linetype=region),position = position_dodge(width=.6)) +
  coord_flip() + 
  ylab('Proportion Detected')+
  xlab('')+
  theme_few(base_size=10) +
  theme(legend.position = "bottom") +
  facet_grid(.~child_id)  +
  scale_size_continuous(name = "log(Detections)") +
  scale_linetype_manual(values=c("solid", "dashed","solid","dashed"), name = "") +
  scale_color_manual(values = c("#268bd2", "#51acec", "#dc322f","#ea5e5c"), name = "") +
  theme(legend.text=element_text(size=8))
  
```


```{r include=FALSE}
### Chi-squared tests for location variability
vid_data_by_location_A <- vid_data_by_location %>%
  filter(child_id == "A") %>%
  group_by(Location) %>%
  summarize(num_faces_by_loc = sum(num_faces), num_no_faces_by_loc = sum(num_frames)-num_faces_by_loc,
            num_hands_by_loc = sum(num_hands),  num_no_hands_by_loc = sum(num_frames)-num_hands_by_loc)  

vid_data_by_location_S <- vid_data_by_location %>%
  filter(child_id == "S") %>%
  group_by(Location) %>%
  summarize(num_faces_by_loc = sum(num_faces), num_no_faces_by_loc = sum(num_frames)-num_faces_by_loc,
            num_hands_by_loc = sum(num_hands),  num_no_hands_by_loc = sum(num_frames)-num_hands_by_loc)  

Faces_A <- cbind(vid_data_by_location_A$num_faces_by_loc, vid_data_by_location_A$num_no_faces_by_loc)
Faces_S <- cbind(vid_data_by_location_S$num_faces_by_loc, vid_data_by_location_S$num_no_faces_by_loc)

Hands_A <- cbind(vid_data_by_location_A$num_hands_by_loc, vid_data_by_location_A$num_no_hands_by_loc)
Hands_S <- cbind(vid_data_by_location_S$num_hands_by_loc, vid_data_by_location_S$num_no_hands_by_loc)

chisq.test(Faces_A)
chisq.test(Faces_S)
chisq.test(Hands_A)
chisq.test(Hands_S)
```


How does the child's context influence the social information in view? 
@bruner1985role discussed the role of children's activities in shaping the information present for learning. Following this idea, we investigated whether there were differences in access to faces and hands by the activity that the child was engaged in. This hypothesis seems intuitively appealing. Some activities seem likely to be characterized by a much higher proportion of faces (e.g., diaper changes) than others (e.g., a car trip). Following this same idea, perhaps other activities involve the presence of more hands in the field of view (e.g., playtime). 
We did not have access to detailed annotations of activity; each video was associated with several different activities. 
Thus, following @roy2015predicting, we used spatial location as a proxy for activity context, taking advantage of the presence of these annotations for a subset of the SAYCam videos. 
In our viewing of the SAYCam videos and in other annotations available with the dataset, activities varied somewhat predictably by location: for example, eating tended to occur in the kitchen, whereas playtime was the dominant activity in the living room. However, every location was associated with multiple activities; see Appendix Figure A8 for a heatmap relating individual activities to locations in the dataset.
Of the `r num_videos` videos in the dataset, `r num_videos_annotated` were annotated for the location or locations they were filmed in. 
These location annotations were only available for two children, S and A.
Annotated locations mostly consisted of rooms of the house (e.g., "living room") but also included some other locations (e.g., "car," "outside").
Of this set, `r videos_included` videos were filmed in only a single location (e.g., the location label did not change within the video), representing 17 percent of the dataset and over 5 million frames. 

Figure \ref{fig:DetByLocation} shows the proportion of faces vs. hands across locations. 
We found substantial variation across locations and, to some extent, across children. 
Separate chi-squared tests for each child and detection type revealed significant variability in detections by location in each case, with all $p$s $<.001$. 
For example, while both A and S saw a relatively similar proportion of faces and hands in the bedroom, the two children saw quite different amounts of faces and hands from one another in the kitchen. 
This difference is likely explained by differences in arrangement of the kitchen in the two children's households (Sullivan, personal communication), such that mealtimes in one kitchen resulted in a face-to-face orientation while they did not in the other. 
This example illustrates how specifics of the geometry of a particular context can play an outsize role in the child's access to social information during that context. 

## Fine-grained changes in the social information in view

```{r}
## construct age bins for plotting
bin_size=7
min_age = min(all_vid_bbs$age_days, na.rm=TRUE)
max_age = max(all_vid_bbs$age_days, na.rm=TRUE)
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age) 

all_vid_bbs <- all_vid_bbs %>%
  mutate(age_months = age_days/30.4) %>%
  mutate(age_day_bin = cut(age_days, bins, labels=round(bin_starts/30,1))) %>%
  mutate(age_day_bin = as.numeric(as.character(age_day_bin)))  %>%
 mutate(avg_center_y = avg_top + avg_height/2, avg_center_x = avg_left + avg_width/2) %>%
  filter(!avg_area==0) # a very few bounding boxes with zero areas (all keypoints on one plane as faces
```


```{r}
# for plotting average area / age for each type 
avg_area_by_age <- all_vid_bbs %>%
  group_by(age_day_bin, child_id, label) %>%
  summarize(avg_area = mean(avg_area), num_detect = sum(num_detect)) %>%
  mutate(label = factor(label, levels = c('pose','face','hand'), labels = c('Full body pose','Faces','Hands'))) %>%
  mutate(child_id = as.factor(child_id))
```

```{r avgSizes, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "Average size of poses, faces, and hands detected in the dataset as a function of age for each child in the dataset (each color = different child). Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range."}

ggplot(avg_area_by_age, aes(x=age_day_bin, y=log(avg_area), col=child_id, size=num_detect)) +
  theme_few(base_size=12) +
  geom_point(alpha=.1) +
  facet_grid(~label) +
  geom_smooth(method='lm') + 
  ylab('Log area (average proportion of frame)') +
  xlab('Age (in months)') +
  scale_color_brewer(palette="Dark2", name = "Child") +
  guides(size = FALSE)
```

```{r}
# eye distance data frame
eye_dist <- all_vid_data %>%
  group_by(child_id, age_day_bin) %>%
  summarize(eye_distance = mean(avg_eye_distance, na.rm=TRUE), num_faces = sum(num_faces)) %>%
  filter(num_faces>2000) %>%
  mutate(variable = 'Eye distance')
```

```{r}
eye_dist_plot <- ggplot(eye_dist, 
       aes(x=age_day_bin, y=eye_distance, 
           col=child_id)) +
  geom_point(alpha=.15, aes(size=num_faces), show.legend= FALSE) +
  geom_smooth(method='lm', aes(weight = num_faces), show.legend = TRUE) + 
  ylab('Avg. distance between eyes') + 
  xlab('Age (in months)') +
  theme_few(base_size=12) +
  facet_grid(~variable) +
  scale_color_brewer(palette="Dark2", name='Child') +
  guides(size = FALSE) +
  ggtitle('A.') +
  theme(legend.text=element_text(size=12))  +
  theme(legend.position="none")
```

```{r} 
CV_faces <- all_vid_bbs %>%
  filter(label=='face') %>%
  group_by(age_day_bin,child_id) %>%
  summarize(cv_x_pos = sd(avg_center_x)/mean(avg_center_x)*100, cv_y_pos = sd(avg_center_y)/mean(avg_center_y)*100, num_detect = sum(num_detect), avg_size = mean(avg_area), num_vids = n(), avg_x_pos = mean(avg_center_x), avg_y_pos = mean(avg_center_y)) %>%
  pivot_longer(cols = c(cv_y_pos, cv_x_pos), values_to = "CV", names_to = "position") %>%
  mutate(position = factor(position, levels = c("cv_x_pos","cv_y_pos"), labels = c("Face X Position", "Face Y Position")))

```


```{r}
CV_faces_plot <- ggplot(CV_faces, aes(x=age_day_bin, y = CV, size=num_detect, color=child_id)) +
  theme_few(base_size=12) +
  geom_point(alpha=.15) + 
  facet_wrap(~position)  +
  geom_smooth(method='lm') +
  ggtitle('B.') +
  scale_color_brewer(palette='Dark2', name='Child') + 
  ylab('Coefficient of variation') +
  xlab('Age (in months)') +
  guides(size = FALSE) 
```


```{r}
# ggplot(CV_faces, aes(x=avg_size, y = CV, size=num_detect, color=child_id)) +
#   geom_point(alpha=.15) + 
#   facet_wrap(~position)  +
#   geom_smooth(method='lm') +
#   ggtitle('B.') +
#   scale_color_brewer(palette='Dark2', name='Child') + 
#   ylab('Coefficient of variation') +
#   xlab('Size of face') +
#   guides(size = FALSE) 
```

```{r}
# perip_faces <- all_vid_bbs %>%
#   filter(label=='face') %>%
#   group_by(age_day_bin,child_id) %>%
#   dplyr::summarize(deviation_from_center_x = abs(mean(avg_center_x) - .5), deviation_from_center_y = abs(mean(avg_center_y) - .5), num_detect = sum(num_detect)) %>%
#   pivot_longer(cols = c(deviation_from_center_x, deviation_from_center_y), values_to = "Deviation", names_to = "position") %>%
#   mutate(position = factor(position, levels = c("deviation_from_center_x","deviation_from_center_y"), labels = c("Face X Deviation", "Face Y Deviation")))
# 
# 
# ggplot(perip_faces, aes(x=age_day_bin, y = Deviation, size=num_detect, color=child_id)) +
#   geom_point(alpha=.15) + 
#   facet_wrap(~position)  +
#   geom_smooth(method='lm', aes(weight = num_detect)) +
#   scale_color_brewer(palette='Dark2', name='Child') + 
#   ylab('Variation from center position') +
#   xlab('Age (in months)') +
#   guides(size = FALSE) 
```


```{r, include=F}
x_pos_summary = summary(lmer(data=CV_faces %>% filter(position == 'Face X Position'), CV ~ scale(age_day_bin) + scale(num_detect) + (1|child_id)))

y_pos_summary = summary(lmer(data=CV_faces %>% filter(position == 'Face Y Position'), CV ~ scale(age_day_bin) + scale(num_detect) +  (1|child_id)))
```


```{r faceDetailed, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=3, fig.cap = "(A) Average distance between eyes and (B) average coefficient of variation for the x and y position of faces detected by OpenPose as a function of each child's age at the time of filming. Data in (A) are restricted to faces where both eyes were detected. Data are binned by each week that the videos were filmed and scaled by the number of face detections in that age range."}

cowplot::plot_grid(eye_dist_plot, CV_faces_plot, rel_widths=c(1,2))
```
In a final set of analyses, we explored fine-grained changes in the SAYCam infants' access to social information across development.
In these analyses, we capitalize on the fact that OpenPose provides not only face and hand detections but also positional keypoints. 
In particular, we explored this keypoint dataset with the idea that greater mobility allows older children to be further from their caregivers on average. 
Thus, younger, less mobile children may tend to see larger faces towards the center of their visual field while older, more mobile children may experience more smaller, more variable views of faces.
The same dynamic would be predicted hold for hands as well, as it would be driven by overall differences in distance. 

Supporting this idea, we found that the averages sizes of the people, faces, and hands in the infant view became smaller over development (Figure \ref{fig:avgSizes}). 
This effect was relatively consistent across the three children in the dataset, despite the fact that the three children showed sometimes disparate overall proportions of faces/hands in view. 
Thus, children may see closer, larger views of people, hands, and faces earlier in development. 

In keeping with this hypothesis, we also found evidence that faces tended to be farther away from older children. 
We restricted our analysis here to faces where both eyes were detected and computed interpupillary distance as a rough metric of distance, since eyes should be closer together on average when a face is further from the camera. 
Figure \ref{fig:faceDetailed}A shows the average interpupillary distance on faces as a function of each child’s age at the time of recording. 
There was a trend from larger, closer faces (with a larger interpupillary distance) to smaller faces that were farther away (with a smaller interpupillary distance). 

```{r}
# Changes in full vs. all faces? --- not significant, inconsistent results across kids
# diff_full_vs_all <- face_hand_by_age %>%
#   mutate(diff = prop_faces - prop_full_faces)
#
# ggplot(diff_full_vs_all, aes(x=age_day_bin, y=diff, color=child_id, size=num_frames_total)) +
#   geom_point(alpha=.2) +
#   geom_smooth(method='lm') d
#
# summary(lmer(data=diff_full_vs_all, diff ~ scale(age_day_bin) + scale(num_frames_total) + (1|child_id)))
```


Finally, we also examined whether there were changes in where faces tended to appear in the camera's (and hence, by proxy, the child's) field of view. 
As expected, faces tended to be located towards the upper field of view, while views of hands were more centrally distributed (see Appendix, Figure A9 for average density distributions). 
However, we also found evidence that older children tended to see more faces in more variable positions than younger children. 
Specifically, we examined how variable the horizontal and vertical coordinates were of the faces in the infant view. 
To do so, we calculated the coefficient of variation of the horizontal (x) and vertical (y) positions of centers of the faces detected by OpenPose (see Figure \ref{fig:faceDetailed}B), and examined changes across age. 
Faces tended to be more variable in the vertical than their horizontal position (see Figure \ref{fig:faceDetailed}B). 
We also found that as children got older, they tended to see faces that varied more in their horizontal -- but not their vertical position -- suggesting that older children might be more likely to see more smaller faces in their periphery (see Figure \ref{fig:faceDetailed}B).


# General Discussion
Here, we validated the use of an automated pose detection model to detect the faces and hands in view in a longitudinal dataset of egocentric, everyday visual experience. We then used these detections to analyze the social information seen from each of three children's egocentric perspective from 6 to 32 months of age. 

First, we found a moderate decrease across age in the proportion of faces in view in the videos, in keeping with previous work [@Fausey2016; @Jayaraman2015]. 
This finding is particularly notable given that, in previous cross-sectional data, this effect seems to be most strongly driven by infants younger than 4 months of age [e.g., @Fausey2016; @Jayaraman2015; @Sugden2014] who see both more frequent and more persistent faces [@Jayaraman2018]. 
We also found this to be true when restricting our analyses to full-field faces, suggesting this effect is not driven by a concurrent shift from more full-view to partial-views of faces.

Second, we also found an unexpectedly high proportion of hands in the egocentric experience of these three infants. We found this to be true even when restricting the analysis to detections in the center field-of-view to make the viewpoints comparable to those of headcams used in prior work and to exclude the portion of the frame most likely to contain children's own hands.
Why might this be the case? One idea is that these videos contain the viewpoints of children not only during structured interactions (e.g., play sessions at home or in the lab) but during everyday activities when children may be playing by themselves or simply observing the actions of caregivers and other people in their environment. 
During these less structured times, caregivers may move about in the vicinity of the child but not interact with them as directly -- leading to views where a person and their hands are visible from a distance, but this person's face may be turned away from the infant or occluded (see examples in Figure 1). 
Indeed, using the same pose detector on videos from in-lab play sessions, @long2021automated found the opposite trend: slightly fewer hand detections than face detections from 8-16 months of age. 
Work that directly examines the variability in the social information in view across more vs. less structured activity contexts could further test this idea.

An analysis based on the location the videos were filmed in further highlights the variability of the social information in view during different activities, showing differences across locations (see Appendix, Figure S7) and between individual children. 
Within a given, well-defined context -- e.g., mealtime in kitchens -- S saw more faces than A, and S saw more faces in the kitchen than in other locations. 
This variability likely stems from the fact that there are at least three ways to feed a young child: 1) sitting in front of the child, facing them as they sit in a high chair; 2) sitting behind the child, holding them as they face outward, and 3) sitting side by side. 
Each of these positions offer the child differing degrees of visual access to faces and hands. 
While the social information in view may be variable across children in different activity contexts, these analyses suggest they could be stable within a given child's day-to-day experience.

We also used these detailed pose annotations to explore finer-grained changes in how children experience the faces and hands of their caregivers over development. 
We found that the faces, hands, and people in the infant view tended to become smaller and that faces tended to be farther away and in more variable horizontal positions, in keeping with prior work examining the sizes of faces in the infant view during the first year of life [@Jayaraman2015; @sugden2017hey; @sugden2014spy; @sugden2019these]. 
Overall, these data support the idea that the social information in view changes across development as infants become increasingly mobile and independent [@franchak2017see; @fausey2016]. 
As children explore the world on their own [@xu2019towards], they may experience fewer close-up interactions with their caregivers and more bouts of play where they are exploring the objects in their environment.

More broadly, however, these analyses underscore the importance of how, when, from whom, and what data we sample; these choices become central when we attempt to draw conclusions about the regularities of experience. 
Indeed, while unprecedented in size, this dataset still has many limitations. 
These videos only represent a small portion of the everyday experience of these three children, all of whom come from relatively privileged households in western societies and thus are not representative in many ways of the global population [@karasik2018ties; @henrich2010most]. 
Any idiosyncrasies in how and when these particular families chose to film these videos also undoubtedly influenced the variability seen here, and may contribute to the individual differences between the three children in this dataset. 
And without eye-tracking data, we do not know the extent to which children are attending to the social information in view.

Nonetheless, we believe that these advances in datasets and methodologies represent a step in the right direction. 
The present paper demonstrates the feasibility of using a modern computer vision model to annotate the entirety of a very large dataset (here, >42M million frames) for the presence and size of people, hands, and faces, representing orders of magnitude more data relative to human annotations in prior work. 
While OpenPose did not provide annotations that were as accurate as those provided by human annotators, we found relatively consistent results with prior literature, suggesting that the sheer scale and density of the annotations provided by this method may overcome some of its limitations. 
Thus, OpenPose may alleviate the burden of manually annotating large egocentric video datasets for the social information in view, helping the field build more generalizable conclusions about how infants experience their early visual social environment.

Of course, these automated detections do not entirely obviate the need for careful inspection and manual annotation of dense video data. While the present results suggest that OpenPose achieves moderate performance at detecting faces and hands in infant egocentric videos -- as in @long2021automated -- these results do not guarantee that OpenPose will generalize with similar accuracy to other video datasets.   While OpenPose does provide a considerable amount of about all of the people in a given image -- estimating the entire poses for every individual -- it may not perform accurately enough to answer certain fine-grained research questions. Going forward, we suggest that researchers continue to compare a small set of manual annotations relevant to their research questions of interest to OpenPose's performance. 

How might pose detection algorithms be improved upon such that they can achieve greater accuracy in analyzing infant egocentric videos?  This step is ripe for collaboration between computer vision experts and developmental psychologists. One promising direction may be to use detailed pose annotations of the people in the infant view to fine-tune later layers of pose detection networks. This has the potential to improve upon the overall accuracy of the network to detect the social information in infant egocentric videos. Standard computer vision models are rarely trained on the egocentric viewpoint, and we suspect that training these models on more naturalistic data may lead to more robust, generalizable detectors. A second potential area for innovation lies in leveraging the dense nature of these large-scale annotations to improve model accuracy: By pooling detections from surrounding frames, we may be able to build classifiers that perform more accurately on a single challenging viewpoint. Finally, a third area for improvement consists of building classifiers that can accurately distinguish between children's hands (as seen from the egocentric point of view) versus adults' hands -- a notoriously challenging problem in computer vision [@bambach2015lending]. We expect that these goals may become more and more feasible as we also improve the head-mounted cameras that are used in both at-home and in-lab studies; increasing the resolution and quality of the images we record will no doubt improve the ability of computer vision algorithms to extract detailed information.

More broadly, we believe that the adaptation of deep neural networks for the infant egocentric view remains a promising avenue for collaboration between computer vision experts and developmental psychologists. 
Indeed, this combination has already yielded new insights about the learning mechanisms needed to build visual representations [@tsutsui2020computational; @zhuang2020unsupervised; @orhan2020self]. 
We propose that the use of novel algorithms with large-scale analysis of dense datasets -- collected with different fields of view, cameras, and from many different laboratories -- will lead to generalizable conclusions about the regularities of infant experience that scaffold learning.

# Acknowledgements
Blinded.

\newpage

# References
```{r create_r-references}
r_refs(file = "library.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\endgroup
