---
title: Appendix to 'Detecting social information in a dense database of infants’ natural
  visual experience'
date: "`r Sys.Date()`"
output:
  pdf_document: default
---


```{r, echo=F, message=F}
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(egg)
library(stargazer)
library(magick)
library(ggeffects)

knitr::opts_chunk$set(echo=F, warning=F, cache=T, 
                      message=F, sanitize = T)
```


```{r}
## Read in preprocessed data 
load(here::here('data/preprocessed_data_2022/all_vid_data_from_bbs_all_detections.RData')) # all detections

## all bounding boxes
load(file=here::here('data/preprocessed_data_2022/bounding_box_summaries.RData'))

```

```{r}
## Load in gold sample detections
load(here::here('data/preprocessed_data_2022/gold_sample_annotations2020-01-31.RData')) # human annotations
load(here::here('data/preprocessed_data_2022/gold_sample_from_bbs_2021_with_centerall_detections.RData')) 

# all_gold_sample_frames_op_hc = load(here::here('data/preprocessed_data_2022/gold_sample_from_bbs_2021_high_conf_detections.RData')) 
```


```{r}
## summary for all detections
face_hand_by_age <- all_vid_data %>%
  ungroup() %>%
  tidyr::replace_na(list(faces_and_hands=0)) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_frames_total = sum(num_frames), 
            # prop faces overall
            num_faces = sum(num_faces), 
            num_hands = sum(num_hands), 
            prop_faces = num_faces / num_frames_total, 
            prop_hands = num_hands / num_frames_total,
            # in center FOV
            num_faces_center = sum(num_faces_center, na.rm=TRUE), 
            num_hands_center = sum(num_hands_center, na.rm=TRUE), 
            prop_faces_center = num_faces_center / num_frames_total, 
            prop_hands_center = num_hands_center / num_frames_total,
            # detailed face info
            prop_full_faces = sum(num_full_faces, na.rm=TRUE)/num_frames_total,
            prop_faces_and_hands = sum(faces_and_hands)/num_frames_total,
            # face/hand contingency
            prop_faces_with_hands = sum(faces_and_hands, na.rm=TRUE)/num_faces, 
            prop_hands_with_faces = sum(faces_and_hands, na.rm=TRUE)/num_hands 
            )


all_detections <- face_hand_by_age %>%
  filter(num_frames_total > 2000) %>% # eliminate small data point that skews scaling
  select(prop_faces, prop_hands, num_frames_total, age_day_bin, child_id) %>%
  pivot_longer(cols = c(prop_faces, prop_hands), names_to = "region", values_to = "prop") %>%
  mutate(approach = "uncropped",
         region = ifelse(region == "prop_faces","Faces","Hands"))  
  

```


```{r}
# Function to evaluate detectors
evaluate_detector <- function(truth, detection) {
  if (truth == TRUE) {
    if (truth == detection) return ("TP") # was face/wrist, detected face/wrist
    else return("FN") # was face/wrist, missed face/wrist
  }
  else {
    if (truth == detection) return("TN") # was not face/wrist, did not detect face/wrist
    else return("FP") # was not face/wrist, detected face/wrist
  }
}

# function to return prfs
 return_prf_short <- function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  p = tp / (tp + fp)
  r = tp / (tp + fn)
  f=( 2 * p * r )/ (p + r)
  return(c(p,r,f))
 }


# join human and OP detections
gold_sample <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op) %>%
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected))

 
 ## output prfs
 face_performance = return_prf_short(gold_sample$face_eval_ketan)
 hand_performance = return_prf_short(gold_sample$hand_eval_ketan)

```

```{r}
## Move to appendix
## get out summary by age for goldset hand-labeled frames
vid_info <- all_vid_data  %>%
  select(child_id, vid_name, age_days, age_day_bin)

summary_by_age_gold <- gold_sample %>%
  left_join(vid_info) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_frames_total = n(), num_faces = sum(face_present_ketan), num_hands = sum(hand_present_ketan), prop_faces= num_faces / num_frames_total, prop_hands = num_hands / num_frames_total)
```


```{r}
### Examine gold sample performance by child hands
load(here::here('data/preprocessed_data_2022/child_adult_hand_annotations_by_frame.RData'))

gold_sample_no_child_hands <- gold_sample %>%
  left_join(child_adult_hand_annotations, by=(c("vid_name","frame"))) %>%
  replace_na(list(child_hand_seg = FALSE)) %>% # replace NAs with false (frames not in annotations (NAs) did not have hands) 
  filter(child_hand_seg==FALSE) %>% # now these are counted as frames where OP didn't need to detect something
  mutate(hand_eval_adults = evaluate_detector(hand_present_ketan, hand_detected))

summary_by_age_adult_hands_gold <- gold_sample_no_child_hands %>%
  left_join(vid_info) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_frames_total = n(), num_faces = sum(face_present_ketan), num_hands = sum(hand_present_ketan), prop_faces = num_faces / num_frames_total, prop_hands = num_hands / num_frames_total)
```

```{r}
goldset <- summary_by_age_gold %>%
  gather(region, prop, prop_faces, prop_hands) %>%
  mutate(approach = "goldset",
         region = ifelse(region == "prop_faces","Faces","Hands"))


goldset_adult_hands <- summary_by_age_adult_hands_gold %>%
  gather(region, prop, prop_faces, prop_hands) %>%
  mutate(approach = "goldset",
         region = ifelse(region == "prop_faces","Faces","Hands"))

```

# Face/hand detections relative to human annotations
```{r goldSetSanity, fig.env="figure*", fig.pos = "h", fig.align = "center", fig.width=7, fig.height=4, fig.cap = "Proportion of faces and hands seen as a function of age for each child in the dataset. Data are binned by each week that the videos were filmed and scaled by the number of frames in that age range. Dashed lines show estimated trend lines from proportion of faces/hands in view when analyzing the gold set of frames made by human annotators. Dotted lines show trend lines from the goldset when frames containing children's own hands were excluded." }

ggplot(all_detections %>% filter(child_id %in% c('S','A')), 
       aes(x=age_day_bin, y=prop, 
           size=log10(num_frames_total),
           col=region)) +
  geom_point(alpha=.2) +
  geom_smooth(span=10, aes(weight = num_frames_total), show.legend = FALSE) + 
  geom_smooth(data = goldset_adult_hands, span=10, aes(weight = num_frames_total), show.legend = FALSE,
              lty = 2, span=10, se = FALSE) +
  geom_smooth(data = goldset, span=10, aes(weight = num_frames_total), show.legend = FALSE,
              lty = 3, span=10, se = FALSE) +
  ylab('Proportion Detections') + 
  xlab('Age (Months)') +
  ylim(0,.6) +
  facet_grid(.~child_id) + 
  theme_few(base_size=10) +
  ggthemes::scale_color_solarized(name = "") + 
  scale_size_continuous(name = "Detections (Log 10)") +
  theme(legend.text=element_text(size=10)) +
  theme(legend.position="bottom") 

```

# Density of child vs. adults hands in the visual field

```{r} 
### Load turk hand annotations with all bounding boxes (multiple dets per frame)
load(here::here('data/preprocessed_data_2022/hand_annotations_2020-01-29.RData'))

child_hands <- hand_annotations %>%
  filter(label=="Child hand") 

adult_hands <- hand_annotations %>%
  filter(label=="Adult hand")
```

```{r}
### plot centers of the bounding boxes made for child and adult hands
child_hand_plot <- ggplot(child_hands, aes(x=center_x, y=center_y)) +
  geom_point(alpha=.1) + 
  stat_density_2d(aes(fill = ..level..), geom="polygon", alpha=.8) +
  coord_fixed(ratio=1) +
  ggtitle('A. Child hand density') +
  ylim(0,480) +
  xlim(0,640) +
  ylab('') +
  xlab('') +
  theme_few(base_size=10) +
  theme(legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  scale_y_reverse()
##
adult_hand_plot <- ggplot(adult_hands, aes(x=center_x, y=center_y)) +
  geom_point(alpha=.1) + 
  stat_density_2d(aes(fill = ..level..), geom="polygon", alpha=.8) +
  coord_fixed(ratio=1) +
  ylim(0,480) +
  xlim(0,640) +
  scale_y_reverse() +
  ylab('') +
  xlab('') +
  theme_few(base_size=10) +
    theme(legend.position="none", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  ggtitle('B. Adult hand density')
```

```{r handLocation, fig.env="figure", fig.pos = "h", fig.align = "center", fig.width=6.5, fig.height=3.5, fig.cap = "Density estimates for the child (left) and adult (right) hands that were detected in the 24K frame random gold set; each dot represents the center of a bounding box made by an adult participant. Brighter values indicate more detections."}
cowplot::plot_grid(child_hand_plot, adult_hand_plot, nrow=1)
```

# Distribution of faces and hands in the visual field
We explored where in the visual field children tended to see faces and hands, suspecting that these distributions might become wider as children grow older and learn to locomote on their own, following preliminary analyses from Frank (2012). 
As expected, faces tended to appear in the upper visual field in contrast to hands, which tended to be more centrally located (see Figure C1). 
However, we found little evidence for any changes in the positions of faces and hands across age, suggesting that this is a relatively stable property of infants’ visual environment from 6 months of age.

```{r}
bin_size=7
min_age = min(all_vid_bbs$age_days, na.rm=TRUE)
max_age = max(all_vid_bbs$age_days, na.rm=TRUE)
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age) 

all_vid_bbs <- all_vid_bbs %>%
  mutate(age_months = age_days/30.4) %>%
  mutate(age_day_bin = cut(age_days, bins, labels=floor(bin_starts/30.4))) %>%
  mutate(age_day_bin = as.numeric(as.character(age_day_bin))) %>%
  mutate(avg_center_y = avg_top + avg_height/2, avg_center_x = avg_left + avg_width/2) %>%
  filter(num_detect > 100)

```


```{r, eval=F, include=F}
# add back to text when high-conf BBs are processed:
# We also evaluated OpenPose's performance while restricting detections to high-confidence detections (>.5 confidence, default threshold for visualization). 
#For faces, we found that the F-score was `r round(face_performance_hc[3],2)`, with a precision of `r round(face_performance_hc[1],2)` and recall of `r round(face_performance_hc[2],2)`. 
#For hands, the F-score for high-confidence detections was `r round(hand_performance_hc[3],2)`, with a precision of `r round(hand_performance_hc[1],2)` and recall of `r round(hand_performance_hc[2],2)`. 
#Thus, as in prior work [@long2020automated], while precision was much higher for both faces and hands, the lower recall for high-confidence detections indicates that these lower-confidence detections still index the presence of social information in the infant view.  
```



```{r faceHandDensity, fig.width=6.5, fig.height=9, fig.cap="Each panel shows the average position of faces and hands in the visual field in a videos from a given age range, i.e., videos when children in the dataset were were 6-31 month-old. Each dot represents the average position from one video within a given age range.", warning=F}

ggplot(all_vid_bbs %>% filter(label %in% c('face','hand')), aes(x=avg_center_x , y = avg_center_y, size=num_detect, color=label)) +
  geom_point(alpha=.3) + 
  stat_density2d(aes(fill = label, color=label), geom="polygon", alpha=.6) +
  coord_fixed(ratio=.76) +
  # ggtitle('Face/hand density') +
  ylab('') +
  xlab('') +
  theme_few(base_size=14) +
  theme(legend.position="top", axis.text.x=element_blank(), axis.ticks.x=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  scale_y_reverse() +
  facet_wrap(~age_day_bin) +
  guides(size = FALSE) +
  ggthemes::scale_color_solarized(name = "")  +
  ggthemes::scale_fill_solarized(name = "") 
```

# Detection threshold estimations

```{r}
# with all thresholds
load(here::here("data","preprocessed_data_2022","gold_sample_from_bbs_2021_all_detections_all_thresholds.RData"))

load(here::here('data/preprocessed_data_2022/gold_sample_annotations2020-01-31.RData')) # human annotations
```

```{r}
evaluate_detector <- function(truth, detection) {
  if (truth == TRUE) {
    if (truth == detection) return ("TP") # was face/wrist, detected face/wrist
    else return("FN") # was face/wrist, missed face/wrist
  }
  else {
    if (truth == detection) return("TN") # was not face/wrist, did not detect face/wrist
    else return("FP") # was not face/wrist, detected face/wrist
  }
}

 return_tpr = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  tn=sum(eval == "TN")
  tpr = tp / (tp + fn)
  return(tpr)
 }

return_fpr = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  tn=sum(eval == "TN")
  fpr = fp / (fp + tn)
  return(fpr)
 }
 
  
```

```{r}
roc_data <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op) %>%
   mutate(hand_detected = hand_detected>0, face_detected = face_detected > 0) %>% # number = number of faces/hands detected
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected)) %>%
  group_by(detection_threshold) %>%
  summarize(tpr_faces = return_tpr(face_eval_ketan), fpr_faces = return_fpr(face_eval_ketan), tpr_hands = return_tpr(hand_eval_ketan), fpr_hands = return_fpr(hand_eval_ketan))
```

```{r}
face_roc = ggplot(data=roc_data, aes(x=fpr_faces, y=tpr_faces, color=detection_threshold)) +
  geom_point() +
  geom_line() +
  ylim(0,1) +
  xlim(0,1) +
  geom_abline(color = 'grey') +
  xlab('False Positive Rate') +
  ylab('True Positive Rate') +
  ggtitle('Face detections') +
  theme(legend.position = 'none')

hand_roc = ggplot(data=roc_data, aes(x=fpr_hands, y=tpr_hands, color=detection_threshold)) +
  geom_point() +
  geom_line() +
  ylim(0,1) +
  xlim(0,1) +
  geom_abline(color = 'grey') +
  xlab('False Positive Rate') +
  ylab('True Positive Rate') +
  ggtitle('Hand detections') +
  theme(legend.position = 'none')

```


```{r roc_curves, fig.width=6.5, fig.height=3.2, fig.cap="ROC curves relating the rate of true vs. false positives for both face and hand detections at each level of detector confidence thresholds (lighter values = stricter cutoffs)."}
ggarrange(face_roc, hand_roc, nrow=1)
```

# OpenPose Error Analyses
```{r}
# remove big data frame with thresholds
rm(all_gold_sample_frames_op)

# without thresholds
load(here::here("data","preprocessed_data_2022","gold_sample_from_bbs_2021_all_detections.RData"))

# join human and OP detections
gold_sample <- gold_sample %>%
  select(vid_name, frame, face_present_ketan, hand_present_ketan) %>%
  mutate(face_present_ketan = as.logical(face_present_ketan), hand_present_ketan = as.logical(hand_present_ketan)) %>%
  right_join(all_gold_sample_frames_op) %>%
  mutate(face_eval_ketan = evaluate_detector(face_present_ketan, face_detected), hand_eval_ketan = evaluate_detector(hand_present_ketan, hand_detected)) 

```

```{r}
# add age day bins to gold_sample
load(file=here::here('data/preprocessed_data_2022/all_vid_data_from_bbs_all_detections.RData')) # all detections
bin_size = 7 # days
min_age = min(all_vid_data$age_days, na.rm=TRUE) # 182
max_age = max(all_vid_data$age_days, na.rm=TRUE) # 966
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age)

gold_sample <- gold_sample %>%
  mutate(age_day_bin = cut(age_days, bins, labels=round(bin_starts/30,1))) %>%
  mutate(age_day_bin = as.numeric(as.character(age_day_bin))) 
```

```{r}
 return_prf = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  p = tp / (tp + fp)
  r = tp / (tp + fn)
  f=( 2 * p * r )/ (p + r)
  num_detect = length(eval)
  out <- data.frame(p,r,f,num_detect)
  return(out)
 }

# function to return prfs
 return_prf_short = function(eval){
  tp=sum(eval == "TP")
  fp=sum(eval == "FP")
  fn=sum(eval == "FN")
  p = tp / (tp + fp)
  r = tp / (tp + fn)
  f=( 2 * p * r )/ (p + r)
  return(c(p,r,f))
 }


```

```{r}
prf_by_age_faces <-  gold_sample %>%
  group_by(child_id, age_day_bin) %>%
  do(return_prf(.$face_eval_ketan))

prf_by_age_hands <-  gold_sample %>%
  group_by(child_id, age_day_bin) %>%
  do(return_prf(.$hand_eval_ketan)) 
```



```{r det_perf_by_age, fig.width=6.5, fig.height=3.2, fig.cap="F-score of OpenPose hand and face detections in the gold sample by child and age. Size indicates number of detections age bin/child."}

prf_by_age_faces$age_day_bin = as.numeric(prf_by_age_faces$age_day_bin)

prf_by_age_hands$age_day_bin = as.numeric(prf_by_age_hands$age_day_bin)

p1 <- ggplot(prf_by_age_faces, aes(x=age_day_bin, y=f, col=child_id, size=num_detect)) +
  geom_point(alpha=.5) +
  geom_smooth(span=20, alpha=.1)  +
  ylim(0,1) +
  ylab('F-score') + xlab('Age (months)') +
  ggtitle("Face detections") + 
  theme(legend.position= 'none')


p2 <- ggplot(prf_by_age_hands, aes(x=age_day_bin, y=f, col=child_id, size=num_detect)) +
  geom_point(alpha=.5) +
  geom_smooth(span=20, alpha=.1)  +
  ylim(0,1) +
  ylab('F-score') + xlab('Age (months)') +
  ggtitle("Hand detections") + 
  theme(legend.position= 'none')
```

```{r}
ggarrange(p1, p2, nrow=1)
```

```{r}
# p2 = ggplot(prf_by_age_faces, aes(x=age_day_bin, y=p, col=child_id, size=num_detect)) +
#   geom_point(alpha=.5) +
#   geom_smooth(span=10, alpha=.1)  +
#   ylim(0,1) +
#   ylab('Precision - Face detection')+
#   xlab('Age (months)') +
#   theme(legend.position= 'none')
# 
# p3 =ggplot(prf_by_age_faces, aes(x=age_day_bin, y=r, col=child_id, size=num_detect)) +
#   geom_point(alpha=.5) +
#   geom_smooth(span=10, alpha=.1)  +
#   ylim(0,1) +
#   ylab('Recall - Face detection')+
#   xlab('Age (months)') +
#   theme(legend.position= 'none')
# #
# ggarrange(p1,p2,p3)

```


```{r}
load(here::here('data/preprocessed_data_2022/saycam_metadata.RData')) # meta

num_videos_annotated = sum(!is.na(meta$Location))
videos_included = sum(!is.na(meta$Location) & meta$count_locations==1)
```

```{r}
gold_sample_by_location <- gold_sample %>%
  left_join(meta) %>%
  filter(!is.na(Location)) %>%
  filter(count_locations==1) 
```

```{r}
prf_by_location_faces <-  gold_sample_by_location %>%
  group_by(Location) %>%
  do(return_prf(.$face_eval_ketan)) %>%
  filter(num_detect > 50)

prf_by_location_hands <-  gold_sample_by_location %>%
  group_by(Location) %>%
  do(return_prf(.$hand_eval_ketan)) %>%
  filter(num_detect > 50)
```


```{r det_perf_by_locations, fig.width=6.5, fig.height=3.0, fig.cap="F-score of OpenPose hand and face detections in the gold sample by location. Size indicates number of detections per location: minimum = 55 (car); maximum = 1753 (living room)."}
p1 <- ggplot(prf_by_location_faces, aes(x=Location, y=f, col=Location, size=num_detect)) +
  geom_point(alpha=.5) +
  ylab('F-score') + ggtitle("Face detections") +
  theme(legend.position = 'none') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0,1)

p2 <- ggplot(prf_by_location_hands, aes(x=Location, y=f, col=Location, size=num_detect)) +
  geom_point(alpha=.5) +
  ylab('F-score') + ggtitle("Hand detections") +
  theme(legend.position = 'none') +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0,1)

ggarrange(p1, p2, nrow=1)
```

# OpenPose Annotation Examples
```{r include=FALSE}
# load(file = here::here('data/preprocessed_data_2022/goldset_category_annotations.RData'))

# fp_face_examples <- d %>%
#   left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#   filter(face_eval_ketan == 'FP')
# 
# fp_hand_examples <- d %>%
#   left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#   filter(hand_eval_ketan == 'FP')
# 
# fn_face_examples <- d %>%
#   left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#   filter(face_eval_ketan == 'FN')
# 
# fn_hand_examples <- d %>%
#   left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#   filter(face_eval_ketan == 'FN')
# 
# images_to_render =  fp_hand_examples
# which_images = 'fp_hand_examples'
# 
# public_urls = unique(images_to_render$public_url)
# img_names_short = unique(images_to_render$img_name)
# 
# dir.create(here::here("data/annotation_examples/", which_images),recursive = TRUE)
# 
#   for (i in 1:length(public_urls)){
#     this_image = image_read(public_urls[i])
#     image_write(this_image,here::here("data/annotation_examples/",which_images, img_names_short[i]))
#   }

```

```{r}
# det_types = c('FP','FN')
# render_examples = FALSE
# if (render_examples == TRUE) {
# for (det_type in det_types) {
#   to_render_hands <- d %>%
#     distinct(vid_name, frame, public_url) %>%
#     left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#     filter(hand_eval_ketan == det_type) %>%
#     ungroup() %>%
#     dplyr::sample_n(7)
# 
#   image_read(to_render_hands$public_url) %>%
#     image_append(stack = FALSE) %>%
#     image_write(paste0(here::here("data/annotation_examples/"), 'example_hands_', det_type,".png"))
#   
#   ###
#     to_render_faces <- d %>%
#     distinct(vid_name, frame, public_url) %>%
#     left_join(gold_sample %>% select(vid_name, frame, face_eval_ketan, hand_eval_ketan)) %>%
#     filter(face_eval_ketan == det_type) %>%
#     ungroup() %>%
#     dplyr::sample_n(7)
# 
#   image_read(to_render_faces$public_url) %>%
#     image_append(stack = FALSE) %>%
#     image_write(paste0(here::here("data/annotation_examples/"), 'example_faces_', det_type,".png"))
#   
# }
# }

```

```{r error_examples, include = T, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=8, fig.height=6, set.cap.width=T, num.cols.cap=2,  fig.fullwidth = TRUE, fig.align = "center", fig.cap = "Example false alarms/misses for faces/hands randomly sampled from the larger set of manual annotations."}
error_examples <- png::readPNG("figs/OP_errors.png")
grid::grid.raster(error_examples)
```

## Location of OpenPose False Positives

Below we visualize the locations of the 1229 faces (in 1093 frames) that OpenPose detected in the gold set frames that were not actually there, as well as the 1778 OpenPose false positive hands (in 1237 frames) from the gold set.
The average location of OpenPose's false positives for both faces (left) and hands (right) are somewhat more common in the center of the frame, but some are scattered across the frame.
Qualitatively, the location of these false positives do not look much different than the true positive detections. 

```{r density-op-fps, fig.width=6, fig.height=3}
load(here::here('data/preprocessed_data_2022/gold_sample_all_indiivdual_dets.RData'))
ddd <- gold_sample_all_dets %>% left_join(gold_sample %>% select(frame, vid_name, face_eval_ketan, hand_eval_ketan))

# density plot of OP's false positive face/hand locations
p1 <- ddd %>% filter(face_eval_ketan=="FP", label=="face") %>%
  mutate(x=left+0.5*width,
         y=1-(top+0.5*height)) %>% # could actually overplot all the face rectangles? prob very messy...
  ggplot(aes(x=x, y=y, fill=..level..)) + 
  stat_density_2d(geom = "polygon") + xlim(-.05,1.05) + ylim(-.05,1.0) + 
  xlab("Mean x-position of OpenPose Detection") + 
  ylab("Mean y-position of OpenPose Detection") + 
  ggtitle("Face False Positives") + theme(legend.position = "none") +
  scale_y_reverse()

p2 <- ddd %>% filter(hand_eval_ketan=="FP", label=="hand") %>%
  mutate(x=left+0.5*width,
         y=1-(top+0.5*height)) %>% # top-left is 0,0
  ggplot(aes(x=x, y=y, fill=..level..)) + 
  stat_density_2d(geom = "polygon") + xlim(-.05,1.05) + ylim(-.05,1.0) + 
  xlab("Mean x-position of OpenPose Detection") + 
  ylab("Mean y-position of OpenPose Detection") + 
  ggtitle("Hand False Positives") + theme(legend.position = "none") +
  scale_y_reverse()

ggpubr::ggarrange(p1, p2)
```


# Activity by Location

```{r activities_by_lo, fig.width=6.5, fig.height=3.0, fig.cap="Counts of activities by location, including multilocation videos."}
meta2 <- read_csv(here::here("data","raw_data","Metadata_SAYcam-2019-12-0922_54_28.csv"))

meta_wide <- meta2 %>% filter(Activity!="N/A", !is.na(Location)) %>%
  select(Child, `File Name`, Activity, Location) %>%
  separate(Activity, c("Act.1","Act.2","Act.3","Act.4","Act.5","Act.6","Act.7","Act.8","Act.9","Act.10","Act.11","Act.12"), sep=',') %>%
  pivot_longer(cols = starts_with("Act."), 
                                        names_to = "activity_index",
                                        names_prefix = "Act.",
                                        values_to = "Activity",
                                        values_drop_na = T) %>%
  mutate(Activity = case_when(Activity=="Waving" ~ "Gesturing",
                              Activity=="Imitation" ~ "Imitating",
                              Activity=="Talking Walk" ~ "Walking",
                              Activity=="Tidying" ~ "Cleaning",
                              Activity=="Preparing for Outing" ~ "Getting Changed/Dressed",
                              Activity=="Play Pretend" ~ "Playing",
                              TRUE ~ Activity)) 

# include videos with multiple locations
meta_long <- meta_wide %>% 
  separate(Location, c("Loc.1","Loc.2","Loc.3","Loc.4","Loc.5","Loc.6"), sep=',') %>%
  pivot_longer(cols = starts_with("Loc."),
               names_to = "location_index",
               names_prefix = "Loc.",
               values_to = "Location",
               values_drop_na = T)

single_locations = c("Bedroom","Car","Deck/Porch","Kitchen","Living Room","Off Property","Office","Outside on Property")

# single-location videos
meta_long_1loc <- meta_wide %>%
  filter(is.element(Location, single_locations)) %>%
  filter(Activity!="N/A", Activity!="Consequence", Activity!="Gesturing")
  
# what is "California Room" ?
meta_long <- meta_long %>% mutate(Location = ifelse(Location=="Bedroom Alice/Sam", "Bedroom", Location))

loc_freq = sort(table(meta_long_1loc$Location))

# multi-location videos
meta_long %>% filter(Activity!="N/A", Activity!="Consequence", Activity!="Kicking",
                     Activity!="Gesturing", Activity!="Look and Listen", 
                     Location!="N/A", Location!="California Room",
                     Location!="Rapidly Moving Between Locations", Location!="Piano Room") %>%
  mutate(Location = ifelse(Location=="Dog Park/Park/Field", "Off Property", Location)) %>%
  group_by(Activity, Location) %>%
  summarise(n = n()) %>% 
  ggplot(aes(x=Activity, y=Location, fill=n)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r, fig.width=6.5, fig.height=3.0, fig.cap="Counts of activities by location in videos taking place in only a single location."}
# single location videos
meta_long_1loc %>% 
  mutate(Location = factor(Location, levels=names(loc_freq))) %>%
  group_by(Activity, Location) %>%
  summarise(n = n()) %>% 
  ggplot(aes(x=Activity, y=Location, fill=n)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
#ggsave("activity_by_location.pdf", width=6.5, height=3.5)
```


```{r, fig.width=6.5, fig.height=3.0, fig.cap="Probability of different activities, given location (in single-location videos)."}
# normalize by location
meta_long_1loc %>% 
  mutate(Location = factor(Location, levels=names(loc_freq))) %>%
  left_join(meta_long_1loc %>% group_by(Location) %>% summarise(loc_freq = n())) %>%
  group_by(Activity, Location) %>%
  summarise(n = n(),
            prop_loc = n / loc_freq) %>% 
  ggplot(aes(x=Activity, y=Location, fill=prop_loc)) +
  geom_tile() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Motor Milestones Analysis Details

We examine whether the SAYcam participants' age of reaching particular developmental milestones (sitting, cruising, and walking) significantly influenced the proportion of hands or faces that they saw, using a mixed-effects regression with per-child random intercepts.

We tried including random slopes by age and milestone, but the models did not converge and/or were singular. Thus, for each milestone the R syntax was: \texttt{prop_faces ~ age * milestone + (1 | child_id)}.Children's age of achieving each milestone was dummy-coded (i.e., 0=milestone not reached; 1=milestone reached).

```{r, get-milestones, echo=F}
motor <- read_csv(here::here("data","raw_data", "saycam_walking_asq.csv"))
# A and S are sitting at 6mos
face_hand_by_age_milestones <- face_hand_by_age %>%
  mutate(sit = ifelse(child_id=="S" | child_id=="A", 1, 
                      ifelse(child_id=="Y" & age_day_bin > 8, 1, 0))) %>%
  mutate(cruise = ifelse((child_id=="S" & age_day_bin >= 9) | 
                      (child_id=="A" & age_day_bin >= 10) | 
                      (child_id=="Y" & age_day_bin > 9), 1, 0)) %>%
  mutate(walk = ifelse((child_id=="S" & age_day_bin > 10) |
                       (child_id=="A" & age_day_bin > 13) |
                       (child_id=="Y" & age_day_bin > 16), 1, 0)) %>%
  mutate(age = age_day_bin)

library(lmerTest)
# https://rpsychologist.com/r-guide-longitudinal-lme-lmer
# with random slopes and intercepts ((age_day_bin | child_id): is singular / not converging
# so we drop random slopes

face_all <- lmer(prop_faces ~ age * (sit + cruise + walk) + (1 | child_id), 
     data=face_hand_by_age_milestones) 
#summary(face_all) # cruise+ age:cruise-

hand_all <- lmer(prop_hands ~ age * (sit + cruise + walk) + (1 | child_id), 
     data=face_hand_by_age_milestones) 
#summary(hand_all) # cruise+ age:cruise-.

face_sit <- lmer(prop_faces ~ age * sit + (1 | child_id), 
     data=face_hand_by_age_milestones) 
hand_sit <- lmer(prop_hands ~ age * sit + (1 | child_id), 
     data=face_hand_by_age_milestones) 

face_cruise <- lmer(prop_faces ~ age * cruise + (1 | child_id), 
     data=face_hand_by_age_milestones)
hand_cruise <- lmer(prop_hands ~ age * cruise + (1 | child_id), 
     data=face_hand_by_age_milestones)

face_walk <- lmer(prop_faces ~ age * walk + (1 | child_id), 
     data=face_hand_by_age_milestones)
hand_walk <- lmer(prop_hands ~ age * walk + (1 | child_id), 
     data=face_hand_by_age_milestones)
# make models compatible with stargazer
class(face_sit) <- "lmerMod"
class(hand_sit) <- "lmerMod"
class(face_cruise) <- "lmerMod"
class(hand_cruise) <- "lmerMod"
class(face_walk) <- "lmerMod"
class(hand_walk) <- "lmerMod"
```

## Sitting

Children's age at which they began sitting did not significantly predict changes in either the amount of faces or hands that they saw.

```{r, include=F, results='asis'}
#summary(face_sit) # ns
stargazer(face_sit, title="Predicting Faces by Ability to Sit", 
          header=F, dep.var.labels = "Proportion of Faces")

stargazer(face_sit, hand_sit)
```

```{r, include=F, results='asis'}
#summary(hand_sit) # ns
stargazer(hand_sit, title="Predicting Hands by Ability to Sit", 
          header=F, dep.var.labels = "Proportion of Hands")
```

```{r, results='asis'}
stargazer(face_sit, hand_sit, title="Change in Face and Hand Prevalence by Ability to Sit", 
          header=F, dep.var.labels = c("Proportion of Faces","Proportion of Hands"), nobs=F)
```

## Cruising

As children became able to cruise, they saw significantly more faces ($\beta = 0.27$) and hands ($\beta = 0.26$. 
For faces, there was a significant negative interaction of age and cruising ($\beta = -0.03$, $p=0.025$), and a marginal positive effect of age ($\beta = 0.03$, $p=0.055$).

```{r, include=F, results='asis'}
#summary(face_cruise) 
# age+., cruise+*, age*cruise-*
stargazer(face_cruise, title="Predicting Faces by Ability to Cruise", 
          header=F, dep.var.labels = "Proportion of Faces")
```


```{r, include=F}
f1 <- ggpredict(face_cruise, c("age", "cruise")) %>% plot(add.data=T) +
  xlim(5, 24) + ylim(0,1) + ggtitle("Predicted effect on faces seen") + 
  xlab("Age (months)") + ylab("Proportion of faces") + theme_classic()

h1 <- ggpredict(hand_cruise, c("age", "cruise")) %>% plot(add.data=T) +
  xlim(5, 24) + ylim(0,1) + ggtitle("Predicted effect on hands seen") + 
  xlab("Age (months)") + ylab("Proportion of hands") + theme_classic()

ggpubr::ggarrange(f1, h1, nrow=1, common.legend=T)
```


```{r, include=F, results='asis'}
#summary(hand_cruise) 
# cruise+*, age*cruise-.
stargazer(hand_cruise, title="Predicting Hands by Ability to Cruise", 
          header=F, dep.var.labels = "Proportion of Hands") 
```

```{r, results='asis'}
stargazer(face_cruise, hand_cruise, title="Change in Face and Hand Prevalence by Ability to Cruise", 
          header=F, dep.var.labels = c("Proportion of Faces","Proportion of Hands"), nobs=F)
```


## Walking

As children became able to walk, they saw marginally more hands ($\beta = 0.09$, $p=0.052$). 

```{r, include=F, results='asis'}
#summary(face_walk) # ns
stargazer(face_walk, title="Predicting Faces by Ability to Walk", 
          header=F, dep.var.labels = "Proportion of Faces")
```


```{r, include=F, results='asis'}
#summary(hand_walk) # walk+.
stargazer(hand_walk, title="Predicting Hands by Ability to Walk", 
          header=F, dep.var.labels = "Proportion of Hands")
```

```{r, results='asis'}
stargazer(face_walk, hand_walk, title="Change in Face and Hand Prevalence by Ability to Walk", 
          header=F, dep.var.labels = c("Proportion of Faces","Proportion of Hands"), nobs=F)
```

