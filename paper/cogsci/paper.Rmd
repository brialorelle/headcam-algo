---
title: "A longitudinal investigation of the social information in natural infant visual experience"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Bria Long (bria@stanford.edu)} 
    \AND {\large \bf George Kachergis (kachergis@stanford.edu)} 
    \AND {\large \bf Ketan Jay Agarwal (agrawalk@stanford.edu)} 
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ 
    Department of Psychology, Street Address \\ Stanford, CA 91305 USA}

abstract: >
    The faces and hands of infants' caregivers and other social partners offer a rich source of social and causal information that may be critical for infants' cognitive and linguistic development. 
    Previous work using manual annotation strategies and cross-sectional data has found systematic changes in the proportion of faces and hands in the egocentric perspective of young infants. 
    The present research aims to test the generality of these findings using the SAYcam dataset [@SAYcam], a longitudinal collection of over 270 hours of headcam videos collected from three children along a span of 6 to 32 months of age. 
    To do so, we validate the use of a modern convolutional neural network for pose detection (OpenPose) for the detection of people, faces, and hands to analyze these naturalistic infant egocentric videos. 
    We then apply this model to the entire dataset, analyzing the prevalence of faces across age, individuals, and activity contexts. 
    Overall, we replicate a decrease the proportion of faces vs. hands across age and find considerably variability in the proportion of faces/hands seen across different locations (e.g., living room vs. kitchen).
    
keywords: >
    social cognition; face perception; infancy; head cameras; deep learning
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---
\newcommand{\wrapmf}[1]{#1}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(lme4)
library(langcog)
library(viridis)
library(magick)
library(stringr)
library(egg)
theme_set(theme_few())
```

# Introduction

Infants are confronted by a blooming, buzzing onslaught of stimuli (James, 1891) which they must learn to parse to make sense of the world around them. Yet infants do not embark on this learning process alone: infants are engaged in learning from their caregivers from early infancy. From as early as 3 months of age, young infants follow overt gaze shifts [@Gredeback2010], and even newborns prefer to look at faces with direct vs. averted gaze [@Farroni2002], despite their limited acuity.  Faces are thus likely to be an important conduit of social information that scaffolds infants cognitive development. Given this importance, cognitive scientists have long hypothesized that faces are prevalent in the visual experience of young infants. However, as even the viewpoint of a walking child is not easily predicted by our own adult intuitions (Clerkin, Hart, Rehg, Yu, & Smith, 2017; Franchak et al., 2011; Yoshida & Smith, 2008), researchers have begun to record the egocentric views collected from infants and toddlers wearing head-mounted cameras to test theories about the infant perspective.

A growing body of work now demonstrates that the viewpoints of very young infants—less than 4 months of age—are indeed dominated by frequent, persistent views of the faces of their caregivers [@Jayaraman2015; @Sugden2014; @Jayaraman2018]. However, as infants mature, their perspective starts to capture views of hands paired with the objects they are acting on [@Fausey2016]. As infants learn to use their own hands to act on the world, they may focus on manual actions taken by their social partners. Furthermore, caregivers may start to use their hands more with communicative intent, directing infants attention with pointing and gestures to particular events or objects during play (Yu & Smith, 2013). 

The present research aims to test the generality of these findings using the SAYcam dataset [@SAYcam], a longitudinal collection of over 1700 headcam videos collected from three children along a span of 6 to 32 months of age. In addition to its size and longitudinal nature, this dataset is more naturalistic on those used in previous research in two key ways. First, recordings were from a variety of many contexts, encompassing infants’ viewpoints during both activities outside and inside the home. Even in other naturalistic datasets,  the incredible variety in a typical infant’s experience has been largely underrepresented; here, we analyze scenes from a large variety of activity contexts that evolve as children get older (see examples in Figure 1; e.g., riding in the car, gardening, watching chickens during a walk, browsing magazines, nursing, brushing teeth).  Second, the head-mounted cameras used in this longitudinal study captured a much wider field of view than those typically used, allowing a more complete picture of the infant perspective.

However, with hundreds of hours of footage (>30M frames), this large dataset truly necessitates a shift to an automated annotation strategy. Indeed, annotation of the frames extracted from egocentric videos has been to be prohibitively time-consuming, meaning that many of the frames are not inspected. For example, @Fausey2016, collected a total of 143 hours of head-mounted camera footage (15.5 million frames), of which one frame every five seconds was hand annotated (by four coders), totalling 103,383 frames (per coder)—an impressive number of annotations but nonetheless only 0.67% of the collected footage. To address this challenge, we use a modern computer vision model of pose detection [@Cao2018openpose] to automatically detect the presence of hands and faces from the infant egocentric viewpoint. Specifically, we use OpenPose [@Cao2018openpose], a model optimized for jointly detecting human face, body, hand, and foot keypoints that operates well on scenes including multiple people even if they are partially-occluded (see Figure 1).  We then apply this method at scale in the larger dataset, allowing us to analyze the proportion of faces and hands observed by each child across age and activity context.

In the following paper, we first describe the dataset and validate the use of this model by comparing face and hand detections to a human-annotated set of 24,000 frames. Next, we report how the proportion of faces and hands changes across age (30M frames), finding a greater prevalence of hands than has been previously reported.  We then investigate sources of variability in our more naturalistic dataset that may explain differences from prior work, including a diversity of locations and activities during which the infant perspective was recorded.



```{r}
## Read in data 
### Load preprocessed and filtered detections
load('../../data/openpose_detections_filtered.RData') #d_all
load('../../data/openpose_detections_filtered_cropped.RData') # d_cropped
```

```{r}
### Make bins across which to analyze data
bin_size = 7
min_age = min(d_all$age_days)
max_age = max(d_all$age_days)
bin_starts = seq(min_age-1, max_age+1,bin_size)
bins = c(bin_starts, max_age)

d_all <- d_all %>%
  mutate(age_day_bin = cut(age_days, bins, labels=round(bin_starts/30,1)))
d_all$age_day_bin = as.numeric(as.character(d_all$age_day_bin))
```

```{r}
## Make summary stats
summary_by_age <- d_all %>%
  mutate(age_day_bin = cut(age_days, bins, labels=round(bin_starts/30,1))) %>%
  mutate(age_day_bin = as.numeric(as.character(age_day_bin))) %>%
  group_by(age_day_bin, child_id) %>%
  summarize(num_detect = length(face_openpose), prop_faces = sum(face_openpose) / num_detect,  prop_hands = sum(hand_openpose) / num_detect, face_or_hand = sum(person_openpose)/ num_detect, face_and_hand = sum(both_openpose)/ num_detect) 

summary_by_age_cropped <- d_cropped %>%
  group_by(age_day_bin,child_id) %>%
  summarize(num_detect = length(face_openpose), prop_faces = sum(face_openpose) / num_detect,  prop_hands = sum(hand_detected_cropped) / num_detect) 
```


```{r} 
### How large is the dataset we're analyzing?
num_frames = length(d_all$frame)

fps=30
num_seconds = length(d_all$frame)/fps
num_videos = length(unique(d_all$vid_name))
num_minutes = num_seconds/60
num_hours = num_minutes/60
num_days = num_hours / 24 
```

# Method

## Dataset

Children wore headcams at least twice weekly, for approximately one hour per recording session.  
One weekly session was on the same day each week at a roughly constant time of day, while the other(s) were chosen arbitrarily at the participating family’s discretion. 
At the time of the recording, all three children were in single-child households. 
Videos captured by the headcam were 640x480 pixels, and a fisheye lens was attached to the camera to increase the field of view to 109 degrees horizontal x 70 degrees vertical. 
Videos[^1] with technical errors or that were not taken from the egocentric perspective were excluded from the dataset.
While data collection for the third child (Y) is still ongoing, here we analyze `r num_videos` videos, with a total duration of `r num_hours` hours (>29 million frames).

[^1]: All videos are available at https://nyu.databrary.org/volume/564 


## Part 1: How well can we capture social information using computer vision?

### Computer vision model
To automatically annotate the millions of frames in SAYcam, we use OpenPose [@Cao2018openpose; @Simon2017hand], a computer vision model optimized for jointly detecting human face, body, hand, and foot keypoints (135 in total) that operates well on scenes including multiple people even if they are partially-occluded. 
This convolutional neural network (CNN)-based pose detector[^2] provided the locations of 18 body parts (ears, nose, wrists, etc.).
The system uses a convolutional neural network for initial anatomical detection and subsequently applies part affinity fields for part association, producing a series of body part candidates. 
The candidates are then matched to a single individual and finally assembled into a pose. Thus, while we only make use of the outputs of the face and hand detections, the pose information from an individual is used to determine the presence of a face/hand.
Specifically, we tagged frames with detected nose keypoints as having a face, and frames with a wrist keypoint were tagged as having a hand present.

[^2]: https://github.com/CMU-Perceptual-Computing-Lab/openpose

### Manual annotation strategy
```{r}
# load('../../data/ketan_gold_sample.RData')
```
To test the validity of OpenPose's hand and face detections, we compared the accuracy of these detections relative to human annotations of 24,000 frames selected uniformly at random from videos of two children (S and A).
Frames were jointly annotated for the presence of faces and hands: raters were asked to indicate presence if at least 20% of one of these body parts was visible.
A second set of coders recruited via AMT (Amazon Mechanical Turk) additionally annotated 3150 frames; agreement with the primary coder was 95.04%. 

### Detection accuracy for faces and hands

To evaluate OpenPose's performance, we compared its detections to the manually-annotated gold set of 24K frames, calculating Precision (hits / hits + false alarms), Recall (hits / hits + misses), and F-score (the harmonic mean of precision and recall).
For faces, the F-score was 0.64, with a precision of 0.70 and recall of 0.58. 
For hands, the F-score was 0.51, with a precision of 0.73 and recall of 0.40.
While faces and hands have moderately good precision, face detections were overall slightly more accurate than hand detections. Hand detections suffered from fairly low recall, indicating that OpenPose likely underestimates the proportion of hands in the dataset.
We also examined whether precision, recall, and F-score for hands and faces varied per child as a function of age to see if there was any bias in the OpenPose detections related to age.
The scores for faces were steady across age and similar for both children, but the scores for hands declined somewhat for the last few months.

## Part 2: Access to social information across age

Next, we analyzed the social information in view across the entire dataset, looking specifically at the proportions of faces and hands that were in view for each child. 
Data from videos were binned according to the age of the child (in weeks) (see Figure 2). 
First, we saw that the proportion of faces in view showed a moderate decrease across this age range, both when analyzing the random 24K frames as well as the entire dataset. 
Note that while these trends appear somewhat different than those observed in @Fausey2016, note that @Fausey2016 included data from very young infants (from 1-4 months of age), while here the youngest videos coming from S and A around 6 and 9 months of age, respectively. 
Similarly, our age range extends 8 months later than those infants in @Fausey2016, throughout a portion of third year of life. 

```{r FaceVsHands, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=6, fig.height=3, fig.cap = "Proportion of faces vs. hands in the entire dataset (A) and when hand detections are restricted to the upper 60% of the field of view (B). Data from each child (A, S) are plotted separately and binned by each week that the videos were filmed; data is scaled by the number of frames in that age range."}

(face_v_hand_cropped <- ggplot(summary_by_age_cropped, aes(x=age_day_bin, y=prop_faces - prop_hands, size=num_detect, col=child_id)) +
  geom_point(alpha=.4) +
  geom_smooth(span=10) + 
  theme_few(base_size=18) +
  ylab('Proportion Faces - Hands') + 
  xlab('Age (months)') +
  ylim(-.3, .3) +
  ggtitle('(B). Upper 60% of FOV'))


(face_v_hand <- ggplot(summary_by_age, aes(x=age_day_bin, y=prop_faces - prop_hands, size=num_detect, col=child_id)) +
  geom_point(alpha=.4) +
  geom_smooth(span=10) + 
  theme_few(base_size=18) +
  ylab('Proportion Faces - Hands') + 
  xlab('Age (months)') +
  ylim(-.3, .3) +
  theme(legend.position = "none") +
  ggtitle('(A). Entire FOV'))

ggarrange(face_v_hand, face_v_hand_cropped, nrow=1)
```

However, the most striking result from the dataset is a much greater proportion of hands in view than have previously been reported. 
We found this to be true across all ages, in both children, and regardless of whether we analyzed human annotations (on the 24K random subset) or the entire dataset.   
One reason this could be the case is the much larger field of view that was captured by the cameras used in this study: unlike previous studies, our cameras were outfitted with a fish-eye lens in an attempt to capture as much of the children’s field of view as possible.  
Thus, the field of view (FOV) of the fisheye lens used in @SAYcam was much wider (109 degrees horizontal x 70 degrees vertical) than the FOV of the lens used in many previous studies; for example, in @Fausey2016 the FOV was 69 x 41 degrees.
This larger field of view may have allowed the SAYcam cameras to capture not only the presence of a social partner’s hands interacting with objects, but also the children’s own hands, leading to more frequent hand detections.

```{r}
load('../../data/child_adult_hand_annotations.RData')
```

To assess this possibility, we obtained annotations for a subsample of `r length(child_adult_hand_annotations$frame)` frames in which a hand was detected in the random gold set; participants (recruited via AMT) were asked to draw bounding boxes around children’s hands and adult hands. 
Overall, we found that `r mean(child_adult_hand_annotations$child_hand_seg)` percent of the hands detected by OpenPose in the random 24K sample were of children’s own hands (compared to 8 percent reported in @Fausey2016) suggesting that this difference in field of view did contribute substantially to the higher rate of hand detections in the frames. 
<!-- Heatmaps of the bounding boxes obtained from these annotations can be seen in Figure 3, showing that children’s hands tended to appear in the lower half of the frames. -->

We thus re-analyzed the entire dataset while restricting our analysis to a smaller, upper portion of the frame comparable to the field of view used in @Fausey2016.
To do so, we excluded hand detections that occurred in the bottom 40 percent of the frame, while retaining all hand detections that occurred in the top 60% of the frame.  This coarse cropping of decreased the proportion of hand detections from `r mean(d_all$hand_openpose)`percent to `r mean(d_cropped$hand_detected_cropped)` percent. 

With this modified field of view, we now observed a more substantive decrease in the proportions of faces relative to hands. To quantify this trend, we fit a generalized linear mixed model to the binned detection rates from this smaller field of field to estimate changes in the proportion of faces vs. hands over age. To better approximate @Fausey2016, we also restricted our analysis to the age range seen in @Fausey2016, excluding videos when children were over 24 months of age, and indeed found that the proportion of faces vs. hands declined modestly with age (see Table 1). 

However, when we considered the entire age range, we also observed an unexpected trend in which the relative proportion of faces vs. hands increased during the third year of life; this trend was present in both the full dataset and when only considering detections in the upper portion of the frames. We discuss possible factors that could cause this trend in the Discussion


### Variability by Location 
```{r}
load('../../data/child_adult_hand_annotations.RData')
load('../../data/saycam_metadata.RData')

num_videos_annotated = sum(!is.na(meta$Location))
videos_included = sum(!is.na(meta$Location) & meta$count_locations==1)

# percentage_of_dataset = sum(faces_vs_hands_to_plot$num_detect)/length(d_all$frame)
# num_frames = sum(faces_by_location_to_plot$num_detect)
```
How does variability across different contexts influence the social information in the infant view? 
HIntuitively, some activities in different contexts may be characterized by a much higher proportion of faces (e.g., diaper changes in bedrooms) than others (e.g., playtime in the living room). 
We thus next examined variation in presence of hands and faces across different locations. 
Of the `r num_videos` videos, `r num_videos_annotated` were annotated [@SAYcam] for the location o videos were filmed in. `r videos_included` were filmed in single location, representing 17% of the dataset and over 5 million frames (see @SAYcam). 

<!-- Not sure video count is the rgiht metric here @George -- eliminating for now... -->
<!-- To give a sense of the contexts the children experienced, the most frequent filming locations were the living room (339 videos), bedroom (182), kitchen (150), outside on property (129), child's bedroom (81), deck/porch (73), hallway (70), and off property (57).  The most frequent activities were sitting (410), playing (375), being held (352), and standing (297); eating was the 11th most-frequent activity (117 videos).  -->

Activities varied somewhat predictability by these contexts: for example, eating occured in the kitchen, whereas playtime was the dominant activity in the living room. Overall, we found that the proportion of faces vs. hands varied across filming locations, and, to some extent, across children (Figure 3).For example, while both children saw a relatively similar proportion of faces vs. hands in the bedroom, they saw quite different amounts of faces vs. hands in kitchen. 

```{r}
face_hand_by_location_cropped <- d_cropped %>%
  left_join(meta) %>%
  filter(!is.na(Location)) %>%
  filter(count_locations==1) %>%
  group_by(Location,child_id,age_day_bin) %>%
  summarize(prop_faces = mean(face_openpose), prop_hands = mean(hand_detected_cropped), num_detect = length(hand_detected_cropped))  %>%
  filter(num_detect > 9000) %>% ## 9000 frames = 5 minutes of video at 30 fps %>%
  mutate(faces_vs_hands = prop_faces - prop_hands)

faces_vs_hands_to_plot_cropped <- face_hand_by_location_cropped %>%
  multi_boot_standard(col="faces_vs_hands") %>%
  ungroup %>%
  left_join(face_hand_by_location_cropped) %>%
  filter(!is.na(ci_lower)) %>% # if we didn't have enough points to make a CI, filter
  mutate(Location = fct_reorder(Location, mean)) 
```

```{r DetByLocation, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=3, fig.height=3, fig.cap = "Proportion of face vs hand detections around in different activity contexts. Error bars represent non-parametric bootstrapped 95 \\% confidence intervals across weekly age bins; each datapoint is scaled by the number of frames included."}

ggplot(faces_vs_hands_to_plot_cropped, aes(x = Location, y = mean, col=child_id)) + 
  geom_point(aes(x=Location, y=faces_vs_hands, size=num_detect), alpha=.2, position = position_dodge(width=.3)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),position = position_dodge(width=.3)) +
  # ylim(-.4,.4) +
  coord_flip() + 
  ylab('Proportion Faces - Hands')

```

# Discussion 

Broadly, analyzing this dataset has yielded a better understanding of infants' evolving access and attention to social information. Despite the considerable variability across different types of activity contexts and age ranges, we still found a moderate decrease in the relative proportion of faces and hands across age, documenting this longitudinal trajectories in individual children. This is particularly notable given that this effect seems to be most strongly driven by infants younger than 4 months of age (e.g., Jayaraman, Fausey, & Smith, 2015; Sugden, Mohamed-Ali, & Moulson, 2014) who see both more frequent and more persistent faces (Jayaraman & Smith, 2018).  Nonetheless, infant’s emerging abilities to act on the world and locomote independently continues to evolve across the first few years of life, and these advances in motoric abilities may be a driving factor that influences infants’ access to social information (Sanchez, Long et al., 2018).

Furthermore, we demonstrate the feasibility of using a modern, off-the-shelf computer vision models to vastly increase the efficiency of processing egocentric headcam footage, allowing us to annotate the entirety of very large datasets (here, >30 million frames) for the presence and size of people, hands, and faces, representing a 300-fold increase in data relative to prior work (@Fausey2016). To do so, we validate the use of this model by comparing to a human-annotated gold set, finding moderate precision and accuracy. While OpenPose was trained on photographs and videos taken by adults, this model still generalized relatively well to the egocentric infant viewpoint with no fine-tuning or post-processing of the detections. 

Indeed, as these detections were imperfect compared to human annotators, fine-tuning these models to better optimize them detecting social information from the infant viewpoint remains an open avenue for future work. For example, infant perspective often contains non-canonical viewpoints of faces (e.g., looking up at a caregiver's chin) as well as partially-occluded or oblique viewpoint. Finally, we noted that human annotators and OpenPose alike were sometimes unreliable about labeling faces seen in books that children were looking at—arguably a common part of many children’s visual experience with hands and faces. As standard computer vision models are rarely exposed on these naturalistic, egocentric viewpoints, we suspect that training these models on more naturalistic data may also lead to more robust, generalizable detectors.



# Acknowledgements

We would like to thank X and Y for helpful comments, and...

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
