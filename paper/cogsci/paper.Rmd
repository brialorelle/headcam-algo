---
title: "Quantifying social information in natural infant visual experience"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Bria Long (bria@stanford.edu)} 
    \AND {\large \bf George Kachergis (kachergis@stanford.edu)} 
    \AND {\large \bf Ketan Jay Agarwal (agrawalk@stanford.edu)} 
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ 
    Department of Psychology, Street Address \\ Stanford, CA 91305 USA}

abstract: >
    The faces and hands of infants' caregivers and other social partners offer a rich source of social and
    causal information that may be critical for infants' cognitive and linguistic development.
    
    
keywords: >
    social cognition; face perception; infancy; head cameras; deep learning
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

# Introduction

Infants are famously confronted by a blooming, buzzing onslaught of stimuli (James, 1891) which they must learn to parse and navigate as their cognitive and social skills develop.
Fortunately, they can depend on regularities not only in their visual environments [@Aslin2009] and linguistic environments, but also in the presence and actions of their caregivers and other social partners [@].
From a very young age, infants show great attention to faces CITE, and indeed faces are a critical part of infants' visual experience as an important conduit of social and linguistic information.
As infants mature, they begin to also attend more to the manual actions taken by their social partners, engaging in joint attention to objects and events around them. 
Such episodes are prompted not only by a glance from a caregiver, but also through pointing and offering; thus, hands are also an important carrier of information, especially relevant for learning language and actions.
It is unsurprising, then, that both hands and faces are prevalent in even young infants' visual experiences, as evidenced through analyses of egocentric views collected with head-mounted cameras (i.e., headcams; CITES).

Of course, not only is children's interest and attention to hands and faces

Previous work has suggested:

Previous work has found changes in the prevalence of faces vs. hands for infants of different ages. For example, Fausey et al. [-@Fausey2016] found that infants less than 12 months of age received face-dense input, relative to 1- to 2-year-olds who received more hand-dense input. 
However, it may be that this effect is driven by infants younger than 4 months of age [e.g., @Jayaraman2015; @Sugden2014] who see both more frequent and more persistent faces [@Jayaraman2018].
Once infants begin to crawl (~6 months) they may see far fewer hands and faces, overall.

Earlier work has also found surprising attention to hand movements and their interactions with objects (Yu & Smith, 2013), particularly in older infants (ages?).

Differences in the availability of social information depending on motor abilities [@Sanchez2018; @Franchak2011] (more Franchak papers?)

One limitation of past work is that it has relied on cross-sectional data, and thus cannot speak to whether these trajectories are present in individual children.

Here, we analyze the SAYcam dataset [@SAYcam], a longitudinal corpus of head-mounted camera data comprising more than 1700 videos from three children, for a total of over 300 hours of videos (>100 million frames). 
Over a span of 6 to 32 months of age, the three children (S, A, and Y) in the dataset wore headcams at least twice weekly, for approximately one hour per recording session. 
One weekly session was on the same day each week at a roughly constant time of day, while the other(s) were chosen arbitrarily at the participating family's discretion.

This dataset differs in four key ways: 1) number of hours/frames 2) not just mealtimes--naturalistic sample of many contexts, 3) longitudinal, 4) much larger field-of-view.

To do so, we first test and validate novel computer vision methods for extracting social information from these egocentric viewpoints on a small subset of randomly selected frames from the dataset.  
We then apply these methods at scale to the larger dataset, allowing us to extract key descriptive variables hypothesized to vary across development.

# Method

## Dataset

(briefly describe dataset; sampling strategy: location of two households, number of hours of video, variability in location, etc; reference published paper on what this dataset is, large field of view (fisheye lens))
(109 degrees horizontal x 70 degrees vertical)

## Part 1: How well can we capture social information using computer vision?

### Description of OpenPose (Figure 1) 
To automatically annotate the millions of frames in SAYcam, we use OpenPose [@Cao2018openpose; @Simon2017hand], a computer vision model optimized for jointly detecting human face, body, hand, and foot keypoints (135 in total) that operates well on scenes including multiple people even if they are partially-occluded.
  
### Description of annotation strategy (24K by Ketan, 4K on Amazon Mechanical Turk, reliability)
To test the validity of OpenPose's hand and face detections, we compared to human annotations of 24,000 frames selected uniformly at random from the videos of two children (S and A). 
  
### Describe main PRF statistics for 24K for faces and hands; interpret.
Relatively higher precision vs. recall.
- P/R/F variation across child/age for faces
- Describe possible sources of variation that decrease scores for:
    - Faces: weird viewpoints, occluded/side viewpoint, faces in books
    - Hands: children's own hands, hands in books, side viewpoints
- Describe additional child vs. hand annotation; P/R/F variation across child vs. adult hands (better for adult hands, still OK for child hands)

## Part 2: Access to social information across age

### Prevalence of hands vs faces across age (in goldset, full dataset) (Figure 2)

### Why so many hands? 

### More child hands as in gold set


## Field-of-View Comparison

The field of view (FOV) of the fisheye lens used in @SAYcam is much wider (109 degrees horizontal x 70 degrees vertical) than the FOV of the lens used in @Fausey2016 (69 deg. x 41 deg.). 
Looks like child hands make up about ~34% of the hands detected in our gold set (in Fausey 2016, they are only 8% of the hands). 
Furthermore, a lot of the lower proportions of hands come from the infants <6 months of age.

## Variability by Location 

Next we examine variation in the presence of hands and faces across different locations.
Of the 3,027 videos, the content of 1,829 have been manually manuallly annotated for filming location, activities taking place, and visible objects (see @SAYcam). 
To give a sense of the contexts the children experienced, the most frequent filming locations were the living room (339 videos), bedroom (182), kitchen (150), outside on property (129), child's bedroom (81), deck/porch (73), hallway (70), and off property (57). Filming only took place twice in the dining room.

The most frequent activities were sitting (410), playing (375), being held (352), and standing (297). Eating was the 11th most-frequent activity (117 videos).

(goldset, full dataset)



List multiple references alphabetically and
separate them by semicolons [@Frank2012; @Smith2011]. 

You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`.


## One-column images

Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}
img <- png::readPNG("figs/lab_logo_stanford.png")
grid::grid.raster(img)
```


## R Plots

You can use R chunks directly to plot graphs. And you can use latex floats in the
fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)**

```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" }
x <- 0:100
y <- 2 * (x + rnorm(length(x), sd = 3) + 3)

ggplot2::ggplot(data = data.frame(x, y), 
                aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


## Tables


You can use the xtable function in the xtable package.

```{r xtable, results="asis"}
n <- 100
x <- rnorm(n)
y <- 2*x + rnorm(n)
out <- lm(y ~ x)

tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2), 
                       caption = "This table prints across one column.")

print(tab1, type="latex", comment = F, table.placement = "H")
```

# Discussion 

Fausey 2016: 103,383 images; 
Here: 30,000,000 frames; 300 fold increase in data

# Acknowledgements

We would like to thank X and Y for helpful comments, and...

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
