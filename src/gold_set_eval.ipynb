{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Importing FaceDetector...\n",
      "['/home/users/agrawalk/miniconda2/envs/headcam/lib/python36.zip', '/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6', '/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/lib-dynload', '', '/home/users/agrawalk/.local/lib/python3.6/site-packages', '/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages', '/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages/IPython/extensions', '/home/users/agrawalk/.ipython']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook contains code for drawing face-detected and random samples from a video, \n",
    "annotating on face/no face, calculating precision, recall, and F-score, \n",
    "and visualizing the scores across detectors, groups, and videos.\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import ntpath\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import collections  as mc\n",
    "\n",
    "print('Importing FaceDetector...')\n",
    "from config import *\n",
    "from detector import FaceDetector\n",
    "from utils import (format_num, create_sample_json, annotate_sample, \n",
    "                   run_detector_on_sample, incorporate_openpose_output, \n",
    "                   calc_prf, display_prf, display_prf2, submit_sbatch, run_openpose)\n",
    "\n",
    "print(sys.path) #should somewhere include [...]/miniconda2/envs/headcam/[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'Submitted batch job 44773571\\n', b'')\n"
     ]
    }
   ],
   "source": [
    "#1. Extract frames from video\n",
    "\n",
    "for vid_path in ['NEW_VID_PATHS']:\n",
    "    msg = submit_sbatch(f'python extract_frames.py {vid_path}', job_name='extract', p='normal,hns', t=2)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          44971496       gpu demovide agrawalk PD       0:00      1 (Priority)\n",
      "          44956279       hns  jupyter agrawalk  R    4:45:05      1 sh-108-15\n"
     ]
    }
   ],
   "source": [
    "#Confirm jobs are submitted\n",
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Run MTCNN on 10000 frames of each video\n",
    "#TODO: add case to check if output json exists, and ask for confirmation to overwrite\n",
    "\n",
    "for frame_dir in FRAME_DIRS:\n",
    "    msg = submit_sbatch(f'python detect_faces_simple.py {vid_path} {MASTER_JSON_PATH}', job_name='extract', p='normal,hns', c=8, t=2)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. select a random sample of [sample_size] face-detected, [sample_size] random frames from each video in the dataframe\n",
    "#e.g. if 6 videos in JSON and sample_size=200, creates a sample dataframe of size (200 + 200)*6 = 2400 frames\n",
    "#TODO: add case to check if output json exists, and ask for confirmation to overwrite\n",
    "\n",
    "create_sample_json(MASTER_JSON_PATH, SAMPLE_JSON_PATH, sample_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a. run + add detections for additional detectors to sample dataframe.\n",
    "\n",
    "for det_name in ['vj']:\n",
    "    run_detector_on_sample(det_name, OUTPUT, SAMPLE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "overwrite existing directory /scratch/users/agrawalk/demo/demovideo? (yes/no) yes\n"
     ]
    }
   ],
   "source": [
    "run_openpose(DEMO_VID_PATH, DEMO_OUTPUT, no_display=True, \n",
    "             render_pose=0, keypoint_scale=3, \n",
    "             frame_rotate=180, face=True, hand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          44971496       gpu demovide agrawalk PD       0:00      1 (Priority)\n",
      "          44956279       hns  jupyter agrawalk  R    4:42:41      1 sh-108-15\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b. Run openpose on videos\n",
    "\n",
    "# Old format of the command, for reference\n",
    "# cmd = ('sbatch -p gpu --gres gpu:1 -t 5:00:00 --mem 8G '\n",
    "#        '--mail-type=FAIL --mail-user=agrawalk@stanford.edu '\n",
    "#        '--wrap=\"singularity exec --nv $SINGULARITY_CACHEDIR/openpose-latest.img bash -c '\n",
    "#        '\\'cd /openpose-master && ./build/examples/openpose/openpose.bin '\n",
    "#        '--no_display true '\n",
    "#        '--render_pose 0 '\n",
    "#        '--video {0} '\n",
    "#        '--keypoint_scale 3 '\n",
    "#        '--frame_rotate 180 '\n",
    "#        '--face ' # maybe don't want this\n",
    "#        '--hand ' # probably don't want this\n",
    "#        '--write_keypoint_json {1}\\'\"')\n",
    "\n",
    "for vid_path in NEW_VID_PATHS:\n",
    "    msg = run_openpose(vid_path, OPENPOSE_OUTPUT, no_display=True, \n",
    "             render_pose=0, keypoint_scale=3, \n",
    "             frame_rotate=180, face=True, hand=True)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4c. Hand-annotate for face (y/n) on the sample. Save annotations to dataframe.\n",
    "#TODO: add case to check if annotation column exists, and ask for confirmation to overwrite\n",
    "\n",
    "annotate_frames(OUTPUT, SAMPLE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '/scratch/users/agrawalk/testvideos/061713-1.AVI'\n",
    "openpose_vid_output = os.path.join(OPENPOSE_OUTPUT, ntpath.basename(vid_path)[:-4])\n",
    "p = subprocess.Popen(cmd.format(vid_path, openpose_vid_output), shell=True, \n",
    "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(p.communicate()) #Output of job submission command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5b. add openpose data to the dataframe.\n",
    "#TODO: add case to check if openpose column exists, and ask for confirmation to overwrite\n",
    "#TODO: in this function, create the calculated columns 'face_openpose' and 'face_openpose_body'\n",
    "incorporate_openpose_output(SAMPLE_JSON_PATH, OPENPOSE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_prf(SAMPLE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_prf2(SAMPLE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Visualize detector scores\n",
    "\n",
    "df = pd.read_json('gold_set_sample.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#TODO: start using the below:\n",
    "#from sklearn.metrics import classification_report\n",
    "#print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.filter(items=[f'face_{det}' for det in ['openpose']])\n",
    "Y = df['face_present']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.30, random_state=101)\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df.index >= 1200]), len(df[df.index < 1200])) #should be 1200 for each\n",
    "groups = {'random' : df[df.index < 1200], 'face' : df[df.index >= 1200]}\n",
    "\n",
    "#TODO: dict of dicts is too complicated and doesn't support nice slicing ops,\n",
    "#switch to using multidimensional array as \"map\" (where i know that a certain index corresponds to a detector)\n",
    "#TODO: move this code to helper functions, once polished\n",
    "#TODO: alternatively, calculate the prf stuff once, save as a json, etc.\n",
    "\n",
    "avg_metrics = defaultdict(lambda: defaultdict(list))\n",
    "vid_metrics = defaultdict(lambda: defaultdict(list))\n",
    "metrics = ['p', 'r', 'f']\n",
    "det_names = ['mtcnn', 'vj', 'openpose', 'openpose_body', 'pcn']\n",
    "\n",
    "#Get metrics for each detector\n",
    "for group in groups:\n",
    "    for det in det_names:\n",
    "        cut = groups[group] #slice by face/random\n",
    "        \n",
    "        #this is p/r/f for a given det/group\n",
    "        prf = calc_prf(cut, det) \n",
    "        \n",
    "        #each element is p/r/f for a given det/group/vid\n",
    "        prf_vids = [calc_prf(cut[cut['vid_name'] == vid_name], det) for vid_name in VID_NAMES] \n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            avg_metrics[metric][det].append(prf[i])\n",
    "            vid_metrics[metric][det].append([x[i] for x in prf_vids])\n",
    "\n",
    "print(avg_metrics)\n",
    "print(vid_metrics)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot metrics\n",
    "#TODO: move this code to helper functions, once polished\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(2)\n",
    "bar_width = 0.15\n",
    "opacity = 0.8\n",
    "\n",
    "colors = [f'C{n}' for n in range(10)]\n",
    "\n",
    "for j, metric in enumerate(metrics):\n",
    "    plt.figure(j)\n",
    "    \n",
    "    #Bar chart plotting\n",
    "    for i, det in enumerate(det_names):\n",
    "        x = index + i*bar_width\n",
    "        plt.bar(x, avg_metrics[metric][det], bar_width,\n",
    "                alpha=opacity,\n",
    "                color=colors[i],\n",
    "                label=det)\n",
    "\n",
    "    #Line chart plotting\n",
    "    for i in range(len(VID_NAMES)):\n",
    "        for k in range(2): #splitting by face/random\n",
    "            #plot the scores for a given a metric, group, and video across detectors.\n",
    "            metric_y = [vid_metrics[metric][det][k][i] for det in vid_metrics[metric]]\n",
    "            metric_x = [index + i*bar_width for i in range(len(det_names))]\n",
    "            plt.plot(metric_x, metric_y, color='C4', marker='o')\n",
    "        \n",
    "    plt.xlabel('Group')\n",
    "    plt.ylabel('Scores')\n",
    "    \n",
    "    metric_print = {'p': 'Precision', 'r': 'Recall', 'f': 'F-score'}\n",
    "    plt.title(metric_print[metric])\n",
    "    \n",
    "    plt.xticks(index + bar_width, ('random', 'face'))\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gold_set_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_true = df[df['face_present'] == df['face_openpose']]\n",
    "openpose_false = df[df['face_present'] != df['face_openpose']]\n",
    "\n",
    "\n",
    "openpose_tp = openpose_true[openpose_true['face_openpose'] == True]\n",
    "openpose_tn = openpose_true[openpose_true['face_openpose'] == False]\n",
    "\n",
    "openpose_fp = openpose_false[openpose_false['face_openpose'] == True]\n",
    "openpose_fn = openpose_false[openpose_false['face_openpose'] == False]\n",
    "\n",
    "print(len(openpose_tp))\n",
    "print(len(openpose_tn))\n",
    "print(len(openpose_fp))\n",
    "print(len(openpose_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_tp[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_tp[-30:].apply(viz_op_keypoints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_tp[2:3]\n",
    "#copy the image\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_fp[:20].apply(viz_op_keypoints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_tn[:20].apply(viz_op_keypoints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_fn[:20].apply(viz_op_keypoints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Visualize frames which Openpose succeeds (True Positive) and MTCNN fails (False Positive/False Negative).\n",
    "\n",
    "def openpose_not_mtcnn(row):\n",
    "    return row['face_present'] == row['face_openpose'] and row['face_present'] != row['face_mtcnn']\n",
    "\n",
    "op_not_mtcnn = df[df.apply(openpose_not_mtcnn, axis=1)]\n",
    "\n",
    "mtcnn_fp = op_not_mtcnn[op_not_mtcnn['face_present'] == False] #but MTCNN returned True\n",
    "mtcnn_fn = op_not_mtcnn[op_not_mtcnn['face_present'] == True] #but MTCNN returned False\n",
    "\n",
    "print(f'Num False positives: {len(mtcnn_fp)}')\n",
    "print(f'Num False negatives: {len(mtcnn_fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openpose_succeeds(row):\n",
    "    return row['face_present'] == row['face_openpose']\n",
    "\n",
    "op_succeeds = df[df.apply(openpose_succeeds, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join(OUTPUT, f'{vid_name}_frames/image-{format_num(num)}.jpg')\n",
    "                 for vid_name, num in zip(mtcnn_fp['vid_name'], mtcnn_fp['frame'])]\n",
    "imgs = [plt.imread(path) for path in img_paths]\n",
    "\n",
    "for img in imgs[:3]:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return x and y arrays for a single frame\n",
    "\n",
    "def get_op_xy(keypt_lists):\n",
    "    x = []\n",
    "    y = []\n",
    "    for keypt in keypt_lists:\n",
    "        x.append(keypt[0::3]) \n",
    "        y.append(keypt[1::3])\n",
    "    if x == [] or y == []:\n",
    "        return [], []\n",
    "    \n",
    "    return x[0], y[0]\n",
    "\n",
    "def get_op_lines(x, y, pairings):\n",
    "    lines = []\n",
    "    print(f'length of x: {len(x)}')\n",
    "    print(f'length of y: {len(y)}')\n",
    "\n",
    "    for x1, y1 in zip(x, y):\n",
    "        line = []\n",
    "        for p1, p2 in pairings:\n",
    "            print(p1, p2)\n",
    "            line.append([(x[p1], y[p1]), (x[p2], y[p2])])\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_pairings = [0,1,  1,2,  2,3,  3,4,  4,5,  5,6,  6,7,  7,8,  8,9,  9,10,  10,11,  11,12,  12,13,  13,14,  14,15,  15,16,  17,18,  18,19,  19,20, \\\n",
    "                 20,21,  22,23,  23,24,  24,25,  25,26,  27,28,  28,29,  29,30,  31,32,  32,33,  33,34,  34,35,  36,37,  37,38,  38,39,  39,40,  40,41, \\\n",
    "                 41,36,  42,43,  43,44,  44,45,  45,46,  46,47,  47,42,  48,49,  49,50,  50,51,  51,52,  52,53,  53,54,  54,55,  55,56,  56,57,  57,58, \\\n",
    "                 58,59,  59,48,  60,61,  61,62,  62,63,  63,64,  64,65,  65,66,  66,67,  67,60]\n",
    "pose_pairings = [1,8,   1,2,   1,5,   2,3,   3,4,   5,6,   6,7,   8,9,   9,10,  10,11, 8,12,  12,13, 13,14,  1,0,   0,15, 15,17,  0,16, 16,18,   2,17,  5,18,   14,19,19,20,14,21, 11,22,22,23,11,24]\n",
    "face_pairings = [(p1, p2) for (p1, p2) in zip(face_pairings[0::2], face_pairings[1::2])]\n",
    "pose_pairings = [(p1, p2) for (p1, p2) in zip(pose_pairings[0::2], pose_pairings[1::2])]\n",
    "\n",
    "print(face_pairings)\n",
    "print(pose_pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_op_keypoints(row):\n",
    "    colors = [f'C{n}' for n in range(10)]\n",
    "    vid_name, num = row['vid_name'], row['frame']\n",
    "    img_path = os.path.join(OUTPUT, '{0}_frames/image-{1}.jpg'.format(vid_name, format_num(num)))\n",
    "    img = plt.imread(img_path)\n",
    "    \n",
    "    x_pose, y_pose = get_op_xy(row['pose_keypoints'])\n",
    "    x_face, y_face = get_op_xy(row['face_keypoints'])\n",
    "    \n",
    "#     pose_lines = get_op_lines(x_pose, y_pose, pose_pairings)\n",
    "#     face_lines = get_op_lines(x_face, y_face, face_pairings)\n",
    "    \n",
    "    plt.scatter(np.array(x_pose)*720/640, y_pose)\n",
    "#     lc = mc.LineCollection(pose_lines, colors=[(1, 0, 0, 1)]*len(pose_lines), linewidths=10)\n",
    "    plt.scatter(np.array(x_face)*720/640, y_face, c=colors[1])\n",
    "#     lc = mc.LineCollection(face_lines, colors=[(0, 0, 1, 1)]*len(face_lines), linewidths=10)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def viz_mtcnn_keypoints(row):\n",
    "    vid_name, num = row['vid_name'], row['frame']\n",
    "    img_path = os.path.join(OUTPUT, '{0}_frames/image-{1}.jpg'.format(vid_name, format_num(num)))\n",
    "    img = plt.imread(img_path)\n",
    "    print(img.shape)\n",
    "    \n",
    "\n",
    "#     x_pose, y_pose = get_op_xy(row['pose_keypoints'])\n",
    "#     x_face, y_face = get_op_xy(row['face_keypoints'])\n",
    "    \n",
    "#     pose_lines = get_op_lines(x_pose, y_pose, pose_pairings)\n",
    "#     face_lines = get_op_lines(x_face, y_face, face_pairings)\n",
    "    \n",
    "#     plt.scatter(np.array(x_pose) + 40, y_pose)\n",
    "#     lc = mc.LineCollection(pose_lines, colors=[(1, 0, 0, 1)]*len(pose_lines), linewidths=10)\n",
    "#     plt.scatter(np.array(x_face) + 40, y_face, c=colors[1])\n",
    "#     lc = mc.LineCollection(face_lines, colors=[(0, 0, 1, 1)]*len(face_lines), linewidths=10)\n",
    "#     fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "#     plt.imshow(img)\n",
    "    for bb in row['bb_mtcnn']:\n",
    "        x, y, w, h = bb\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    \n",
    "# img_paths = [os.path.join(OUTPUT, '{0}_frames/image-{1}.jpg'.format(vid_name, format_num(num)))\n",
    "#                  for vid_name, num in zip(mtcnn_fn['vid_name'], mtcnn_fn['frame'])][:3]\n",
    "# imgs = [plt.imread(path) for path in img_paths]\n",
    "\n",
    "\n",
    "# mtcnn_fn.apply(viz_op_keypoints, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
