{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4b27972cb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ujson\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from utils import calc_prf, calc_prf_hand\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load/preprocess openpose data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DF_PATH = '/scratch/users/agrawalk/headcam-algo-output/alice_sample.json' #Change this\n",
    "df = pd.read_json(SAMPLE_DF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folded below: utility functions for face presence calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to be applied row-wise to dataframes to calculate columns\n",
    "\n",
    "def face_openpose(row):\n",
    "    vid_name = row['vid_name'][:-4]\n",
    "    frame_num = str(row['frame']).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/Alice/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        return -1\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    face_keypts = [person['face_keypoints'] for person in keypts['people']]\n",
    "    return 1 if np.sum(face_keypts) != 0 else 0\n",
    "\n",
    "def face_openpose_nose(row):\n",
    "    vid_name = row['vid_name'][:-4]\n",
    "    frame_num = str(row['frame']).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/Alice/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        return -1\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    nose_keypts = [person['pose_keypoints'][0*3+2] for person in keypts['people']]\n",
    "    return 1 if np.sum(nose_keypts) != 0 else 0\n",
    "\n",
    "def hand_openpose(row):\n",
    "    vid_name = row['vid_name'][:-4]\n",
    "    frame_num = str(row['frame']).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/Alice/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        return -1\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    left_hand_keypts = [np.array(person['hand_left_keypoints']) for person in keypts['people']]\n",
    "    right_hand_keypts = [np.array(person['hand_right_keypoints']) for person in keypts['people']]\n",
    "    return 1 if np.sum(left_hand_keypts) != 0 or np.sum(right_hand_keypts) != 0 else 0\n",
    "\n",
    "def hand_openpose_wrist(row):\n",
    "    vid_name = row['vid_name'][:-4]\n",
    "    frame_num = str(row['frame']).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/Alice/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        return -1\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    hand_keypts = [np.array(person['pose_keypoints'])[[4*3+2, 7*3+2]] for person in keypts['people']]\n",
    "    return 1 if np.sum(hand_keypts) != 0 else 0\n",
    "\n",
    "def get_keypts_tuple(row, keypt_type, tuple_size=5):\n",
    "    vid_name = row['vid_name'][:-4]\n",
    "    middle_frame = row['frame']\n",
    "    keypts_tuple = []\n",
    "    \n",
    "    for frame in range(middle_frame - tuple_size//2, middle_frame + tuple_size//2 + 1):\n",
    "        frame = str(frame).zfill(12)\n",
    "        filename = f'{vid_name}_{frame}_keypoints.json'\n",
    "        fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/Alice/', vid_name, filename)\n",
    "        \n",
    "        if not os.path.exists(fp):\n",
    "            if frame == middle_frame:\n",
    "                return -1 #if the center frame doesn't exist, mark it for discarding\n",
    "            keypts_tuple.append([0]*70*3)\n",
    "            continue \n",
    "            \n",
    "        keypts = ujson.load(open(fp, 'r'))\n",
    "        keypts = [person[f'{keypt_type}_keypoints'] for person in keypts['people']]\n",
    "        keypts_tuple.append([0]*70*3 if len(keypts) == 0 else keypts[0])\n",
    "    \n",
    "    return keypts_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['face_openpose'] = df.apply(face_openpose, axis=1)\n",
    "df['face_openpose_nose'] = df.apply(face_openpose_nose, axis=1)\n",
    "df['hand_openpose'] = df.apply(hand_openpose, axis=1)\n",
    "df['hand_openpose_wrist'] = df.apply(hand_openpose_wrist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11879"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.query('face_openpose != -1')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face PRF Scores: Raw\n",
      "face_openpose: (0.6590131900341963, 0.533201581027668, 0.5894690845532007)\n",
      "face_openpose_nose: (0.5606060606060606, 0.7019762845849803, 0.6233766233766234)\n",
      "\n",
      "Hand PRF Scores: Raw\n",
      "hand_openpose: (0.7197953561589925, 0.3754875795524533, 0.4935240151106315)\n",
      "hand_openpose_wrist: (0.7197953561589925, 0.3754875795524533, 0.4935240151106315)\n"
     ]
    }
   ],
   "source": [
    "print('Face PRF Scores: Raw')\n",
    "prf = calc_prf(df, 'openpose')\n",
    "print(f'face_openpose: {prf}')\n",
    "prf = calc_prf(df, 'openpose_nose')\n",
    "print(f'face_openpose_nose: {prf}')\n",
    "print()\n",
    "\n",
    "print('Hand PRF Scores: Raw')\n",
    "prf = calc_prf_hand(df, 'openpose')\n",
    "print(f'hand_openpose: {prf}')\n",
    "prf = calc_prf_hand(df, 'openpose_wrist')\n",
    "print(f'hand_openpose_wrist: {prf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting face tuples...\n",
      "Getting pose tuples...\n"
     ]
    }
   ],
   "source": [
    "print('Getting face tuples...')\n",
    "df['face_tuple'] = df.apply(lambda row: get_keypts_tuple(row, 'face'), axis=1)\n",
    "print('Getting pose tuples...')\n",
    "df['pose_tuple'] = df.apply(lambda row: get_keypts_tuple(row, 'pose'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11879, 5, 70)\n",
      "(7602, 5, 70) (7602,) (1901, 5, 70) (1901,) (2376, 5, 70) (2376,)\n"
     ]
    }
   ],
   "source": [
    "#Next up: (maybe later: tuple of xy+conf, xy+conf) tuple of conf, conf\n",
    "X = df['face_tuple'].values\n",
    "y = df['face_present'].values\n",
    "X = np.array([np.array(x) for x in X])\n",
    "y = np.array([np.array(x) for x in y])\n",
    "\n",
    "X = X[:, :, 2::3] #only keypoint confidences, not x/y's (N, seq_len, 210) => (N, seq_len, 70); comment out to keep it\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='adam', alpha=0.0001, \n",
    "                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                    power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, \n",
    "                    verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-08, n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenposeLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(OpenposeLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2*2, tagset_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "                torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, keypts):\n",
    "        keypts = torch.Tensor(keypts)\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         print(keypts.shape)\n",
    "        lstm_out, _ = self.lstm(keypts)\n",
    "#         print(lstm_out[:, 0, :].shape)\n",
    "\n",
    "        # concatenating the first and last sequence element outputs \n",
    "        # (the ends of the reverse and forward chains, respectively)\n",
    "        lstm_out = torch.cat((lstm_out[:, 0], lstm_out[:, -1]), dim=1)\n",
    "#         print(lstm_out.shape)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 70\n",
    "HIDDEN_DIM = 64\n",
    "lstm = OpenposeLSTM(EMBEDDING_DIM, HIDDEN_DIM, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifiers on openpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_frame(X):\n",
    "    return X[:, X.shape[1]//2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(middle_frame(X_train), y_train)\n",
    "mlp.fit(middle_frame(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      1499\n",
      "           1       0.93      0.19      0.32       402\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1901\n",
      "   macro avg       0.87      0.59      0.61      1901\n",
      "weighted avg       0.84      0.83      0.78      1901\n",
      "\n",
      "MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1499\n",
      "           1       0.72      0.37      0.49       402\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1901\n",
      "   macro avg       0.79      0.67      0.70      1901\n",
      "weighted avg       0.82      0.84      0.82      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regression:')\n",
    "y_pred = logreg.predict(middle_frame(X_val))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print('MLP:')\n",
    "y_pred = mlp.predict(middle_frame(X_val))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#THe F-scores for the positive (1) detections are in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "#     loss_fn = nn.MSELoss(size_average=False)\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch: {e}')\n",
    "#             t, x, y = e, X_train, y_train\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "#             print(x, y)\n",
    "#             if i == 1:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 i+=1\n",
    "            model.train()  # put model to training mode\n",
    "            # Clear stored gradient\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Initialise hidden state\n",
    "            # Don't do this if you want your LSTM to be stateful\n",
    "            model.hidden = model.init_hidden()\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            scores = model(x)\n",
    "#             loss_fn = nn.LLoss()\n",
    "            print(scores.shape)\n",
    "#             loss = loss_fn(scores, y)\n",
    "#             loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 10 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "#                 print(f'Val acc: {(model(X_val).max(1)[1] == torch.Tensor(y_val).to(dtype=torch.long)).sum() / len(y_val)}')\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        X_r = np.random.random((10000, 5, 210))\n",
    "        y_r = np.random.random((10000,))\n",
    "        self.len = len(X_r)\n",
    "        self.x_data = X_r\n",
    "        self.y_data = y_r\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class OpenposeTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeValDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_val)\n",
    "        self.x_data = X_val\n",
    "        self.y_data = y_val\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "        self.len = len(X_test)\n",
    "        self.x_data = X_test\n",
    "        self.y_data = y_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = OpenposeTrainDataset()\n",
    "test_data = OpenposeTestDataset()\n",
    "val_data = OpenposeValDataset()\n",
    "loader_train = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(train_data))))\n",
    "loader_test = DataLoader(test_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(test_data))))\n",
    "loader_val = DataLoader(val_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(val_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "print_every = 100\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas)\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "torch.Size([100, 5, 70])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-8e64cbde8315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-41642d03e6f0>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#             loss_fn = nn.LLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#             loss = F.cross_entropy(scores, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/headcam/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/headcam/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/headcam/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/headcam/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train_part34(lstm, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try on the sequence of frames\n",
    "logreg.fit((X_train.reshape(X_train.shape[0], -1)), y_train)\n",
    "mlp.fit((X_train.reshape(X_train.shape[0], -1)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1499\n",
      "           1       0.94      0.26      0.40       402\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1901\n",
      "   macro avg       0.89      0.63      0.66      1901\n",
      "weighted avg       0.86      0.84      0.80      1901\n",
      "\n",
      "MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1499\n",
      "           1       0.78      0.42      0.55       402\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1901\n",
      "   macro avg       0.82      0.69      0.73      1901\n",
      "weighted avg       0.84      0.85      0.83      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regression:')\n",
    "y_pred = logreg.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print('MLP:')\n",
    "y_pred = mlp.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#THe F-scores for the positive (1) detections are in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/scratch/users/agrawalk/headcam-algo-output/alice_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>vid_path</th>\n",
       "      <th>frame</th>\n",
       "      <th>face_present</th>\n",
       "      <th>hand_present</th>\n",
       "      <th>face_openpose</th>\n",
       "      <th>face_openpose_nose</th>\n",
       "      <th>hand_openpose</th>\n",
       "      <th>hand_openpose_wrist</th>\n",
       "      <th>face_tuple</th>\n",
       "      <th>pose_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_20141124_2611_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>53210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.025443499999999997, 0.348607, 0.0002138550...</td>\n",
       "      <td>[[0.0408498, 0.403014, 0.27369099999999996, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_20140115_1602_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>4655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A_20130617_0904_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>2190</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.6433479999999999, 0.0514446, 0.000182589, ...</td>\n",
       "      <td>[[0.687879, 0.00672222, 0.706591, 0.6757369999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>A_20150425_3112_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>2070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>A_20130925_1212_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>24145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.8232600000000001, 0.293527, 0.000214731000...</td>\n",
       "      <td>[[0.765446, 0.24773099999999998, 0.628132, 0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    vid_name  \\\n",
       "0     A_20141124_2611_01.mp4   \n",
       "1     A_20140115_1602_01.mp4   \n",
       "10    A_20130617_0904_01.mp4   \n",
       "100   A_20150425_3112_01.mp4   \n",
       "1000  A_20130925_1212_01.mp4   \n",
       "\n",
       "                                               vid_path  frame  face_present  \\\n",
       "0     /scratch/groups/mcfrank/Home_Headcam_new/Alice...  53210             1   \n",
       "1     /scratch/groups/mcfrank/Home_Headcam_new/Alice...   4655             0   \n",
       "10    /scratch/groups/mcfrank/Home_Headcam_new/Alice...   2190             1   \n",
       "100   /scratch/groups/mcfrank/Home_Headcam_new/Alice...   2070             0   \n",
       "1000  /scratch/groups/mcfrank/Home_Headcam_new/Alice...  24145             1   \n",
       "\n",
       "      hand_present  face_openpose  face_openpose_nose  hand_openpose  \\\n",
       "0                1              0                   0              0   \n",
       "1                0              0                   0              0   \n",
       "10               1              1                   1              1   \n",
       "100              0              0                   0              0   \n",
       "1000             1              1                   1              1   \n",
       "\n",
       "      hand_openpose_wrist                                         face_tuple  \\\n",
       "0                       1  [[0.025443499999999997, 0.348607, 0.0002138550...   \n",
       "1                       0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "10                      1  [[0.6433479999999999, 0.0514446, 0.000182589, ...   \n",
       "100                     0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1000                    1  [[0.8232600000000001, 0.293527, 0.000214731000...   \n",
       "\n",
       "                                             pose_tuple  \n",
       "0     [[0.0408498, 0.403014, 0.27369099999999996, 0....  \n",
       "1     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "10    [[0.687879, 0.00672222, 0.706591, 0.6757369999...  \n",
       "100   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1000  [[0.765446, 0.24773099999999998, 0.628132, 0.8...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checking to see if there's even any extra non-zero info to be gained from looking at surrounding frames (esp. in FN cases)\n",
    "def extra_info(row):\n",
    "    return 1 if np.sum(np.array(row['face_tuple'])[[0,1,3,4], :]) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fp = df.query('face_present == 0 and face_openpose == 1')\n",
    "len(alice_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091690544412608"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = alice_fp.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fn = df.query('face_present == 1 and face_openpose == 0')\n",
    "len(alice_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2726502963590178"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OK, so there's certainly some to be gained on the FN frames. Why aren't the classifiers picking up on it, then?\n",
    "extra = alice_fn.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
