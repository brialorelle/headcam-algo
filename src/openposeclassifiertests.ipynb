{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a396a37b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ujson\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from config import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from detector_validation_helpers import calc_prf\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load/preprocess openpose data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>face_openpose</th>\n",
       "      <th>face_openpose_nose</th>\n",
       "      <th>face_present</th>\n",
       "      <th>frame</th>\n",
       "      <th>hand_openpose</th>\n",
       "      <th>hand_openpose_wrist</th>\n",
       "      <th>hand_present</th>\n",
       "      <th>vid_name</th>\n",
       "      <th>vid_path</th>\n",
       "      <th>face_keypoints</th>\n",
       "      <th>pose_keypoints</th>\n",
       "      <th>hand_left_keypoints</th>\n",
       "      <th>hand_right_keypoints</th>\n",
       "      <th>tuples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3515</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>S_20141112_2426_03.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.471533, 0.0980919, 0.37534799999999996, 0....</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4925</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>S_20131127_1310_04.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8785</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>S_20141228_2611_08.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[[0.281036, 0.524218, 0.0931592, 0.281939, 0.5...</td>\n",
       "      <td>[[0.295931, 0.558211, 0.7980659999999999, 0.37...</td>\n",
       "      <td>[[0.319839, 0.901076, 0.47536000000000006, 0.3...</td>\n",
       "      <td>[[0.183019, 0.7725029999999999, 0.0520404, 0.1...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14425</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>S_20130619_0802_03.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1470</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>S_20141115_2429_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      face_openpose  face_openpose_nose  face_present  frame  hand_openpose  \\\n",
       "0                 0                   1             1   3515              0   \n",
       "1                 0                   0             0   4925              0   \n",
       "10                1                   1             1   8785              1   \n",
       "100               0                   0             1  14425              0   \n",
       "1000              0                   0             1   1470              0   \n",
       "\n",
       "      hand_openpose_wrist  hand_present                vid_name  \\\n",
       "0                     NaN             1  S_20141112_2426_03.mp4   \n",
       "1                     NaN             1  S_20131127_1310_04.mp4   \n",
       "10                    NaN             1  S_20141228_2611_08.mp4   \n",
       "100                   NaN             0  S_20130619_0802_03.mp4   \n",
       "1000                  NaN             0  S_20141115_2429_01.mp4   \n",
       "\n",
       "                                               vid_path  \\\n",
       "0     /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "1     /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "10    /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "100   /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "1000  /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "\n",
       "                                         face_keypoints  \\\n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1                                                    []   \n",
       "10    [[0.281036, 0.524218, 0.0931592, 0.281939, 0.5...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                         pose_keypoints  \\\n",
       "0     [[0.471533, 0.0980919, 0.37534799999999996, 0....   \n",
       "1                                                    []   \n",
       "10    [[0.295931, 0.558211, 0.7980659999999999, 0.37...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                    hand_left_keypoints  \\\n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1                                                    []   \n",
       "10    [[0.319839, 0.901076, 0.47536000000000006, 0.3...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                   hand_right_keypoints  \\\n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1                                                    []   \n",
       "10    [[0.183019, 0.7725029999999999, 0.0520404, 0.1...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                                 tuples  \n",
       "0     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "10    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "100   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1000  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DATAFRAME_PATH = 'data/gold_sample.json'\n",
    "new_gold = pd.read_json(SAMPLE_DATAFRAME_PATH)\n",
    "new_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "face_openpose           24000 non-null int64\n",
      "face_openpose_nose      24000 non-null int64\n",
      "face_present            24000 non-null int64\n",
      "frame                   24000 non-null int64\n",
      "hand_openpose           24000 non-null int64\n",
      "hand_openpose_wrist     12000 non-null float64\n",
      "hand_present            24000 non-null int64\n",
      "vid_name                24000 non-null object\n",
      "vid_path                24000 non-null object\n",
      "face_keypoints          24000 non-null object\n",
      "pose_keypoints          24000 non-null object\n",
      "hand_left_keypoints     24000 non-null object\n",
      "hand_right_keypoints    24000 non-null object\n",
      "tuples                  24000 non-null object\n",
      "dtypes: float64(1), int64(6), object(7)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "new_gold.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folded below: utility functions for face presence calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "\n",
    "def get_op_xyconf(keypt_lists):\n",
    "    x = []\n",
    "    y = []\n",
    "    conf = []\n",
    "    for keypt in keypt_lists:\n",
    "        x.append(keypt[0::3]) \n",
    "        y.append(keypt[1::3])\n",
    "        conf.append(keypt[2::3])\n",
    "    if x == [] or y == [] or conf == []:\n",
    "        return [], [], []\n",
    "    \n",
    "    return x, y, conf\n",
    "\n",
    "def get_pose_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['pose_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_face_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['face_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_hand_left_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['hand_left_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_hand_right_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['hand_right_keypoints'] for person in keypts['people']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't need to be run; already in the df\n",
    "# new_gold['face_keypoints'] = new_gold.apply(lambda row: get_face_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('pose')\n",
    "# new_gold['pose_keypoints'] = new_gold.apply(lambda row: get_pose_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('left')\n",
    "# new_gold['hand_left_keypoints'] = new_gold.apply(lambda row: get_hand_left_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('right')\n",
    "# new_gold['hand_right_keypoints'] = new_gold.apply(lambda row: get_hand_right_keypoints(row['vid_path'], row['frame']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note: you need the files for this one; coming soon.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Functions to be applied row-wise to dataframes to calculate columns\n",
    "\n",
    "def face_openpose(row):\n",
    "    return 1 if np.sum(row['face_keypoints']) != 0 else 0\n",
    "\n",
    "def face_openpose_nose(row):\n",
    "    nose_keypts = [person_pose[0*3+2] for person_pose in row['pose_keypoints']]\n",
    "    return 1 if np.sum(nose_keypts) != 0 else 0\n",
    "\n",
    "def hand_openpose(row):\n",
    "    return 1 if np.sum(row['hand_left_keypoints']) != 0 or np.sum(row['hand_right_keypoints'])  != 0 else 0\n",
    "\n",
    "def hand_openpose_wrist(row):\n",
    "    #turns out to be the same as hand_openpose\n",
    "    hand_keypts = [np.array(person_pose[[4*3+2, 7*3+2]]) for person_pose in row['pose_keypoints']]\n",
    "    return 1 if np.sum(hand_keypts) != 0 else 0\n",
    "\n",
    "\"\"\"Note: you need the files for this one; coming soon.\"\"\"\n",
    "# def get_keypts_tuple(row, keypt_type, tuple_size=5):\n",
    "#     vid_name = row['vid_name'][:-4]\n",
    "#     middle_frame = row['frame']\n",
    "#     keypts_tuple = []\n",
    "    \n",
    "#     for frame in range(middle_frame - tuple_size//2, middle_frame + tuple_size//2 + 1):\n",
    "#         frame = str(frame).zfill(12)\n",
    "#         filename = f'{vid_name}_{frame}_keypoints.json'\n",
    "#         fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "        \n",
    "#         if not os.path.exists(fp):\n",
    "#             if frame == middle_frame:\n",
    "#                 return -1 #if the center frame doesn't exist, mark it for discarding\n",
    "#             keypts_tuple.append([0]*70*3)\n",
    "#             continue \n",
    "            \n",
    "#         keypts = ujson.load(open(fp, 'r'))\n",
    "#         keypts = [person[f'{keypt_type}_keypoints'] for person in keypts['people']]\n",
    "#         keypts_tuple.append([0]*70*3 if len(keypts) == 0 else keypts[0])\n",
    "    \n",
    "#     return keypts_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_gold['face_openpose'] = new_gold.apply(face_openpose, axis=1)\n",
    "# new_gold['face_openpose_nose'] = new_gold.apply(face_openpose_nose, axis=1)\n",
    "# new_gold['hand_openpose'] = new_gold.apply(hand_openpose, axis=1)\n",
    "# new_gold['hand_openpose_wrist'] = new_gold.apply(hand_openpose_wrist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face PRF Scores: Raw\n",
      "face_openpose: (0.7270251872021783, 0.5531767955801105, 0.6282968918521423)\n",
      "face_openpose_nose: (0.6190910472716218, 0.7032113259668509, 0.6584754668175572)\n",
      "\n",
      "Hand PRF Scores: Raw\n",
      "hand_openpose: (0.7432731293770733, 0.3930416138777897, 0.5141837190029961)\n",
      "hand_openpose_wrist: (0.719921875, 0.17961212357470033, 0.2874970751111458)\n"
     ]
    }
   ],
   "source": [
    "print('Face PRF Scores: Raw')\n",
    "prf = calc_prf(new_gold['face_openpose'], new_gold['face_present'])\n",
    "print(f'face_openpose: {prf}')\n",
    "prf = calc_prf(new_gold['face_openpose_nose'], new_gold['face_present'])\n",
    "print(f'face_openpose_nose: {prf}')\n",
    "print()\n",
    "\n",
    "print('Hand PRF Scores: Raw')\n",
    "prf = calc_prf(new_gold['hand_openpose'], new_gold['hand_present'])\n",
    "print(f'hand_openpose: {prf}')\n",
    "prf = calc_prf(new_gold['hand_openpose_wrist'], new_gold['hand_present'])\n",
    "print(f'hand_openpose_wrist: {prf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Code coming soon for this'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Code coming soon for this\"\"\"\n",
    "# print('Getting face tuples...')\n",
    "# new_gold['face_tuple'] = new_gold.apply(lambda row: get_keypts_tuple(row, 'face'), axis=1)\n",
    "# print('Getting pose tuples...')\n",
    "# new_gold['pose_tuple'] = new_gold.apply(lambda row: get_keypts_tuple(row, 'pose'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000,)\n"
     ]
    }
   ],
   "source": [
    "#Next up: (maybe later: tuple of xy+conf, xy+conf) tuple of conf, conf\n",
    "# X = new_gold['face_tuple'].values #NOTE: need the tuples data for this one-- too big for Github, coming soon.\n",
    "X = new_gold['tuples'].values \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "for i, x in enumerate(X):\n",
    "    #if i % 1000 == 0: print(i)\n",
    "    new_X.append(np.array(x))\n",
    "new_X = np.array(new_X)\n",
    "X = new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each element of tuple has 130 confidences-- the confidences for pose (18), face (70), L hand (21), \n",
    "# and R hand (21) keypoints, in that order. If you want to only keep the pose and face keypoints, for example, \n",
    "# you could say X = X[.., :88] to cut off the hand keypoints.\n",
    "# Q: are these the sum all confidences for all detected people per frame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 5, 130) (24000,) (24000,)\n"
     ]
    }
   ],
   "source": [
    "# faces\n",
    "y = new_gold['face_present'].values\n",
    "y = np.array([np.array(yi) for yi in y])\n",
    "\n",
    "# hands\n",
    "yh = new_gold['hand_present'].values\n",
    "yh = np.array([np.array(yi) for yi in yh])\n",
    "\n",
    "print(X.shape, y.shape, yh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1) # flatten the five frames of confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale feature values?\n",
    "#from sklearn import preprocessing\n",
    "#Xsc = preprocessing.scale(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19200, 650) (19200,) (4800, 650) (4800,)\n"
     ]
    }
   ],
   "source": [
    "# we will do cross-validated hyperparameter fitting on 80% of the data, and then evaluate on the final 20%\n",
    "\n",
    "# faces\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# hands\n",
    "Xh_train, Xh_test, yh_train, yh_test = train_test_split(X, yh, test_size=0.2, random_state=1)\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) # X_val.shape, y_val.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces train/test:\n",
      "0    14577\n",
      "1     4623\n",
      "Name: 0, dtype: int64\n",
      "0    3631\n",
      "1    1169\n",
      "Name: 0, dtype: int64\n",
      "Hands train/test:\n",
      "0    10923\n",
      "1     8277\n",
      "Name: 0, dtype: int64\n",
      "0    2816\n",
      "1    1984\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Faces train/test:\")\n",
    "print(pd.DataFrame(y_train)[0].value_counts())\n",
    "print(pd.DataFrame(y_test)[0].value_counts())\n",
    "print(\"Hands train/test:\")\n",
    "print(pd.DataFrame(yh_train)[0].value_counts())\n",
    "print(pd.DataFrame(yh_test)[0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy=1.0, random_state=12)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train) # faces\n",
    "\n",
    "Xh_train, yh_train = sm.fit_sample(Xh_train, yh_train) # hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upsample detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_frame(X):\n",
    "    return X[:, X.shape[1]//2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrain = pd.DataFrame(X_train) \n",
    "trainClass = pd.DataFrame(data=y_train, columns=['present'])\n",
    "allTrain = pd.concat([allTrain, trainClass], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14577\n",
       "0    14577\n",
       "Name: present, dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# separate minority and majority classes\n",
    "no_detections = allTrain[allTrain.present==0] # majority\n",
    "detections = allTrain[allTrain.present==1] # minority\n",
    "\n",
    "# upsample minority\n",
    "X_upsampled = resample(detections,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(no_detections), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([no_detections, X_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.present.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = upsampled.present\n",
    "X_train = upsampled.drop('present', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_model(X, y):\n",
    "    # Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        param_grid={\n",
    "            'max_depth': range(2,7),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        }, \n",
    "        cv=5, \n",
    "        scoring='f1_weighted', \n",
    "        verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    rfc = RandomForestClassifier(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"], \n",
    "                                 random_state=False, verbose=False)\n",
    "    # K-Fold CV\n",
    "    scores = cross_val_score(rfc, X, y, cv=5, scoring='f1_weighted')\n",
    "    return (rfc, best_params, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_rf_model, face_rf_params, face_rf_scores = rfc_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_rf_model, hand_rf_params, hand_rf_scores = rfc_model(Xh_train, yh_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77421262 0.78330646 0.81004933 0.80290346 0.80219998]\n"
     ]
    }
   ],
   "source": [
    "print(face_rf_scores) # CV faces getting .785 with upsampling, SMOTE = .7945\n",
    "\n",
    "face_rf_model.fit(X_train, y_train)\n",
    "f_pred = face_rf_model.predict(X_test)\n",
    "print(classification_report(y_test,f_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63235099 0.62244092 0.63154637 0.65064885 0.63453031]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76      2816\n",
      "           1       0.69      0.39      0.49      1984\n",
      "\n",
      "    accuracy                           0.67      4800\n",
      "   macro avg       0.68      0.63      0.63      4800\n",
      "weighted avg       0.68      0.67      0.65      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hand_rf_scores) # CV hands with SMOTE getting .63\n",
    "\n",
    "hand_rf_model.fit(Xh_train, yh_train)\n",
    "h_pred = hand_rf_model.predict(Xh_test)\n",
    "print(classification_report(yh_test, h_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7638484  0.77396673 0.7942034  0.79523238 0.78833619]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      3631\n",
      "           1       0.68      0.63      0.65      1169\n",
      "\n",
      "    accuracy                           0.84      4800\n",
      "   macro avg       0.78      0.77      0.77      4800\n",
      "weighted avg       0.83      0.84      0.84      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "print(cross_val_score(dt, X_train, y_train, cv=5)) # .76-.80\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test) \n",
    "print(classification_report(y_test, dt_pred)) # .84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.86      0.75      2816\n",
      "           1       0.66      0.40      0.50      1984\n",
      "\n",
      "    accuracy                           0.67      4800\n",
      "   macro avg       0.67      0.63      0.62      4800\n",
      "weighted avg       0.67      0.67      0.65      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.fit(Xh_train, yh_train)\n",
    "dt_pred = dt.predict(Xh_test) \n",
    "print(\"Hands:\")\n",
    "print(classification_report(yh_test, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77070828 0.77242326 0.80243526 0.79608986 0.79725557]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      3631\n",
      "           1       0.65      0.65      0.65      1169\n",
      "\n",
      "    accuracy                           0.83      4800\n",
      "   macro avg       0.77      0.77      0.77      4800\n",
      "weighted avg       0.83      0.83      0.83      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab = AdaBoostClassifier()\n",
    "print(cross_val_score(ab, X_train, y_train, cv=5)) # .77-.80\n",
    "ab.fit(X_train, y_train)\n",
    "ab_pred = ab.predict(X_test) \n",
    "print(classification_report(y_test, ab_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.75      2816\n",
      "           1       0.67      0.42      0.51      1984\n",
      "\n",
      "    accuracy                           0.67      4800\n",
      "   macro avg       0.67      0.64      0.63      4800\n",
      "weighted avg       0.67      0.67      0.66      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ab.fit(Xh_train, yh_train)\n",
    "ab_pred = ab.predict(Xh_test) \n",
    "print(classification_report(yh_test, ab_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='adam', alpha=0.0001, \n",
    "                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                    power_t=0.5, max_iter=300, shuffle=True, random_state=None, tol=0.0001, \n",
    "                    verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-08, n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=500, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenposeLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(OpenposeLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2*2, tagset_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "                torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, keypts):\n",
    "        keypts = torch.Tensor(keypts)\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         print(keypts.shape)\n",
    "        lstm_out, _ = self.lstm(keypts)\n",
    "#         print(lstm_out[:, 0, :].shape)\n",
    "\n",
    "        # concatenating the first and last sequence element outputs \n",
    "        # (the ends of the reverse and forward chains, respectively)\n",
    "        lstm_out = torch.cat((lstm_out[:, 0], lstm_out[:, -1]), dim=1)\n",
    "#         print(lstm_out.shape)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 70\n",
    "HIDDEN_DIM = 64\n",
    "lstm = OpenposeLSTM(EMBEDDING_DIM, HIDDEN_DIM, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifiers on openpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces: Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      3631\n",
      "           1       0.67      0.63      0.65      1169\n",
      "\n",
      "    accuracy                           0.83      4800\n",
      "   macro avg       0.78      0.77      0.77      4800\n",
      "weighted avg       0.83      0.83      0.83      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print('Faces: Logistic regression:')\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands: Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      3631\n",
      "           1       0.59      0.57      0.58      1169\n",
      "\n",
      "    accuracy                           0.80      4800\n",
      "   macro avg       0.73      0.72      0.73      4800\n",
      "weighted avg       0.80      0.80      0.80      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.fit(Xh_train, yh_train)\n",
    "\n",
    "print('Hands: Logistic regression:')\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces: MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      3631\n",
      "           1       0.63      0.55      0.59      1169\n",
      "\n",
      "    accuracy                           0.81      4800\n",
      "   macro avg       0.75      0.72      0.73      4800\n",
      "weighted avg       0.80      0.81      0.81      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print('Faces: MLP:')\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hands: MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      2816\n",
      "           1       0.62      0.44      0.51      1984\n",
      "\n",
      "    accuracy                           0.66      4800\n",
      "   macro avg       0.65      0.62      0.62      4800\n",
      "weighted avg       0.65      0.66      0.64      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(Xh_train, yh_train)\n",
    "\n",
    "print('Hands: MLP:')\n",
    "y_pred = mlp.predict(Xh_test)\n",
    "print(classification_report(yh_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29154, 650)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91      3631\n",
      "           1       0.76      0.57      0.65      1169\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      4800\n",
      "   macro avg       0.81      0.75      0.78      4800\n",
      "weighted avg       0.84      0.85      0.84      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forests are less affected by class imbalance\n",
    "rfc = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01533374, 0.0056569 , 0.00460523, 0.00241395, 0.00106702,\n",
       "       0.00699311, 0.0021666 , 0.00114083, 0.00238593, 0.00122409,\n",
       "       0.00114047, 0.00256548, 0.00166596, 0.00121426, 0.00448479,\n",
       "       0.01359379, 0.00325627, 0.00474551, 0.00151747, 0.00054069,\n",
       "       0.00082325, 0.00052289, 0.00073893, 0.00124199, 0.00032308,\n",
       "       0.00066082, 0.00044664, 0.00059804, 0.00062037, 0.00141899,\n",
       "       0.00043511, 0.00105366, 0.0004223 , 0.00062543, 0.00056016,\n",
       "       0.00065422, 0.00040565, 0.00071695, 0.00044249, 0.00047334,\n",
       "       0.00046042, 0.00040078, 0.00049588, 0.00040274, 0.00052837,\n",
       "       0.00038384, 0.00051384, 0.00054823, 0.00061692, 0.00049124,\n",
       "       0.00134706, 0.00046779, 0.00049189, 0.00042151, 0.001411  ,\n",
       "       0.00042987, 0.00039325, 0.00063405, 0.00055551, 0.00055367,\n",
       "       0.00061449, 0.00053225, 0.00044723, 0.00064338, 0.0005472 ,\n",
       "       0.00033611, 0.00057768, 0.00052449, 0.00037011, 0.00055051,\n",
       "       0.00058788, 0.0005277 , 0.00041728, 0.00702212, 0.00039271,\n",
       "       0.00033815, 0.00046541, 0.00035274, 0.00090796, 0.00061258,\n",
       "       0.00045828, 0.00050339, 0.00058427, 0.00050087, 0.00040066,\n",
       "       0.00038389, 0.0004728 , 0.00071719, 0.0010673 , 0.00072533,\n",
       "       0.00071887, 0.00107891, 0.00100396, 0.00073706, 0.00087816,\n",
       "       0.0007739 , 0.00083896, 0.00087894, 0.00072808, 0.00080362,\n",
       "       0.00113453, 0.00106903, 0.00067751, 0.00094281, 0.00065414,\n",
       "       0.00100392, 0.00081622, 0.000807  , 0.00091696, 0.00077409,\n",
       "       0.00071642, 0.00082256, 0.00082008, 0.00113957, 0.00083086,\n",
       "       0.00092586, 0.00087438, 0.00075183, 0.00121596, 0.00068461,\n",
       "       0.00062577, 0.00061381, 0.00087555, 0.00071383, 0.00070145,\n",
       "       0.00051405, 0.00087812, 0.0009154 , 0.00059673, 0.00064075,\n",
       "       0.02124272, 0.00597174, 0.00489716, 0.0029994 , 0.00164091,\n",
       "       0.00533781, 0.00266427, 0.00212352, 0.00243587, 0.00121829,\n",
       "       0.000746  , 0.00236205, 0.00165016, 0.00134284, 0.01494853,\n",
       "       0.01807049, 0.00313687, 0.0036931 , 0.00643423, 0.0063781 ,\n",
       "       0.01196453, 0.00773383, 0.00040964, 0.00027853, 0.00036384,\n",
       "       0.0004357 , 0.00608479, 0.00039173, 0.00625294, 0.00043499,\n",
       "       0.00051639, 0.00075092, 0.0004709 , 0.00050329, 0.0006119 ,\n",
       "       0.00625221, 0.00045934, 0.00633419, 0.00036364, 0.0005149 ,\n",
       "       0.00039734, 0.00027074, 0.00035902, 0.00032476, 0.00044548,\n",
       "       0.00639893, 0.00706059, 0.01178265, 0.0062394 , 0.00031332,\n",
       "       0.00050893, 0.00038977, 0.00041098, 0.00057757, 0.00045142,\n",
       "       0.00044465, 0.00047003, 0.00044962, 0.00056558, 0.00632621,\n",
       "       0.00043853, 0.00038944, 0.00063009, 0.00050535, 0.00056338,\n",
       "       0.00039326, 0.00041081, 0.0068052 , 0.00675011, 0.00045619,\n",
       "       0.0003573 , 0.00038466, 0.00053518, 0.0014491 , 0.00032525,\n",
       "       0.00119995, 0.00031583, 0.00033105, 0.00043414, 0.00033733,\n",
       "       0.00042677, 0.00046537, 0.0004076 , 0.00037695, 0.00028828,\n",
       "       0.02372204, 0.00049916, 0.00032105, 0.00118396, 0.00135687,\n",
       "       0.00094744, 0.00063579, 0.0008664 , 0.00078739, 0.00102436,\n",
       "       0.00095467, 0.0019135 , 0.00082811, 0.00063733, 0.00191394,\n",
       "       0.00075121, 0.00078971, 0.00084681, 0.00070215, 0.00083568,\n",
       "       0.00079074, 0.00121377, 0.00074269, 0.00079713, 0.00080039,\n",
       "       0.00078122, 0.00083885, 0.00074787, 0.00072992, 0.00074697,\n",
       "       0.00058224, 0.00076014, 0.00078587, 0.00078354, 0.00060602,\n",
       "       0.00065798, 0.00096322, 0.00060048, 0.0007391 , 0.00061816,\n",
       "       0.00069383, 0.00055698, 0.00083486, 0.00068141, 0.00079189,\n",
       "       0.03483818, 0.00564796, 0.00444347, 0.00190459, 0.00162688,\n",
       "       0.00589694, 0.00280606, 0.00135078, 0.00244509, 0.00140729,\n",
       "       0.00120552, 0.00259956, 0.00136197, 0.00107127, 0.00436396,\n",
       "       0.00426294, 0.00305866, 0.00362713, 0.00037817, 0.00044507,\n",
       "       0.00041722, 0.00032971, 0.00046899, 0.00031024, 0.00037039,\n",
       "       0.00026051, 0.00050663, 0.00050337, 0.00031747, 0.00037446,\n",
       "       0.00043971, 0.00050454, 0.00079335, 0.00047898, 0.00062228,\n",
       "       0.00034793, 0.00055806, 0.00049267, 0.00033366, 0.00037479,\n",
       "       0.00038138, 0.00048327, 0.00042805, 0.0004666 , 0.00051407,\n",
       "       0.00079675, 0.00599059, 0.00057246, 0.00038452, 0.00038225,\n",
       "       0.00081506, 0.00038128, 0.00044287, 0.00040352, 0.00033363,\n",
       "       0.0004846 , 0.0003505 , 0.00035429, 0.00040726, 0.00036457,\n",
       "       0.00044198, 0.00058558, 0.00026647, 0.00053213, 0.00037349,\n",
       "       0.00029696, 0.00041225, 0.00032781, 0.0004376 , 0.00049707,\n",
       "       0.00029064, 0.00052511, 0.00028522, 0.00033248, 0.0003291 ,\n",
       "       0.0003389 , 0.00037716, 0.00028196, 0.0003751 , 0.00045281,\n",
       "       0.00045691, 0.00045481, 0.00033512, 0.00049832, 0.00044558,\n",
       "       0.00033294, 0.00036434, 0.00052494, 0.00130622, 0.00144039,\n",
       "       0.00109378, 0.00112599, 0.00224242, 0.0009563 , 0.00098547,\n",
       "       0.00163881, 0.00128763, 0.00117513, 0.00088785, 0.00179395,\n",
       "       0.00085645, 0.00114479, 0.00089015, 0.00094979, 0.00201655,\n",
       "       0.00089124, 0.0023425 , 0.00083285, 0.00093109, 0.00073824,\n",
       "       0.00063771, 0.00077858, 0.00072502, 0.00076382, 0.00055955,\n",
       "       0.00106417, 0.00071277, 0.00053537, 0.00065086, 0.00062649,\n",
       "       0.00057497, 0.00086111, 0.00059671, 0.00080124, 0.00053123,\n",
       "       0.00063335, 0.00074148, 0.00057031, 0.00058069, 0.00073527,\n",
       "       0.02172211, 0.01061591, 0.00513058, 0.00216692, 0.00138845,\n",
       "       0.00699067, 0.00331065, 0.00138787, 0.00276151, 0.00184492,\n",
       "       0.00095442, 0.0024744 , 0.00149859, 0.00090763, 0.00506087,\n",
       "       0.01237071, 0.00389254, 0.00413153, 0.00609342, 0.00589857,\n",
       "       0.00046195, 0.00117221, 0.0010554 , 0.00048223, 0.0012651 ,\n",
       "       0.00052003, 0.00088298, 0.00046242, 0.00598113, 0.00054182,\n",
       "       0.00076176, 0.00035559, 0.00055497, 0.00053718, 0.00051345,\n",
       "       0.00025087, 0.00081652, 0.01159041, 0.00038747, 0.00040604,\n",
       "       0.00037741, 0.00029287, 0.00035053, 0.00048522, 0.0003769 ,\n",
       "       0.00071162, 0.00045174, 0.00030987, 0.0012858 , 0.00037963,\n",
       "       0.0003713 , 0.00106288, 0.00044733, 0.00032943, 0.00048108,\n",
       "       0.00027616, 0.00058963, 0.00040233, 0.00037825, 0.00042205,\n",
       "       0.00044252, 0.00040799, 0.00042693, 0.00054255, 0.00043323,\n",
       "       0.00030298, 0.00047613, 0.00612659, 0.00157828, 0.00046194,\n",
       "       0.00042996, 0.00044748, 0.00050074, 0.00031796, 0.00029983,\n",
       "       0.00033068, 0.00033857, 0.00044959, 0.00053151, 0.00053678,\n",
       "       0.00064447, 0.00040447, 0.00061003, 0.0004426 , 0.00036703,\n",
       "       0.00035869, 0.00056844, 0.0004175 , 0.00115864, 0.0011218 ,\n",
       "       0.00093117, 0.00079147, 0.00103321, 0.00102383, 0.00078531,\n",
       "       0.00101677, 0.00068194, 0.00105324, 0.00075576, 0.00071998,\n",
       "       0.00091236, 0.00109444, 0.00092283, 0.00103548, 0.00063482,\n",
       "       0.00083015, 0.00101273, 0.00107906, 0.00076966, 0.00113629,\n",
       "       0.00083429, 0.00098988, 0.00129318, 0.0009041 , 0.00077829,\n",
       "       0.00075907, 0.00063924, 0.00071468, 0.0008443 , 0.00088571,\n",
       "       0.00151552, 0.00152713, 0.00080873, 0.00069532, 0.00113855,\n",
       "       0.00110308, 0.00139124, 0.00107511, 0.00069554, 0.00064782,\n",
       "       0.02808929, 0.00672178, 0.00626415, 0.00275743, 0.0018024 ,\n",
       "       0.00501171, 0.00453245, 0.0020452 , 0.00234538, 0.00175699,\n",
       "       0.00122015, 0.00227547, 0.00130664, 0.00120012, 0.00448352,\n",
       "       0.01500169, 0.00381931, 0.00402081, 0.00068367, 0.00049873,\n",
       "       0.00048387, 0.00064333, 0.00077556, 0.00628475, 0.00040058,\n",
       "       0.00050948, 0.00048767, 0.00059452, 0.00039358, 0.00064232,\n",
       "       0.00058457, 0.00052655, 0.00109663, 0.00066564, 0.00195547,\n",
       "       0.00068263, 0.00044596, 0.0006212 , 0.00046264, 0.00053284,\n",
       "       0.00047874, 0.00055161, 0.000394  , 0.00039502, 0.00046862,\n",
       "       0.00063146, 0.00078826, 0.00049204, 0.00075134, 0.00060919,\n",
       "       0.00052616, 0.00048041, 0.00595658, 0.00049831, 0.00046468,\n",
       "       0.00039356, 0.00060986, 0.00046638, 0.0005724 , 0.0004699 ,\n",
       "       0.00053844, 0.00039336, 0.00047703, 0.00056354, 0.00061407,\n",
       "       0.00046121, 0.00036521, 0.00116656, 0.00087867, 0.00049979,\n",
       "       0.00035145, 0.00036601, 0.00057192, 0.00181912, 0.0004218 ,\n",
       "       0.00035594, 0.00034658, 0.00052433, 0.00062119, 0.00061435,\n",
       "       0.00060325, 0.00038949, 0.00045255, 0.00068325, 0.00060972,\n",
       "       0.00065195, 0.00082765, 0.00058681, 0.00113257, 0.00111141,\n",
       "       0.00095023, 0.0009323 , 0.00095744, 0.00135716, 0.00223021,\n",
       "       0.00116796, 0.00092078, 0.00121875, 0.00105427, 0.00081593,\n",
       "       0.00104785, 0.00106428, 0.00094527, 0.00137193, 0.0007951 ,\n",
       "       0.00152096, 0.00119349, 0.00099479, 0.00088549, 0.00128035,\n",
       "       0.00129812, 0.00063898, 0.00090064, 0.0007987 , 0.00196548,\n",
       "       0.00097186, 0.00097143, 0.00138319, 0.00074706, 0.00108506,\n",
       "       0.00099397, 0.00119909, 0.00133693, 0.00132195, 0.00085162,\n",
       "       0.0008888 , 0.00076032, 0.00088124, 0.00080062, 0.00082026])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.feature_importances_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3929\n",
      "1     871\n",
      "Name: 0, dtype: int64\n",
      "0    3776\n",
      "1    1024\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# what are the predictions?\n",
    "print(pd.DataFrame(rfc_pred)[0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try yet another classifier...\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(gamma='scale')\n",
    "clf.set_params(kernel='rbf').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      3631\n",
      "           1       0.71      0.63      0.67      1169\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      4800\n",
      "   macro avg       0.80      0.77      0.78      4800\n",
      "weighted avg       0.84      0.85      0.84      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "#     loss_fn = nn.MSELoss(size_average=False)\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch: {e}')\n",
    "#             t, x, y = e, X_train, y_train\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "#             print(x, y)\n",
    "#             if i == 1:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 i+=1\n",
    "            model.train()  # put model to training mode\n",
    "            # Clear stored gradient\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Initialise hidden state\n",
    "            # Don't do this if you want your LSTM to be stateful\n",
    "            model.hidden = model.init_hidden()\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            scores = model(x)\n",
    "#             loss_fn = nn.LLoss()\n",
    "            print(scores.shape)\n",
    "#             loss = loss_fn(scores, y)\n",
    "#             loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 10 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "#                 print(f'Val acc: {(model(X_val).max(1)[1] == torch.Tensor(y_val).to(dtype=torch.long)).sum() / len(y_val)}')\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        X_r = np.random.random((10000, 5, 210))\n",
    "        y_r = np.random.random((10000,))\n",
    "        self.len = len(X_r)\n",
    "        self.x_data = X_r\n",
    "        self.y_data = y_r\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class OpenposeTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeValDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_val)\n",
    "        self.x_data = X_val\n",
    "        self.y_data = y_val\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "        self.len = len(X_test)\n",
    "        self.x_data = X_test\n",
    "        self.y_data = y_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = OpenposeTrainDataset()\n",
    "test_data = OpenposeTestDataset()\n",
    "val_data = OpenposeValDataset()\n",
    "loader_train = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(train_data))))\n",
    "loader_test = DataLoader(test_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(test_data))))\n",
    "loader_val = DataLoader(val_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(val_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "print_every = 100\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas)\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "14714",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 14714",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8e64cbde8315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-500b8a10ae01>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#             t, x, y = e, X_train, y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m#             print(x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             if i == 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-850a3edfa661>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 14714"
     ]
    }
   ],
   "source": [
    "train_part34(lstm, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now try on the sequence of frames\n",
    "logreg.fit((X_train.reshape(X_train.shape[0], -1)), y_train)\n",
    "mlp.fit((X_train.reshape(X_train.shape[0], -1)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic regression:')\n",
    "y_pred = logreg.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print('MLP:')\n",
    "y_pred = mlp.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#THe F-scores for the positive (1) detections are in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/scratch/users/agrawalk/headcam-algo-output/alice_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checking to see if there's even any extra non-zero info to be gained from looking at surrounding frames (esp. in FN cases)\n",
    "def extra_info(row):\n",
    "    return 1 if np.sum(np.array(row['face_tuple'])[[0,1,3,4], :]) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_fp = df.query('face_present == 0 and face_openpose == 1')\n",
    "len(alice_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = alice_fp.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_fn = df.query('face_present == 1 and face_openpose == 0')\n",
    "len(alice_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, so there's certainly some to be gained on the FN frames. Why aren't the classifiers picking up on it, then?\n",
    "extra = alice_fn.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
