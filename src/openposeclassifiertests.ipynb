{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2dd7892370>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ujson\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "from utils import calc_prf, calc_prf_hand\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load/preprocess openpose data into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>face_openpose</th>\n",
       "      <th>face_openpose_nose</th>\n",
       "      <th>face_present</th>\n",
       "      <th>frame</th>\n",
       "      <th>hand_openpose</th>\n",
       "      <th>hand_openpose_wrist</th>\n",
       "      <th>hand_present</th>\n",
       "      <th>index</th>\n",
       "      <th>vid_name</th>\n",
       "      <th>vid_path</th>\n",
       "      <th>face_keypoints</th>\n",
       "      <th>pose_keypoints</th>\n",
       "      <th>hand_left_keypoints</th>\n",
       "      <th>hand_right_keypoints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3515</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S_20141112_2426_03.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.471533, 0.0980919, 0.37534799999999996, 0....</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4925</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S_20131127_1310_04.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8785</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>S_20141228_2611_08.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[[0.281036, 0.524218, 0.0931592, 0.281939, 0.5...</td>\n",
       "      <td>[[0.295931, 0.558211, 0.7980659999999999, 0.37...</td>\n",
       "      <td>[[0.319839, 0.901076, 0.47536000000000006, 0.3...</td>\n",
       "      <td>[[0.183019, 0.7725029999999999, 0.0520404, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14425</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10087</td>\n",
       "      <td>S_20130619_0802_03.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1470</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10898</td>\n",
       "      <td>S_20141115_2429_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Samca...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      level_0  face_openpose  face_openpose_nose  face_present  frame  \\\n",
       "0           0              0                   1             1   3515   \n",
       "1           1              0                   0             0   4925   \n",
       "10         10              1                   1             1   8785   \n",
       "100       100              0                   0             1  14425   \n",
       "1000     1000              0                   0             1   1470   \n",
       "\n",
       "      hand_openpose  hand_openpose_wrist  hand_present  index  \\\n",
       "0                 0                  NaN             1      0   \n",
       "1                 0                  NaN             1      1   \n",
       "10                1                  NaN             1  10005   \n",
       "100               0                  NaN             0  10087   \n",
       "1000              0                  NaN             0  10898   \n",
       "\n",
       "                    vid_name  \\\n",
       "0     S_20141112_2426_03.mp4   \n",
       "1     S_20131127_1310_04.mp4   \n",
       "10    S_20141228_2611_08.mp4   \n",
       "100   S_20130619_0802_03.mp4   \n",
       "1000  S_20141115_2429_01.mp4   \n",
       "\n",
       "                                               vid_path  \\\n",
       "0     /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "1     /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "10    /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "100   /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "1000  /scratch/groups/mcfrank/Home_Headcam_new/Samca...   \n",
       "\n",
       "                                         face_keypoints  \\\n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1                                                    []   \n",
       "10    [[0.281036, 0.524218, 0.0931592, 0.281939, 0.5...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                         pose_keypoints  \\\n",
       "0     [[0.471533, 0.0980919, 0.37534799999999996, 0....   \n",
       "1                                                    []   \n",
       "10    [[0.295931, 0.558211, 0.7980659999999999, 0.37...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                    hand_left_keypoints  \\\n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1                                                    []   \n",
       "10    [[0.319839, 0.901076, 0.47536000000000006, 0.3...   \n",
       "100                                                  []   \n",
       "1000                                                 []   \n",
       "\n",
       "                                   hand_right_keypoints  \n",
       "0     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1                                                    []  \n",
       "10    [[0.183019, 0.7725029999999999, 0.0520404, 0.1...  \n",
       "100                                                  []  \n",
       "1000                                                 []  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DF_PATH = 'new_gold_sample_big.json' #Change this\n",
    "new_gold = pd.read_json(SAMPLE_DF_PATH)\n",
    "new_gold.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folded below: utility functions for face presence calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "\n",
    "def get_op_xyconf(keypt_lists):\n",
    "    x = []\n",
    "    y = []\n",
    "    conf = []\n",
    "    for keypt in keypt_lists:\n",
    "        x.append(keypt[0::3]) \n",
    "        y.append(keypt[1::3])\n",
    "        conf.append(keypt[2::3])\n",
    "    if x == [] or y == [] or conf == []:\n",
    "        return [], [], []\n",
    "    \n",
    "    return x, y, conf\n",
    "\n",
    "def get_pose_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['pose_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_face_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['face_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_hand_left_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['hand_left_keypoints'] for person in keypts['people']]\n",
    "\n",
    "def get_hand_right_keypoints(vid_path, frame):\n",
    "    vid_name = ntpath.basename(vid_path)[:-4]\n",
    "    frame_num = str(frame).zfill(12)\n",
    "    filename = f'{vid_name}_{frame_num}_keypoints.json'\n",
    "    fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "    if not os.path.exists(fp):\n",
    "        print('near start or end of video')\n",
    "        return []\n",
    "    keypts = ujson.load(open(fp, 'r'))\n",
    "    return [person['hand_right_keypoints'] for person in keypts['people']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pose\n",
      "left\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "#Doesn't need to be run; already in the df\n",
    "# new_gold['face_keypoints'] = new_gold.apply(lambda row: get_face_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('pose')\n",
    "# new_gold['pose_keypoints'] = new_gold.apply(lambda row: get_pose_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('left')\n",
    "# new_gold['hand_left_keypoints'] = new_gold.apply(lambda row: get_hand_left_keypoints(row['vid_path'], row['frame']), axis=1)\n",
    "# print('right')\n",
    "# new_gold['hand_right_keypoints'] = new_gold.apply(lambda row: get_hand_right_keypoints(row['vid_path'], row['frame']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to be applied row-wise to dataframes to calculate columns\n",
    "\n",
    "def face_openpose(row):\n",
    "    return 1 if np.sum(row['face_keypoints']) != 0 else 0\n",
    "\n",
    "def face_openpose_nose(row):\n",
    "    nose_keypts = [person_pose[0*3+2] for person_pose in row['pose_keypoints']]\n",
    "    return 1 if np.sum(nose_keypts) != 0 else 0\n",
    "\n",
    "def hand_openpose(row):\n",
    "    return 1 if np.sum(row['hand_left_keypoints']) != 0 or np.sum(row['hand_right_keypoints'])  != 0 else 0\n",
    "\n",
    "def hand_openpose_wrist(row):\n",
    "    #turns out to be the same as hand_openpose\n",
    "    hand_keypts = [np.array(person_pose[[4*3+2, 7*3+2]]) for person_pose in row['pose_keypoints']]\n",
    "    return 1 if np.sum(hand_keypts) != 0 else 0\n",
    "\n",
    "\"\"\"Note: you need the files for this one; coming soon.\"\"\"\n",
    "# def get_keypts_tuple(row, keypt_type, tuple_size=5):\n",
    "#     vid_name = row['vid_name'][:-4]\n",
    "#     middle_frame = row['frame']\n",
    "#     keypts_tuple = []\n",
    "    \n",
    "#     for frame in range(middle_frame - tuple_size//2, middle_frame + tuple_size//2 + 1):\n",
    "#         frame = str(frame).zfill(12)\n",
    "#         filename = f'{vid_name}_{frame}_keypoints.json'\n",
    "#         fp = os.path.join('/scratch/users/agrawalk/headcam-algo-output/gold_sample_openpose/', vid_name, filename)\n",
    "        \n",
    "#         if not os.path.exists(fp):\n",
    "#             if frame == middle_frame:\n",
    "#                 return -1 #if the center frame doesn't exist, mark it for discarding\n",
    "#             keypts_tuple.append([0]*70*3)\n",
    "#             continue \n",
    "            \n",
    "#         keypts = ujson.load(open(fp, 'r'))\n",
    "#         keypts = [person[f'{keypt_type}_keypoints'] for person in keypts['people']]\n",
    "#         keypts_tuple.append([0]*70*3 if len(keypts) == 0 else keypts[0])\n",
    "    \n",
    "#     return keypts_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_gold['face_openpose'] = new_gold.apply(face_openpose, axis=1)\n",
    "# new_gold['face_openpose_nose'] = new_gold.apply(face_openpose_nose, axis=1)\n",
    "# new_gold['hand_openpose'] = new_gold.apply(hand_openpose, axis=1)\n",
    "# new_gold['hand_openpose_wrist'] = new_gold.apply(hand_openpose_wrist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face PRF Scores: Raw\n",
      "4407 5792 3204\n",
      "face_openpose: (0.7270251872021783, 0.5531767955801105, 0.6282968918521423)\n",
      "6579 5792 4073\n",
      "face_openpose_nose: (0.6190910472716218, 0.7032113259668509, 0.6584754668175572)\n",
      "\n",
      "Hand PRF Scores: Raw\n",
      "5426 10261 4033\n",
      "hand_openpose: (0.7432731293770733, 0.3930416138777897, 0.5141837190029961)\n"
     ]
    }
   ],
   "source": [
    "print('Face PRF Scores: Raw')\n",
    "prf = calc_prf(new_gold['face_openpose'], new_gold['face_present'])\n",
    "print(f'face_openpose: {prf}')\n",
    "prf = calc_prf(new_gold['face_openpose_nose'], new_gold['face_present'])\n",
    "print(f'face_openpose_nose: {prf}')\n",
    "print()\n",
    "\n",
    "print('Hand PRF Scores: Raw')\n",
    "prf = calc_prf(new_gold['hand_openpose'], new_gold['hand_present'])\n",
    "print(f'hand_openpose: {prf}')\n",
    "# prf = calc_prf(new_gold['hand_openpose_wrist'], new_gold['hand_present'])\n",
    "# print(f'hand_openpose_wrist: {prf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting face tuples...\n",
      "Getting pose tuples...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Code coming soon for this\"\"\"\n",
    "# print('Getting face tuples...')\n",
    "# new_gold['face_tuple'] = new_gold.apply(lambda row: get_keypts_tuple(row, 'face'), axis=1)\n",
    "# print('Getting pose tuples...')\n",
    "# new_gold['pose_tuple'] = new_gold.apply(lambda row: get_keypts_tuple(row, 'pose'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 210)\n",
      "(24000, 70)\n",
      "(15360, 70) (15360,) (3840, 70) (3840,) (4800, 70) (4800,)\n"
     ]
    }
   ],
   "source": [
    "#Next up: (maybe later: tuple of xy+conf, xy+conf) tuple of conf, conf\n",
    "# X = new_gold['face_tuple'].values #NOTE: need the tuples data for this one-- too big for Github, coming soon.\n",
    "X = new_gold['face_keypoints'].values \n",
    "#Making it a consistent shape\n",
    "X = np.array([[0] * 210 if x == [] else x[0] for x in X]) #NOTE: Right now this selects only the first person.\n",
    "#TODO: Change to sum all people's confidences\n",
    "\n",
    "y = new_gold['face_present'].values\n",
    "X = np.array([np.array(x) for x in X])\n",
    "y = np.array([np.array(x) for x in y])\n",
    "\n",
    "# X = X[:, :, 2::3] #only keypoint confidences, not x/y's (N, seq_len, 210) => (N, seq_len, 70); comment out to keep it\n",
    "print(X.shape)\n",
    "X = X[:, 2::3] #only keypoint confidences, not x/y's (N, 210) => (N, 70); comment out to keep it\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation='relu', solver='adam', alpha=0.0001, \n",
    "                    batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                    power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, \n",
    "                    verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-08, n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenposeLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(OpenposeLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2*2, tagset_size)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(1, BATCH_SIZE, self.hidden_dim),\n",
    "                torch.zeros(1, BATCH_SIZE, self.hidden_dim))\n",
    "    \n",
    "    def forward(self, keypts):\n",
    "        keypts = torch.Tensor(keypts)\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         print(keypts.shape)\n",
    "        lstm_out, _ = self.lstm(keypts)\n",
    "#         print(lstm_out[:, 0, :].shape)\n",
    "\n",
    "        # concatenating the first and last sequence element outputs \n",
    "        # (the ends of the reverse and forward chains, respectively)\n",
    "        lstm_out = torch.cat((lstm_out[:, 0], lstm_out[:, -1]), dim=1)\n",
    "#         print(lstm_out.shape)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 70\n",
    "HIDDEN_DIM = 64\n",
    "lstm = OpenposeLSTM(EMBEDDING_DIM, HIDDEN_DIM, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifiers on openpose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_frame(X):\n",
    "    return X[:, X.shape[1]//2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(middle_frame(X_train), y_train)\n",
    "mlp.fit(middle_frame(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      1499\n",
      "           1       0.93      0.19      0.32       402\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1901\n",
      "   macro avg       0.87      0.59      0.61      1901\n",
      "weighted avg       0.84      0.83      0.78      1901\n",
      "\n",
      "MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1499\n",
      "           1       0.72      0.37      0.49       402\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1901\n",
      "   macro avg       0.79      0.67      0.70      1901\n",
      "weighted avg       0.82      0.84      0.82      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regression:')\n",
    "y_pred = logreg.predict(middle_frame(X_val))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print('MLP:')\n",
    "y_pred = mlp.predict(middle_frame(X_val))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#THe F-scores for the positive (1) detections are in the second row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "#     loss_fn = nn.MSELoss(size_average=False)\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(f'Epoch: {e}')\n",
    "#             t, x, y = e, X_train, y_train\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "#             print(x, y)\n",
    "#             if i == 1:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 i+=1\n",
    "            model.train()  # put model to training mode\n",
    "            # Clear stored gradient\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Initialise hidden state\n",
    "            # Don't do this if you want your LSTM to be stateful\n",
    "            model.hidden = model.init_hidden()\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            print(x.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            scores = model(x)\n",
    "#             loss_fn = nn.LLoss()\n",
    "            print(scores.shape)\n",
    "#             loss = loss_fn(scores, y)\n",
    "#             loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % 10 == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "#                 print(f'Val acc: {(model(X_val).max(1)[1] == torch.Tensor(y_val).to(dtype=torch.long)).sum() / len(y_val)}')\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        X_r = np.random.random((10000, 5, 210))\n",
    "        y_r = np.random.random((10000,))\n",
    "        self.len = len(X_r)\n",
    "        self.x_data = X_r\n",
    "        self.y_data = y_r\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "class OpenposeTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeValDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.len = len(X_val)\n",
    "        self.x_data = X_val\n",
    "        self.y_data = y_val\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class OpenposeTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.train = False\n",
    "        self.len = len(X_test)\n",
    "        self.x_data = X_test\n",
    "        self.y_data = y_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "train_data = OpenposeTrainDataset()\n",
    "test_data = OpenposeTestDataset()\n",
    "val_data = OpenposeValDataset()\n",
    "loader_train = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(train_data))))\n",
    "loader_test = DataLoader(test_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(test_data))))\n",
    "loader_val = DataLoader(val_data, batch_size=BATCH_SIZE, sampler=sampler.SubsetRandomSampler(range(len(val_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "print_every = 100\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas)\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part34(lstm, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/agrawalk/miniconda2/envs/headcam/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now try on the sequence of frames\n",
    "logreg.fit((X_train.reshape(X_train.shape[0], -1)), y_train)\n",
    "mlp.fit((X_train.reshape(X_train.shape[0], -1)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1499\n",
      "           1       0.94      0.26      0.40       402\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1901\n",
      "   macro avg       0.89      0.63      0.66      1901\n",
      "weighted avg       0.86      0.84      0.80      1901\n",
      "\n",
      "MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      1499\n",
      "           1       0.78      0.42      0.55       402\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1901\n",
      "   macro avg       0.82      0.69      0.73      1901\n",
      "weighted avg       0.84      0.85      0.83      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic regression:')\n",
    "y_pred = logreg.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print('MLP:')\n",
    "y_pred = mlp.predict((X_val.reshape(X_val.shape[0], -1)))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "#THe F-scores for the positive (1) detections are in the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/scratch/users/agrawalk/headcam-algo-output/alice_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>vid_path</th>\n",
       "      <th>frame</th>\n",
       "      <th>face_present</th>\n",
       "      <th>hand_present</th>\n",
       "      <th>face_openpose</th>\n",
       "      <th>face_openpose_nose</th>\n",
       "      <th>hand_openpose</th>\n",
       "      <th>hand_openpose_wrist</th>\n",
       "      <th>face_tuple</th>\n",
       "      <th>pose_tuple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A_20141124_2611_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>53210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.025443499999999997, 0.348607, 0.0002138550...</td>\n",
       "      <td>[[0.0408498, 0.403014, 0.27369099999999996, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A_20140115_1602_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>4655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A_20130617_0904_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>2190</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.6433479999999999, 0.0514446, 0.000182589, ...</td>\n",
       "      <td>[[0.687879, 0.00672222, 0.706591, 0.6757369999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>A_20150425_3112_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>2070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>A_20130925_1212_01.mp4</td>\n",
       "      <td>/scratch/groups/mcfrank/Home_Headcam_new/Alice...</td>\n",
       "      <td>24145</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.8232600000000001, 0.293527, 0.000214731000...</td>\n",
       "      <td>[[0.765446, 0.24773099999999998, 0.628132, 0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    vid_name  \\\n",
       "0     A_20141124_2611_01.mp4   \n",
       "1     A_20140115_1602_01.mp4   \n",
       "10    A_20130617_0904_01.mp4   \n",
       "100   A_20150425_3112_01.mp4   \n",
       "1000  A_20130925_1212_01.mp4   \n",
       "\n",
       "                                               vid_path  frame  face_present  \\\n",
       "0     /scratch/groups/mcfrank/Home_Headcam_new/Alice...  53210             1   \n",
       "1     /scratch/groups/mcfrank/Home_Headcam_new/Alice...   4655             0   \n",
       "10    /scratch/groups/mcfrank/Home_Headcam_new/Alice...   2190             1   \n",
       "100   /scratch/groups/mcfrank/Home_Headcam_new/Alice...   2070             0   \n",
       "1000  /scratch/groups/mcfrank/Home_Headcam_new/Alice...  24145             1   \n",
       "\n",
       "      hand_present  face_openpose  face_openpose_nose  hand_openpose  \\\n",
       "0                1              0                   0              0   \n",
       "1                0              0                   0              0   \n",
       "10               1              1                   1              1   \n",
       "100              0              0                   0              0   \n",
       "1000             1              1                   1              1   \n",
       "\n",
       "      hand_openpose_wrist                                         face_tuple  \\\n",
       "0                       1  [[0.025443499999999997, 0.348607, 0.0002138550...   \n",
       "1                       0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "10                      1  [[0.6433479999999999, 0.0514446, 0.000182589, ...   \n",
       "100                     0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1000                    1  [[0.8232600000000001, 0.293527, 0.000214731000...   \n",
       "\n",
       "                                             pose_tuple  \n",
       "0     [[0.0408498, 0.403014, 0.27369099999999996, 0....  \n",
       "1     [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "10    [[0.687879, 0.00672222, 0.706591, 0.6757369999...  \n",
       "100   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "1000  [[0.765446, 0.24773099999999998, 0.628132, 0.8...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checking to see if there's even any extra non-zero info to be gained from looking at surrounding frames (esp. in FN cases)\n",
    "def extra_info(row):\n",
    "    return 1 if np.sum(np.array(row['face_tuple'])[[0,1,3,4], :]) != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fp = df.query('face_present == 0 and face_openpose == 1')\n",
    "len(alice_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091690544412608"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = alice_fp.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_fn = df.query('face_present == 1 and face_openpose == 0')\n",
    "len(alice_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2726502963590178"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OK, so there's certainly some to be gained on the FN frames. Why aren't the classifiers picking up on it, then?\n",
    "extra = alice_fn.apply(extra_info, axis=1).values\n",
    "extra.sum()/len(extra)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
